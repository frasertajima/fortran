# CIFAR-10 v27: Python + CUDA Fortran Workflow

## Overview

Version 27 introduces a **two-stage workflow** that combines the best of both worlds:
- **Python**: Familiar data loading and preprocessing
- **CUDA Fortran**: Ultra-fast CNN training

This follows the successful pattern from Oxford Flowers 102 and makes CIFAR-10 training more accessible while maintaining the speed advantage.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Python (Data Preprocessing)                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  â€¢ Load CIFAR-10 dataset                                         â”‚
â”‚  â€¢ Normalize images [0, 1]                                       â”‚
â”‚  â€¢ Transpose for Fortran column-major order                      â”‚
â”‚  â€¢ Save to binary files                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: CUDA Fortran (Fast Training)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  â€¢ Load binary files (simple!)                                   â”‚
â”‚  â€¢ Train 3-layer CNN on GPU                                      â”‚
â”‚  â€¢ cuDNN convolutions + custom kernels                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### 1. Prepare Data (One-Time)

```bash
python prepare_cifar10_v27.py
```

**Output**:
```
cifar10_data/
â”œâ”€â”€ images_train.bin  (300 MB)
â”œâ”€â”€ labels_train.bin  (200 KB)
â”œâ”€â”€ images_test.bin   (60 MB)
â””â”€â”€ labels_test.bin   (40 KB)
```

### 2. Compile Training Code

```bash
./compile_cifar10_v27.sh
```

### 3. Train Model

```bash
./cifar10_cudnn_v27
```

**Expected Performance**: ~85-90% test accuracy in ~52 seconds on RTX 4060

## Data Format

### Python Output (Column-Major for Fortran)

**Images**:
- Format: `float32`, normalized [0, 1]
- Layout: `(C, H, W, N)` = (3, 32, 32, N)
- Training: 50,000 images
- Test: 10,000 images

**Labels**:
- Format: `int32`
- Range: [0, 9] (10 classes)
- No one-hot encoding needed!

### Fortran Input

Fortran reads the transposed data and flattens to (N, 3072) for CNN processing. Much simpler than v26!

## Advantages Over v26

| Aspect | v26 (All Fortran) | v27 (Python + Fortran) |
|--------|-------------------|------------------------|
| **Data Loading** | Complex CIFAR-10 binary parsing | Simple binary file load |
| **Preprocessing** | Manual Fortran normalization | Familiar Python code |
| **Accessibility** | Fortran expertise needed | Python skills sufficient |
| **Debugging** | Hard to verify data | Easy Python verification |
| **Training Speed** | ~3.5 seconds/epoch | ~3.5 seconds/epoch (same!) |
| **Code Lines** | ~600 lines data loading | ~150 lines data loading |

## CNN Architecture (Same as v26)

```
Input: 32x32x3 images

Conv1: 32 filters, 3x3, ReLU + MaxPool(2x2)  â†’ 16x16x32
Conv2: 64 filters, 3x3, ReLU + MaxPool(2x2)  â†’ 8x8x64
Conv3: 128 filters, 3x3, ReLU + MaxPool(2x2) â†’ 4x4x128

Flatten â†’ 2048 features
Dense1: 2048 â†’ 512, ReLU, Dropout(0.5)
Dense2: 512 â†’ 10 (softmax)

Total parameters: ~1.2M
Optimizer: Adam (lr=0.001)
```

## File Structure

```
CIFAR-10/
â”œâ”€â”€ prepare_cifar10_v27.py       # Stage 1: Python preprocessing
â”œâ”€â”€ cifar10_cudnn_v27.cuf        # Stage 2: CUDA Fortran training
â”œâ”€â”€ compile_cifar10_v27.sh       # Compilation script
â”œâ”€â”€ cifar10_data/                # Generated by Python
â”‚   â”œâ”€â”€ images_train.bin
â”‚   â”œâ”€â”€ labels_train.bin
â”‚   â”œâ”€â”€ images_test.bin
â”‚   â””â”€â”€ labels_test.bin
â””â”€â”€ cifar-10-batches-py/         # Downloaded by Python script
    â””â”€â”€ [CIFAR-10 dataset files]
```

## Comparison with Oxford Flowers Pattern

| Oxford Flowers 102 | CIFAR-10 v27 |
|-------------------|--------------|
| Python: Extract MobileNetV2 features | Python: Load & normalize images |
| Fortran: Train Dense(1280â†’102) | Fortran: Train full CNN |
| Runtime: ~1 second/100 epochs | Runtime: ~3.5 seconds/epoch |
| Accuracy: 78.94% (beats PyTorch!) | Accuracy: ~85-90% |

**Same philosophy**: Python handles data, Fortran handles fast training!

## When to Use v27 vs v26

### Use v27 if:
- âœ… You're new to CUDA Fortran
- âœ… You want familiar Python data handling
- âœ… You need to modify preprocessing easily
- âœ… You want simpler, more maintainable code
- âœ… You're following the Oxford Flowers pattern

### Use v26 if:
- âœ… You want pure Fortran (no Python dependency)
- âœ… You're debugging low-level data issues
- âœ… You need maximum control over every step
- âœ… You're a Fortran purist

**Both achieve the same training speed!**

## Workflow Benefits

### Familiar Python Stage
```python
# Load CIFAR-10 (standard PyTorch/TensorFlow approach)
train_images = ...  # (50000, 3, 32, 32)
train_labels = ...  # (50000,)

# Normalize
train_images = train_images / 255.0

# Save for Fortran (transpose to column-major)
train_images.T.tofile('images_train.bin')
train_labels.tofile('labels_train.bin')
```

### Simple Fortran Stage
```fortran
! Load preprocessed data
open(unit=10, file='cifar10_data/images_train.bin', ...)
read(10) train_images

! Train CNN
call train_cnn(model, train_images, train_labels)
```


**Iteration speed**: Same as v26, more accessible!

## Future Enhancements

With this clean separation, you can easily:
- [ ] Add data augmentation in Python (flips, crops, etc.)
- [ ] Experiment with different normalizations
- [ ] Try different feature extractors (ResNet, etc.)
- [ ] Add validation split
- [ ] Save checkpoints
- [ ] Visualize training progress

**All without touching the fast Fortran training code!**

## Conclusion

v27 brings the Oxford Flowers success pattern to CIFAR-10:
- **Accessibility**: Familiar Python preprocessing
- **Speed**: Same ultra-fast Fortran training
- **Maintainability**: Clean separation of concerns
- **Flexibility**: Easy to modify data pipeline

This is the **recommended approach** for new CIFAR-10 projects!

---

**The best of both worlds**: Python's ease + Fortran's speed = Perfect ML workflow ğŸš€
