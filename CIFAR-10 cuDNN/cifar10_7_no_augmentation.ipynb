{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1d4a60",
   "metadata": {},
   "source": [
    "It turns out for CIFAR-10, augmentation does not help noticably. The gap with Fortran is due to the training, not the augmentation!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99198a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imported tensor_core_wrapper7 with TensorCoreLinear\n",
      "ðŸ§ª Running CIFAR-10 Baseline Test (NO AUGMENTATION)\n",
      "This will establish the Python baseline for comparison\n",
      "\n",
      "ðŸš€ CIFAR-10 BASELINE TRAINING - NO AUGMENTATION\n",
      "============================================================\n",
      "Device: cuda\n",
      "ðŸ”§ TensorCore debug mode: disabled\n",
      "ðŸ“ Loading CIFAR-10 dataset with NO AUGMENTATION...\n",
      "âœ… CIFAR-10 loaded with NO AUGMENTATION:\n",
      "   Training samples: 50,000\n",
      "   Test samples: 10,000\n",
      "   Batch size: 128\n",
      "   Only normalization applied: (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
      "\n",
      "ðŸ“Š Model Information:\n",
      "   Parameters: 1,276,682\n",
      "   Epochs: 15\n",
      "   Batch size: 128\n",
      "   Learning rate: 0.001\n",
      "   Architecture: Same as Fortran version\n",
      "   Augmentation: NONE (baseline test)\n",
      "\n",
      "ðŸ‹ï¸ Starting Baseline Training (NO AUGMENTATION)...\n",
      "\n",
      "ðŸ“ˆ EPOCH 1/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 2.340947 Acc: 11.719%\n",
      "   Batch 100/391 Loss: 1.478746 Acc: 32.534%\n",
      "   Batch 200/391 Loss: 1.343262 Acc: 39.342%\n",
      "   Batch 300/391 Loss: 1.418553 Acc: 43.628%\n",
      "    plane:  64.50%\n",
      "      car:  68.30%\n",
      "     bird:  36.40%\n",
      "      cat:  44.70%\n",
      "     deer:  60.10%\n",
      "      dog:  47.90%\n",
      "     frog:  84.40%\n",
      "    horse:  71.90%\n",
      "     ship:  72.80%\n",
      "    truck:  80.50%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "ðŸ’¾ New best accuracy: 63.15%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 2/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 1.282077 Acc: 52.344%\n",
      "   Batch 100/391 Loss: 1.070060 Acc: 59.916%\n",
      "   Batch 200/391 Loss: 1.000974 Acc: 61.381%\n",
      "   Batch 300/391 Loss: 1.028473 Acc: 62.160%\n",
      "    plane:  69.70%\n",
      "      car:  79.20%\n",
      "     bird:  59.10%\n",
      "      cat:  35.90%\n",
      "     deer:  69.20%\n",
      "      dog:  60.00%\n",
      "     frog:  79.30%\n",
      "    horse:  80.80%\n",
      "     ship:  76.90%\n",
      "    truck:  82.50%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "ðŸ’¾ New best accuracy: 69.26%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 3/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.871101 Acc: 71.875%\n",
      "   Batch 100/391 Loss: 1.032250 Acc: 66.700%\n",
      "   Batch 200/391 Loss: 0.813100 Acc: 67.289%\n",
      "   Batch 300/391 Loss: 0.986809 Acc: 67.655%\n",
      "    plane:  81.00%\n",
      "      car:  80.70%\n",
      "     bird:  60.30%\n",
      "      cat:  64.60%\n",
      "     deer:  67.50%\n",
      "      dog:  55.90%\n",
      "     frog:  79.00%\n",
      "    horse:  73.80%\n",
      "     ship:  79.20%\n",
      "    truck:  76.20%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "ðŸ’¾ New best accuracy: 71.82%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 4/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.895362 Acc: 69.531%\n",
      "   Batch 100/391 Loss: 0.800278 Acc: 70.475%\n",
      "   Batch 200/391 Loss: 0.679087 Acc: 71.385%\n",
      "   Batch 300/391 Loss: 0.824153 Acc: 71.577%\n",
      "    plane:  77.50%\n",
      "      car:  85.60%\n",
      "     bird:  54.00%\n",
      "      cat:  43.70%\n",
      "     deer:  81.40%\n",
      "      dog:  67.70%\n",
      "     frog:  81.00%\n",
      "    horse:  71.50%\n",
      "     ship:  85.30%\n",
      "    truck:  88.60%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "ðŸ’¾ New best accuracy: 73.63%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 5/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.704246 Acc: 73.438%\n",
      "   Batch 100/391 Loss: 0.688872 Acc: 74.381%\n",
      "   Batch 200/391 Loss: 0.594784 Acc: 74.063%\n",
      "   Batch 300/391 Loss: 0.832205 Acc: 73.936%\n",
      "    plane:  83.50%\n",
      "      car:  83.10%\n",
      "     bird:  57.30%\n",
      "      cat:  51.20%\n",
      "     deer:  69.40%\n",
      "      dog:  61.90%\n",
      "     frog:  91.10%\n",
      "    horse:  78.70%\n",
      "     ship:  91.10%\n",
      "    truck:  87.70%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "ðŸ’¾ New best accuracy: 75.50%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 6/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.878003 Acc: 67.188%\n",
      "   Batch 100/391 Loss: 0.708619 Acc: 75.920%\n",
      "   Batch 200/391 Loss: 0.696740 Acc: 76.084%\n",
      "   Batch 300/391 Loss: 0.574407 Acc: 76.337%\n",
      "    plane:  74.90%\n",
      "      car:  87.40%\n",
      "     bird:  59.30%\n",
      "      cat:  46.60%\n",
      "     deer:  77.50%\n",
      "      dog:  65.00%\n",
      "     frog:  89.50%\n",
      "    horse:  86.00%\n",
      "     ship:  73.10%\n",
      "    truck:  86.30%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 7/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.559917 Acc: 82.031%\n",
      "   Batch 100/391 Loss: 0.715777 Acc: 78.326%\n",
      "   Batch 200/391 Loss: 0.651132 Acc: 78.109%\n",
      "   Batch 300/391 Loss: 0.766345 Acc: 78.128%\n",
      "    plane:  87.10%\n",
      "      car:  89.00%\n",
      "     bird:  66.70%\n",
      "      cat:  53.10%\n",
      "     deer:  77.40%\n",
      "      dog:  67.80%\n",
      "     frog:  83.20%\n",
      "    horse:  83.10%\n",
      "     ship:  81.00%\n",
      "    truck:  82.20%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "ðŸ’¾ New best accuracy: 77.06%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 8/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.563738 Acc: 79.688%\n",
      "   Batch 100/391 Loss: 0.489970 Acc: 80.453%\n",
      "   Batch 200/391 Loss: 0.533652 Acc: 80.340%\n",
      "   Batch 300/391 Loss: 0.472045 Acc: 79.913%\n",
      "    plane:  88.20%\n",
      "      car:  89.20%\n",
      "     bird:  53.50%\n",
      "      cat:  63.30%\n",
      "     deer:  86.10%\n",
      "      dog:  63.00%\n",
      "     frog:  85.90%\n",
      "    horse:  77.90%\n",
      "     ship:  77.70%\n",
      "    truck:  86.90%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "ðŸ’¾ New best accuracy: 77.17%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 9/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.427290 Acc: 87.500%\n",
      "   Batch 100/391 Loss: 0.562181 Acc: 81.289%\n",
      "   Batch 200/391 Loss: 0.587489 Acc: 81.297%\n",
      "   Batch 300/391 Loss: 0.544025 Acc: 81.128%\n",
      "    plane:  76.30%\n",
      "      car:  86.50%\n",
      "     bird:  70.20%\n",
      "      cat:  61.00%\n",
      "     deer:  71.20%\n",
      "      dog:  61.60%\n",
      "     frog:  91.00%\n",
      "    horse:  85.40%\n",
      "     ship:  91.80%\n",
      "    truck:  89.40%\n",
      "ðŸ“‰ Learning rate: 0.001000\n",
      "ðŸ’¾ New best accuracy: 78.44%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 10/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.362309 Acc: 86.719%\n",
      "   Batch 100/391 Loss: 0.585671 Acc: 82.379%\n",
      "   Batch 200/391 Loss: 0.466719 Acc: 82.303%\n",
      "   Batch 300/391 Loss: 0.482172 Acc: 82.306%\n",
      "    plane:  85.10%\n",
      "      car:  86.80%\n",
      "     bird:  63.70%\n",
      "      cat:  58.70%\n",
      "     deer:  80.30%\n",
      "      dog:  69.90%\n",
      "     frog:  88.80%\n",
      "    horse:  82.70%\n",
      "     ship:  85.60%\n",
      "    truck:  87.70%\n",
      "ðŸ“‰ Learning rate: 0.000500\n",
      "ðŸ’¾ New best accuracy: 78.93%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 11/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.435945 Acc: 84.375%\n",
      "   Batch 100/391 Loss: 0.441853 Acc: 85.999%\n",
      "   Batch 200/391 Loss: 0.345610 Acc: 86.233%\n",
      "   Batch 300/391 Loss: 0.410393 Acc: 86.275%\n",
      "    plane:  86.00%\n",
      "      car:  88.90%\n",
      "     bird:  70.60%\n",
      "      cat:  64.10%\n",
      "     deer:  79.70%\n",
      "      dog:  66.30%\n",
      "     frog:  79.40%\n",
      "    horse:  90.60%\n",
      "     ship:  88.40%\n",
      "    truck:  88.40%\n",
      "ðŸ“‰ Learning rate: 0.000500\n",
      "ðŸ’¾ New best accuracy: 80.24%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 12/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.315263 Acc: 86.719%\n",
      "   Batch 100/391 Loss: 0.428241 Acc: 87.925%\n",
      "   Batch 200/391 Loss: 0.302481 Acc: 88.056%\n",
      "   Batch 300/391 Loss: 0.435804 Acc: 87.897%\n",
      "    plane:  82.60%\n",
      "      car:  91.70%\n",
      "     bird:  62.00%\n",
      "      cat:  67.00%\n",
      "     deer:  77.20%\n",
      "      dog:  71.80%\n",
      "     frog:  87.80%\n",
      "    horse:  86.30%\n",
      "     ship:  92.30%\n",
      "    truck:  83.10%\n",
      "ðŸ“‰ Learning rate: 0.000500\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 13/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.270370 Acc: 91.406%\n",
      "   Batch 100/391 Loss: 0.317082 Acc: 89.581%\n",
      "   Batch 200/391 Loss: 0.407945 Acc: 89.206%\n",
      "   Batch 300/391 Loss: 0.334534 Acc: 89.122%\n",
      "    plane:  85.80%\n",
      "      car:  83.20%\n",
      "     bird:  66.00%\n",
      "      cat:  69.90%\n",
      "     deer:  75.20%\n",
      "      dog:  63.50%\n",
      "     frog:  83.10%\n",
      "    horse:  86.60%\n",
      "     ship:  93.60%\n",
      "    truck:  89.00%\n",
      "ðŸ“‰ Learning rate: 0.000500\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 14/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.317042 Acc: 89.062%\n",
      "   Batch 100/391 Loss: 0.240992 Acc: 90.238%\n",
      "   Batch 200/391 Loss: 0.370824 Acc: 89.731%\n",
      "   Batch 300/391 Loss: 0.296443 Acc: 89.763%\n",
      "    plane:  86.50%\n",
      "      car:  89.40%\n",
      "     bird:  73.20%\n",
      "      cat:  64.80%\n",
      "     deer:  78.10%\n",
      "      dog:  69.20%\n",
      "     frog:  82.10%\n",
      "    horse:  85.30%\n",
      "     ship:  86.10%\n",
      "    truck:  88.20%\n",
      "ðŸ“‰ Learning rate: 0.000500\n",
      "ðŸ’¾ New best accuracy: 80.29%\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ“ˆ EPOCH 15/15\n",
      "------------------------------------------------------------\n",
      "   Batch   0/391 Loss: 0.216514 Acc: 92.969%\n",
      "   Batch 100/391 Loss: 0.241225 Acc: 90.695%\n",
      "   Batch 200/391 Loss: 0.356061 Acc: 90.652%\n",
      "   Batch 300/391 Loss: 0.241974 Acc: 90.485%\n",
      "    plane:  84.00%\n",
      "      car:  89.80%\n",
      "     bird:  63.00%\n",
      "      cat:  52.10%\n",
      "     deer:  78.70%\n",
      "      dog:  81.60%\n",
      "     frog:  86.30%\n",
      "    horse:  87.00%\n",
      "     ship:  86.80%\n",
      "    truck:  88.40%\n",
      "ðŸ“‰ Learning rate: 0.000500\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸŽ‰ BASELINE Training Completed (NO AUGMENTATION)!\n",
      "============================================================\n",
      "ðŸ† Best test accuracy: 80.29%\n",
      "â±ï¸ Total training time: 61.9s\n",
      "ðŸ“Š Average time per epoch: 4.1s\n",
      "\n",
      "ðŸ“Š Baseline Performance Analysis:\n",
      "   Python baseline (no aug): 80.29%\n",
      "   Fortran current (no aug): ~58.7%\n",
      "   Gap to close: 21.6 percentage points\n",
      "âœ… GOOD: Baseline shows Python can achieve higher accuracy\n",
      "   The gap is likely in training implementation, not augmentation\n",
      "\n",
      "ðŸ“‹ Summary:\n",
      "   Python baseline (no augmentation): 80.29%\n",
      "   Use this to compare with Fortran's 58.7% no-augmentation result\n",
      "   Any remaining gap indicates fundamental training differences\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CIFAR-10 Baseline Test - NO AUGMENTATION\n",
    "========================================\n",
    "Modified from cifar10-7.ipynb to remove all data augmentation\n",
    "This will establish the Python baseline accuracy without augmentation\n",
    "for direct comparison with Fortran implementation.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import version 7 TensorCore\n",
    "try:\n",
    "    from tensor_core_wrapper7 import TensorCore, TensorCoreLinear, get_global_tensorcore\n",
    "    print(\"âœ… Imported tensor_core_wrapper7 with TensorCoreLinear\")\n",
    "    TENSORCORE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âŒ Could not import tensor_core_wrapper7!\")\n",
    "    print(\"   Please ensure tensor_core_wrapper7 is available\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "CIFAR10_CLASSES = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def set_debug_mode(enabled):\n",
    "    \"\"\"Set debug mode for TensorCore operations\"\"\"\n",
    "    try:\n",
    "        tc = TensorCore(debug_mode=enabled)\n",
    "        print(f\"ðŸ”§ TensorCore debug mode: {'enabled' if enabled else 'disabled'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not set debug mode: {e}\")\n",
    "\n",
    "def load_cifar10_data_no_augmentation(batch_size=128, num_workers=4):\n",
    "    \"\"\"\n",
    "    Load CIFAR-10 with NO AUGMENTATION - only normalization\n",
    "    \"\"\"\n",
    "    print(\"ðŸ“ Loading CIFAR-10 dataset with NO AUGMENTATION...\")\n",
    "    \n",
    "    # NO AUGMENTATION - only normalization for both train and test\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    # Same transform for test (no augmentation needed anyway)\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=train_transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=test_transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False, \n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… CIFAR-10 loaded with NO AUGMENTATION:\")\n",
    "    print(f\"   Training samples: {len(train_dataset):,}\")\n",
    "    print(f\"   Test samples: {len(test_dataset):,}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Only normalization applied: (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\")\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL ARCHITECTURE - Same as Fortran version\n",
    "# =============================================================================\n",
    "\n",
    "class CIFAR10NetTensorCoreBaseline(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN model matching the Fortran architecture exactly\n",
    "    No augmentation dependency - baseline comparison\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CIFAR10NetTensorCoreBaseline, self).__init__()\n",
    "        \n",
    "        # Convolutional layers (matching Fortran)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Calculate feature size after conv layers: 32->16->8->4, 4*4*128 = 2048\n",
    "        self.feature_size = 4 * 4 * 128\n",
    "        \n",
    "        # Fully connected layers using TensorCore\n",
    "        self.fc1 = TensorCoreLinear(self.feature_size, 512)\n",
    "        self.fc2 = TensorCoreLinear(512, 256)  \n",
    "        self.fc3 = TensorCoreLinear(256, 10)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv block 1: Conv -> BN -> LeakyReLU -> Pool\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        x = self.pool(x)  # 32x32 -> 16x16\n",
    "        \n",
    "        # Conv block 2: Conv -> BN -> LeakyReLU -> Pool  \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        x = self.pool(x)  # 16x16 -> 8x8\n",
    "        \n",
    "        # Conv block 3: Conv -> BN -> LeakyReLU -> Pool\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        x = self.pool(x)  # 8x8 -> 4x4\n",
    "        \n",
    "        # Flatten and FC layers\n",
    "        x = torch.flatten(x, 1)  # Shape: (batch_size, 2048)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, negative_slope=0.01)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device, epoch):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        # Progress update every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'   Batch {batch_idx:3d}/{len(train_loader)} '\n",
    "                  f'Loss: {loss.item():.6f} '\n",
    "                  f'Acc: {100.*correct/total:.3f}%')\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, epoch_time\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = [0] * 10\n",
    "    class_total = [0] * 10\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            c = (predicted == target).squeeze()\n",
    "            for i in range(target.size(0)):\n",
    "                label = target[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    # Calculate per-class accuracies\n",
    "    class_accuracies = []\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            class_acc = 100. * class_correct[i] / class_total[i]\n",
    "            class_accuracies.append(class_acc)\n",
    "            print(f'   {CIFAR10_CLASSES[i]:>6}: {class_acc:6.2f}%')\n",
    "        else:\n",
    "            class_accuracies.append(0.0)\n",
    "    \n",
    "    return test_loss, accuracy, 0, class_accuracies  # 0 for eval_time compatibility\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "def main_baseline_training(num_epochs=15, batch_size=128, lr=0.001):\n",
    "    \"\"\"\n",
    "    Main training function - NO AUGMENTATION BASELINE\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ CIFAR-10 BASELINE TRAINING - NO AUGMENTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Disable TensorCore debug for speed\n",
    "    set_debug_mode(False)\n",
    "    \n",
    "    # Load data WITHOUT augmentation\n",
    "    train_loader, test_loader = load_cifar10_data_no_augmentation(batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model (same architecture as successful version 6)\n",
    "    torch.manual_seed(42)  # Reproducibility\n",
    "    model = CIFAR10NetTensorCoreBaseline().to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nðŸ“Š Model Information:\")\n",
    "    print(f\"   Parameters: {params:,}\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Learning rate: {lr}\")\n",
    "    print(f\"   Architecture: Same as Fortran version\")\n",
    "    print(f\"   Augmentation: NONE (baseline test)\")\n",
    "    \n",
    "    # Optimizer and scheduler (matching Python version but no augmentation)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'test_loss': [], \n",
    "        'test_acc': [], 'epoch_times': []\n",
    "    }\n",
    "    \n",
    "    best_test_acc = 0.0\n",
    "    total_time = 0.0\n",
    "    \n",
    "    print(f\"\\nðŸ‹ï¸ Starting Baseline Training (NO AUGMENTATION)...\")\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\nðŸ“ˆ EPOCH {epoch}/{num_epochs}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Training\n",
    "        train_loss, train_acc, epoch_time = train_epoch(\n",
    "            model, train_loader, optimizer, criterion, device, epoch\n",
    "        )\n",
    "        \n",
    "        # Evaluation\n",
    "        test_loss, test_acc, eval_time, class_accs = evaluate_model(\n",
    "            model, test_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Learning rate step\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"ðŸ“‰ Learning rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Track best performance\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            print(f\"ðŸ’¾ New best accuracy: {best_test_acc:.2f}%\")\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['epoch_times'].append(epoch_time)\n",
    "        \n",
    "        total_time += epoch_time\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # Final results\n",
    "    print(f\"\\nðŸŽ‰ BASELINE Training Completed (NO AUGMENTATION)!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ðŸ† Best test accuracy: {best_test_acc:.2f}%\")\n",
    "    print(f\"â±ï¸ Total training time: {total_time:.1f}s\")\n",
    "    print(f\"ðŸ“Š Average time per epoch: {total_time/num_epochs:.1f}s\")\n",
    "    \n",
    "    # Compare with expected results\n",
    "    print(f\"\\nðŸ“Š Baseline Performance Analysis:\")\n",
    "    print(f\"   Python baseline (no aug): {best_test_acc:.2f}%\")\n",
    "    print(f\"   Fortran current (no aug): ~58.7%\")\n",
    "    print(f\"   Gap to close: {abs(best_test_acc - 58.7):.1f} percentage points\")\n",
    "    \n",
    "    if best_test_acc > 65.0:\n",
    "        print(\"âœ… GOOD: Baseline shows Python can achieve higher accuracy\")\n",
    "        print(\"   The gap is likely in training implementation, not augmentation\")\n",
    "    elif best_test_acc < 60.0:\n",
    "        print(\"âš ï¸ CONCERNING: Python baseline is also low\")\n",
    "        print(\"   This suggests model architecture or hyperparameter issues\")\n",
    "    else:\n",
    "        print(\"ðŸ“Š COMPARABLE: Python and Fortran baselines are similar\")\n",
    "        print(\"   Focus should be on augmentation implementation\")\n",
    "    \n",
    "    return history, best_test_acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run baseline test\n",
    "    print(\"ðŸ§ª Running CIFAR-10 Baseline Test (NO AUGMENTATION)\")\n",
    "    print(\"This will establish the Python baseline for comparison\")\n",
    "    print()\n",
    "    \n",
    "    history, best_acc = main_baseline_training(num_epochs=15, batch_size=128, lr=0.001)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Summary:\")\n",
    "    print(f\"   Python baseline (no augmentation): {best_acc:.2f}%\")\n",
    "    print(f\"   Use this to compare with Fortran's 58.7% no-augmentation result\")\n",
    "    print(f\"   Any remaining gap indicates fundamental training differences\")\n",
    "# works with py313 but crashes with py314!\n",
    "# needs tensor_core_gemm7b.cuf to work (7c and 7d have errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc1929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Python cuDNN BatchNorm Reference Implementation\n",
      "============================================================\n",
      "\n",
      "ðŸ”¬ Testing BatchNorm in TRAINING mode\n",
      "==================================================\n",
      "ðŸ“Š Input shape: torch.Size([128, 32, 32, 32])\n",
      "ðŸ“Š Input mean: -0.000438\n",
      "ðŸ“Š Input std: 1.000164\n",
      "\n",
      "ðŸ“‹ INITIAL BatchNorm Parameters:\n",
      "   Weight (gamma) - first 5: [1. 1. 1. 1. 1.]\n",
      "   Bias (beta) - first 5: [0. 0. 0. 0. 0.]\n",
      "   Running mean - first 5: [0. 0. 0. 0. 0.]\n",
      "   Running var - first 5: [1. 1. 1. 1. 1.]\n",
      "   Momentum: 0.1\n",
      "   Epsilon: 1e-05\n",
      "\n",
      "ðŸ”„ Performing forward pass...\n",
      "ðŸ“Š Output shape: torch.Size([128, 32, 32, 32])\n",
      "ðŸ“Š Output mean: -0.000000\n",
      "ðŸ“Š Output std: 0.999995\n",
      "\n",
      "ðŸ“‹ UPDATED BatchNorm Parameters (after training forward pass):\n",
      "   Running mean - first 5: [-2.18730528e-04 -4.03176960e-04  1.52532919e-04  3.58769088e-04\n",
      "  5.86154079e-06]\n",
      "   Running var - first 5: [0.99982336 1.00089806 0.9994461  0.99983228 0.99973512]\n",
      "\n",
      "ðŸ“‹ COMPUTED Batch Statistics (for verification):\n",
      "   Batch mean - first 5: [-2.18730528e-03 -4.03176960e-03  1.52532919e-03  3.58769088e-03\n",
      "  5.86154079e-05]\n",
      "   Batch var - first 5: [0.99822603 1.00897291 0.99445344 0.99831517 0.99734355]\n",
      "\n",
      "ðŸ” MOMENTUM FORMULA VERIFICATION:\n",
      "   Expected running mean - first 5: [-2.18730528e-04 -4.03176960e-04  1.52532919e-04  3.58769088e-04\n",
      "  5.86154079e-06]\n",
      "   Actual running mean - first 5: [-2.18730528e-04 -4.03176960e-04  1.52532919e-04  3.58769088e-04\n",
      "  5.86154079e-06]\n",
      "   Match: True\n",
      "\n",
      "   Expected running var - first 5: [0.9998226  1.00089729 0.99944534 0.99983152 0.99973436]\n",
      "   Actual running var - first 5: [0.99982336 1.00089806 0.9994461  0.99983228 0.99973512]\n",
      "   Match: True\n",
      "\n",
      "\n",
      "============================================================\n",
      "\n",
      "ðŸ”¬ Testing BatchNorm in INFERENCE mode\n",
      "==================================================\n",
      "ðŸ“‹ Running Statistics BEFORE inference:\n",
      "   Running mean - first 5: [-2.18730528e-04 -4.03176960e-04  1.52532919e-04  3.58769088e-04\n",
      "  5.86154079e-06]\n",
      "   Running var - first 5: [0.99982336 1.00089806 0.9994461  0.99983228 0.99973512]\n",
      "\n",
      "ðŸ“‹ Running Statistics AFTER inference:\n",
      "   Running mean - first 5: [-2.18730528e-04 -4.03176960e-04  1.52532919e-04  3.58769088e-04\n",
      "  5.86154079e-06]\n",
      "   Running var - first 5: [0.99982336 1.00089806 0.9994461  0.99983228 0.99973512]\n",
      "\n",
      "âœ… Running mean unchanged: True\n",
      "âœ… Running var unchanged: True\n",
      "\n",
      "============================================================\n",
      "ðŸ’¾ Reference data saved to 'batchnorm_reference_data.json'\n",
      "   Use this file to compare with your Fortran implementation!\n",
      "\n",
      "ðŸŽ¯ KEY FINDINGS:\n",
      "   1. PyTorch uses BIASED variance in training\n",
      "   2. Momentum formula: new_running = (1-momentum)*old_running + momentum*batch\n",
      "   3. Running statistics are NOT updated in eval mode\n",
      "   4. Default eps=1e-05, momentum=0.1\n",
      "\n",
      "âœ… Compare these results with your Fortran implementation!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Python cuDNN BatchNorm Reference Implementation\n",
    "Compare exact cuDNN batch normalization behavior vs your Fortran implementation\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "def create_test_data():\n",
    "    \"\"\"Create identical test data for both Python and Fortran\"\"\"\n",
    "    \n",
    "    # Set fixed seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Test parameters matching your Fortran setup\n",
    "    batch_size = 128\n",
    "    channels = 32\n",
    "    height = 32 \n",
    "    width = 32\n",
    "    \n",
    "    # Create test input (same as CIFAR-10 dimensions after first conv)\n",
    "    input_data = torch.randn(batch_size, channels, height, width, dtype=torch.float64).cuda()\n",
    "    \n",
    "    # Create BatchNorm layer with exact PyTorch defaults\n",
    "    bn_layer = nn.BatchNorm2d(\n",
    "        num_features=channels,\n",
    "        eps=1e-05,           # PyTorch default\n",
    "        momentum=0.1,        # PyTorch default  \n",
    "        affine=True,         # learnable parameters\n",
    "        track_running_stats=True\n",
    "    ).cuda().double()  # Use double precision to match Fortran\n",
    "    \n",
    "    return input_data, bn_layer\n",
    "\n",
    "def extract_batchnorm_parameters(bn_layer):\n",
    "    \"\"\"Extract all BatchNorm parameters for comparison\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        'weight': bn_layer.weight.detach().cpu().numpy(),\n",
    "        'bias': bn_layer.bias.detach().cpu().numpy(),\n",
    "        'running_mean': bn_layer.running_mean.detach().cpu().numpy(),\n",
    "        'running_var': bn_layer.running_var.detach().cpu().numpy(),\n",
    "        'eps': bn_layer.eps,\n",
    "        'momentum': bn_layer.momentum,\n",
    "        'num_batches_tracked': bn_layer.num_batches_tracked.item()\n",
    "    }\n",
    "    \n",
    "    return params\n",
    "\n",
    "def test_training_mode_batchnorm():\n",
    "    \"\"\"Test BatchNorm in training mode (updates running statistics)\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”¬ Testing BatchNorm in TRAINING mode\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    input_data, bn_layer = create_test_data()\n",
    "    \n",
    "    # Ensure training mode\n",
    "    bn_layer.train()\n",
    "    \n",
    "    print(f\"ðŸ“Š Input shape: {input_data.shape}\")\n",
    "    print(f\"ðŸ“Š Input mean: {input_data.mean().item():.6f}\")\n",
    "    print(f\"ðŸ“Š Input std: {input_data.std().item():.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Get initial parameters\n",
    "    print(\"ðŸ“‹ INITIAL BatchNorm Parameters:\")\n",
    "    initial_params = extract_batchnorm_parameters(bn_layer)\n",
    "    print(f\"   Weight (gamma) - first 5: {initial_params['weight'][:5]}\")\n",
    "    print(f\"   Bias (beta) - first 5: {initial_params['bias'][:5]}\")\n",
    "    print(f\"   Running mean - first 5: {initial_params['running_mean'][:5]}\")\n",
    "    print(f\"   Running var - first 5: {initial_params['running_var'][:5]}\")\n",
    "    print(f\"   Momentum: {initial_params['momentum']}\")\n",
    "    print(f\"   Epsilon: {initial_params['eps']}\")\n",
    "    print()\n",
    "    \n",
    "    # Forward pass\n",
    "    print(\"ðŸ”„ Performing forward pass...\")\n",
    "    output = bn_layer(input_data)\n",
    "    \n",
    "    print(f\"ðŸ“Š Output shape: {output.shape}\")\n",
    "    print(f\"ðŸ“Š Output mean: {output.mean().item():.6f}\")\n",
    "    print(f\"ðŸ“Š Output std: {output.std().item():.6f}\")\n",
    "    print()\n",
    "    \n",
    "    # Get updated parameters\n",
    "    print(\"ðŸ“‹ UPDATED BatchNorm Parameters (after training forward pass):\")\n",
    "    updated_params = extract_batchnorm_parameters(bn_layer)\n",
    "    print(f\"   Running mean - first 5: {updated_params['running_mean'][:5]}\")\n",
    "    print(f\"   Running var - first 5: {updated_params['running_var'][:5]}\")\n",
    "    print()\n",
    "    \n",
    "    # Compute batch statistics for comparison\n",
    "    batch_mean = input_data.mean(dim=[0, 2, 3])  # Average over batch, height, width\n",
    "    batch_var = input_data.var(dim=[0, 2, 3], unbiased=False)  # Biased variance\n",
    "    \n",
    "    print(\"ðŸ“‹ COMPUTED Batch Statistics (for verification):\")\n",
    "    print(f\"   Batch mean - first 5: {batch_mean[:5].cpu().numpy()}\")\n",
    "    print(f\"   Batch var - first 5: {batch_var[:5].cpu().numpy()}\")\n",
    "    print()\n",
    "    \n",
    "    # Verify the momentum update formula\n",
    "    expected_running_mean = (1 - initial_params['momentum']) * initial_params['running_mean'] + \\\n",
    "                           initial_params['momentum'] * batch_mean.cpu().numpy()\n",
    "    expected_running_var = (1 - initial_params['momentum']) * initial_params['running_var'] + \\\n",
    "                          initial_params['momentum'] * batch_var.cpu().numpy()\n",
    "    \n",
    "    print(\"ðŸ” MOMENTUM FORMULA VERIFICATION:\")\n",
    "    print(f\"   Expected running mean - first 5: {expected_running_mean[:5]}\")\n",
    "    print(f\"   Actual running mean - first 5: {updated_params['running_mean'][:5]}\")\n",
    "    print(f\"   Match: {np.allclose(expected_running_mean[:5], updated_params['running_mean'][:5], atol=1e-6)}\")\n",
    "    print()\n",
    "    print(f\"   Expected running var - first 5: {expected_running_var[:5]}\")\n",
    "    print(f\"   Actual running var - first 5: {updated_params['running_var'][:5]}\")\n",
    "    print(f\"   Match: {np.allclose(expected_running_var[:5], updated_params['running_var'][:5], atol=1e-6)}\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'input': input_data.detach().cpu().numpy(),\n",
    "        'output': output.detach().cpu().numpy(),\n",
    "        'initial_params': initial_params,\n",
    "        'updated_params': updated_params,\n",
    "        'batch_mean': batch_mean.detach().cpu().numpy(),\n",
    "        'batch_var': batch_var.detach().cpu().numpy()\n",
    "    }\n",
    "\n",
    "def test_inference_mode_batchnorm():\n",
    "    \"\"\"Test BatchNorm in inference mode (uses running statistics)\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”¬ Testing BatchNorm in INFERENCE mode\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    input_data, bn_layer = create_test_data()\n",
    "    \n",
    "    # First, do a training pass to populate running statistics\n",
    "    bn_layer.train()\n",
    "    _ = bn_layer(input_data)\n",
    "    \n",
    "    # Now switch to eval mode\n",
    "    bn_layer.eval()\n",
    "    \n",
    "    # Get running statistics before inference\n",
    "    before_params = extract_batchnorm_parameters(bn_layer)\n",
    "    print(\"ðŸ“‹ Running Statistics BEFORE inference:\")\n",
    "    print(f\"   Running mean - first 5: {before_params['running_mean'][:5]}\")\n",
    "    print(f\"   Running var - first 5: {before_params['running_var'][:5]}\")\n",
    "    print()\n",
    "    \n",
    "    # Forward pass in inference mode\n",
    "    output = bn_layer(input_data)\n",
    "    \n",
    "    # Get running statistics after inference\n",
    "    after_params = extract_batchnorm_parameters(bn_layer)\n",
    "    print(\"ðŸ“‹ Running Statistics AFTER inference:\")\n",
    "    print(f\"   Running mean - first 5: {after_params['running_mean'][:5]}\")\n",
    "    print(f\"   Running var - first 5: {after_params['running_var'][:5]}\")\n",
    "    print()\n",
    "    \n",
    "    # Verify they didn't change\n",
    "    mean_unchanged = np.allclose(before_params['running_mean'], after_params['running_mean'])\n",
    "    var_unchanged = np.allclose(before_params['running_var'], after_params['running_var'])\n",
    "    \n",
    "    print(f\"âœ… Running mean unchanged: {mean_unchanged}\")\n",
    "    print(f\"âœ… Running var unchanged: {var_unchanged}\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'input': input_data.detach().cpu().numpy(),\n",
    "        'output': output.detach().cpu().numpy(),\n",
    "        'running_stats': after_params\n",
    "    }\n",
    "\n",
    "def save_reference_data(training_result, inference_result):\n",
    "    \"\"\"Save reference data for Fortran comparison\"\"\"\n",
    "    \n",
    "    reference_data = {\n",
    "        'training_mode': {\n",
    "            'input_shape': training_result['input'].shape,\n",
    "            'input_sample': training_result['input'][:2, :5, :5, :5].tolist(),  # Small sample for verification\n",
    "            'output_sample': training_result['output'][:2, :5, :5, :5].tolist(),\n",
    "            'batch_statistics': {\n",
    "                'mean': training_result['batch_mean'].tolist(),\n",
    "                'var': training_result['batch_var'].tolist()\n",
    "            },\n",
    "            'running_statistics_before': {\n",
    "                'mean': training_result['initial_params']['running_mean'].tolist(),\n",
    "                'var': training_result['initial_params']['running_var'].tolist()\n",
    "            },\n",
    "            'running_statistics_after': {\n",
    "                'mean': training_result['updated_params']['running_mean'].tolist(),\n",
    "                'var': training_result['updated_params']['running_var'].tolist()\n",
    "            },\n",
    "            'parameters': {\n",
    "                'weight': training_result['initial_params']['weight'].tolist(),\n",
    "                'bias': training_result['initial_params']['bias'].tolist(),\n",
    "                'eps': training_result['initial_params']['eps'],\n",
    "                'momentum': training_result['initial_params']['momentum']\n",
    "            }\n",
    "        },\n",
    "        'inference_mode': {\n",
    "            'input_sample': inference_result['input'][:2, :5, :5, :5].tolist(),\n",
    "            'output_sample': inference_result['output'][:2, :5, :5, :5].tolist(),\n",
    "            'running_statistics': {\n",
    "                'mean': inference_result['running_stats']['running_mean'].tolist(),\n",
    "                'var': inference_result['running_stats']['running_var'].tolist()\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('batchnorm_reference_data.json', 'w') as f:\n",
    "        json.dump(reference_data, f, indent=2)\n",
    "    \n",
    "    print(\"ðŸ’¾ Reference data saved to 'batchnorm_reference_data.json'\")\n",
    "    print(\"   Use this file to compare with your Fortran implementation!\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main test function\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Python cuDNN BatchNorm Reference Implementation\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Test training mode\n",
    "    training_result = test_training_mode_batchnorm()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    # Test inference mode  \n",
    "    inference_result = test_inference_mode_batchnorm()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Save reference data for Fortran comparison\n",
    "    save_reference_data(training_result, inference_result)\n",
    "    \n",
    "    print()\n",
    "    print(\"ðŸŽ¯ KEY FINDINGS:\")\n",
    "    print(\"   1. PyTorch uses BIASED variance in training\")\n",
    "    print(\"   2. Momentum formula: new_running = (1-momentum)*old_running + momentum*batch\")\n",
    "    print(\"   3. Running statistics are NOT updated in eval mode\")\n",
    "    print(\"   4. Default eps=1e-05, momentum=0.1\")\n",
    "    print()\n",
    "    print(\"âœ… Compare these results with your Fortran implementation!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
