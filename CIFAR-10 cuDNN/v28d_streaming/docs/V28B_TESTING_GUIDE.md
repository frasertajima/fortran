# v28b Testing Guide - 10M Sample (114GB) Dataset

## Quick Start

Updated config to **10M samples (114GB)** to match your generated dataset!

### 1. Recompile (Required!)

```bash
cd v28b_managed/datasets/cifar10/
bash compile_cifar10.sh
```

This creates `cifar10_train_large` binary using the 10M sample config.

### 2. Run Training (This is the Big One!)

```bash
./cifar10_train_large
```

**Expected behavior:**
- Loading message: "Training samples: 10000000"
- RAM usage: ~120GB system RAM (for loading/copying)
- GPU RAM: Still ~2GB (constant!)
- Training time: **~8-10 minutes** (200√ó slower than 50K baseline)
  - 15 epochs √ó 10M samples √∑ 128 batch = 1.17M batches
  - ~0.5ms per batch = 585s ‚âà 10 minutes

**Not expected:**
- ‚ùå 29s training (that's only 50K samples!)
- ‚ùå 2.8GB total RAM (should use ~120GB)

### 3. Verify You're Using the Large Dataset

Check the startup output:

```
======================================================================
Loading Large Synthetic Dataset (v28b - Managed Memory)
======================================================================
Dataset configuration:
  Training samples: 10000000  ‚Üê Should say 10M, not 1M or 50K!
  Test samples:      10000
  Dataset size:       114.44  GB
  Memory mode:      MANAGED (can exceed GPU RAM!)
```

### 4. Monitor During Training

```bash
# In another terminal, watch memory:
watch -n 1 nvidia-smi

# Expected GPU RAM: ~2 GB (constant, regardless of 114GB dataset!)
```

## Troubleshooting

### Problem: Training only takes 29s

**You ran the wrong binary!**

```bash
# Wrong (50K samples):
./cifar10_train

# Correct (10M samples):
./cifar10_train_large
```

### Problem: Loading says "1000000 samples" not "10000000"

**Config didn't update or didn't recompile.**

```bash
# Check config:
grep "train_samples =" cifar10_config_large.cuf
# Should show: train_samples = 10000000

# Recompile:
bash compile_cifar10.sh
```

### Problem: "Cannot open images_train.bin"

**Dataset in wrong location.**

```bash
# The binary looks for: cifar10_data/images_train.bin
# Make sure you're running from: v28b_managed/datasets/cifar10/
pwd

# And that data exists:
ls -lh cifar10_data/
```

## Expected Results

With 10M samples (114GB dataset):

- **GPU RAM:** ~2 GB (constant! That's the whole point!)
- **System RAM:** ~120GB (dataset lives here)
- **Training time:** ~8-10 minutes (15 epochs)
- **Accuracy:** ~79% (same as baseline)
- **Performance:** 100% (zero penalty!)

This proves the breakthrough: **114GB dataset trains with only 2GB GPU RAM at full speed!** üöÄ

## What About cifar10_train (without "_large")?

That binary uses `cifar10_config.cuf` (50K samples). It's the baseline for comparison:
- 50K samples, 29s, 79% accuracy
- Only 2GB GPU RAM

Your 10M sample training should be **200√ó slower** (10M √∑ 50K = 200√ó) but still only use **2GB GPU RAM!**
