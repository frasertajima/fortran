# v28 Baseline Plus Export - Project Roadmap

## Current Status (November 2025)

### ‚úÖ Completed Features

**Core Framework:**
- ‚úÖ Modular architecture (70% code reduction: 12K ‚Üí 1.5K lines)
- ‚úÖ 4 datasets fully integrated: CIFAR-10, CIFAR-100, Fashion-MNIST, SVHN
- ‚úÖ **4√ó faster than PyTorch** (with persistent memory pools)
- ‚úÖ Complete Fortran‚ÜíPython export pipeline
- ‚úÖ Jupyter notebook inference for all 4 datasets
- ‚úÖ Comprehensive documentation (1,500+ lines)

**Performance:**
- ‚úÖ GPU-only batch extraction (eliminates 75,000+ CPU‚ÜîGPU transfers)
- ‚úÖ Blocking synchronization (5-10% CPU usage vs 100%)
- ‚úÖ Persistent memory pools (achieved 4√ó speedup)
- ‚úÖ 85-95% GPU utilization (compute-bound)

**Export System:**
- ‚úÖ Binary weight export (19 files per model)
- ‚úÖ PyTorch model loader with memory layout conversion
- ‚úÖ Accuracy preservation (training: 79%, PyTorch inference: 81%)
- ‚úÖ Dataset-specific inference notebooks with visualizations

### üü° Partially Complete

**Oxford Flowers 102:**
- ‚úÖ Implemented in v28_baseline (dense layer only)
- ‚úÖ 78.94% accuracy, 1.2s training time
- ‚ùå **Not yet integrated into v28_baseline_plus_export**
- ‚ùå **No export functionality**
- ‚ùå **No inference notebook**

---

## Missing Features & Improvements

### Priority 1: Production-Critical Features

#### 1. **Data Augmentation** üéØ
**Status:** Intentionally disabled for benchmarking
**Impact:** Could improve CIFAR-10 accuracy from ~79% to 85%+

**Implementation Plan:**
- Random crops, horizontal flips, rotations
- Color jittering for RGB datasets
- GPU-based augmentation (faster than CPU)
- Configurable per-dataset

**Effort:** 2-3 days
**Files to modify:**
- `common/data_augmentation.cuf` (new module)
- `datasets/*/config.cuf` (enable/disable flags)

**Trade-offs:**
- ‚úÖ Better accuracy and generalization
- ‚úÖ More realistic ML workflow
- ‚ùå Slightly slower training (minimal with GPU augmentation)
- ‚ùå Harder to compare with PyTorch baseline (need to match augmentations)

---

#### 2. **Model Checkpointing & Validation** üéØ
**Status:** Missing
**Impact:** Production necessity for real ML workflows

**Implementation Plan:**
- Save best model based on validation accuracy
- Early stopping to prevent overfitting
- Resume training from checkpoint
- Validation split (e.g., 10% of training data)

**Effort:** 3-4 days
**Files to create:**
- `common/checkpoint.cuf` - Checkpoint save/load
- `common/early_stopping.cuf` - Early stopping logic

**Files to modify:**
- `datasets/*/main.cuf` - Add validation loop

**Trade-offs:**
- ‚úÖ Prevents overfitting
- ‚úÖ Can resume long training runs
- ‚úÖ Automatic best model selection
- ‚ùå Slightly more complex training loop
- ‚ùå Requires validation data split

---

#### 3. **Learning Rate Scheduling** üéØ
**Status:** Intentionally disabled for benchmarking
**Impact:** 2-5% accuracy improvement on all datasets

**Implementation Plan:**
- Cosine annealing with warmup
- Step decay (reduce LR every N epochs)
- Plateau detection (reduce on validation plateau)
- Configurable per-dataset

**Effort:** 2-3 days
**Files to create:**
- `common/lr_scheduler.cuf` - Scheduler implementations

**Files to modify:**
- `datasets/*/main.cuf` - Update learning rate each epoch

**Trade-offs:**
- ‚úÖ Better convergence
- ‚úÖ Higher final accuracy
- ‚úÖ Industry standard practice
- ‚ùå More hyperparameters to tune
- ‚ùå Longer training time (more epochs needed)

---

### Priority 2: Usability & Developer Experience

#### 4. **YAML/JSON Configuration Files** üöÄ
**Status:** Currently using Fortran modules
**Impact:** Much easier to modify hyperparameters

**Implementation Plan:**
- Replace `*_config.cuf` with `config.yaml`
- Parse YAML in Python, generate Fortran parameters
- Store configs alongside trained models
- Version control for reproducibility

**Example Config:**
```yaml
dataset: cifar10
model:
  conv_filters: [32, 64, 128]
  fc_units: [512, 256]
  dropout: 0.0
training:
  epochs: 15
  batch_size: 128
  learning_rate: 0.001
  optimizer: adam
  lr_schedule:
    type: cosine
    warmup_epochs: 2
augmentation:
  enabled: true
  horizontal_flip: true
  random_crop: true
  color_jitter: true
```

**Effort:** 4-5 days
**Files to create:**
- `common/config_parser.py` - Parse YAML ‚Üí Fortran
- `configs/*.yaml` - Per-dataset configs

**Files to modify:**
- `datasets/*/compile_*.sh` - Generate config from YAML

**Trade-offs:**
- ‚úÖ Much easier to experiment with hyperparameters
- ‚úÖ Better reproducibility
- ‚úÖ Config versioning with git
- ‚úÖ No recompilation needed for hyperparameter changes
- ‚ùå Adds Python dependency to build process
- ‚ùå More complex build system

---

#### 5. **Generic Training Binary** üí°
**Status:** Phase 4 vision (not started)
**Impact:** Massive UX improvement

**Implementation Plan:**
```bash
# Current workflow (requires recompilation)
cd datasets/cifar10
nvfortran cifar10_main.cuf ...  # 30+ seconds
./cifar10_train                 # 31 seconds training

# Proposed workflow (no recompilation)
./train_cnn --dataset=cifar10 --epochs=15 --lr=0.001
./train_cnn --config=configs/cifar10_experiment.yaml
```

**Architecture:**
- Single executable for all datasets
- Runtime polymorphism (Fortran 2003 abstract types)
- Plugin system for dataset loaders
- Dynamic architecture configuration

**Effort:** 2-3 weeks (major refactor)
**Files to create:**
- `src/train_generic.cuf` - Main binary
- `common/dataset_interface.cuf` - Abstract dataset type
- `common/model_builder.cuf` - Dynamic model construction

---

### Priority 3: Advanced Features

#### 6. **ONNX Export** üåê
**Status:** Currently exports to PyTorch only
**Impact:** Deploy to any framework (TensorFlow, ONNX Runtime, etc.)

**Implementation Plan:**
- Generate ONNX graph from trained weights
- Support Conv, FC, BatchNorm, LeakyReLU, MaxPool layers
- Validate against PyTorch ONNX export
- Inference benchmarks (ONNX Runtime vs PyTorch)

**Effort:** 4-5 days
**Files to create:**
- `inference/onnx_exporter.py` - Convert weights ‚Üí ONNX

**Trade-offs:**
- ‚úÖ Framework-agnostic deployment
- ‚úÖ Optimized inference (ONNX Runtime)
- ‚úÖ Mobile/edge deployment ready
- ‚ùå ONNX has limitations for custom ops
- ‚ùå Additional testing burden

---

#### 7. **Mixed Precision Training (FP16)** ‚ö°
**Status:** Currently FP32 only
**Impact:** 2-3√ó speedup, 50% memory reduction

**Implementation Plan:**
- Use NVIDIA Tensor Cores (FP16 compute, FP32 accumulation)
- Loss scaling to prevent underflow
- Dynamic loss scaling for stability
- Benchmark accuracy impact

**Effort:** 1 week
**Files to create:**
- `common/mixed_precision.cuf` - FP16 utilities

**Files to modify:**
- `datasets/*/main.cuf` - FP16 training loop

**Trade-offs:**
- ‚úÖ 2-3√ó faster training
- ‚úÖ 50% less GPU memory
- ‚úÖ Can train larger models
- ‚ùå Numerical instability (need loss scaling)
- ‚ùå Accuracy might drop slightly (usually <0.5%)
- ‚ùå More complex to debug

---

#### 8. **Different CNN Architectures** üèóÔ∏è
**Status:** Fixed architecture (3 conv + 3 FC)
**Impact:** Support modern architectures

**Proposed Architectures:**
- ResNet-18/34 (residual connections)
- VGG-16 (deeper stacks)
- MobileNetV2 (depthwise separable convolutions)
- EfficientNet (compound scaling)

**Effort:** 2-3 weeks per architecture
**Files to create:**
- `common/resnet_layers.cuf`
- `common/mobilenet_layers.cuf`

**Trade-offs:**
- ‚úÖ Better accuracy (ResNet: 92%+ on CIFAR-10)
- ‚úÖ More flexible framework
- ‚úÖ Educational value
- ‚ùå Much more complex codebase
- ‚ùå Harder to maintain modularity
- ‚ùå Longer compilation times

---

## Trade-Off Analysis: Generic Binary vs Current Structure

### Current Structure (Fortran Recompilation)

**Workflow:**
```bash
cd datasets/cifar10
nvfortran cifar10_main.cuf -o cifar10_train ...
./cifar10_train
```

**Advantages:**
- ‚úÖ **Maximum performance:** Compiler optimizations per dataset
- ‚úÖ **Full control:** Can customize training loop per dataset
- ‚úÖ **Memory pools:** Can experiment with custom memory layouts
- ‚úÖ **No runtime overhead:** Everything resolved at compile time
- ‚úÖ **Easy to experiment:** Modify `.cuf` files directly
- ‚úÖ **Debugging:** Easier to debug dataset-specific issues

**Disadvantages:**
- ‚ùå **30+ second recompilation** for every hyperparameter change
- ‚ùå **Separate binary per dataset** (disk space, maintenance)
- ‚ùå **Higher barrier to entry** (need Fortran knowledge)

---

### Generic Binary (Proposed)

**Workflow:**
```bash
./train_cnn --dataset=cifar10 --epochs=15 --lr=0.001
./train_cnn --config=configs/my_experiment.yaml
```

**Advantages:**
- ‚úÖ **Zero recompilation:** Change hyperparameters instantly
- ‚úÖ **Better UX:** Command-line interface
- ‚úÖ **Easy experimentation:** Just edit YAML file
- ‚úÖ **Single binary:** Easier deployment
- ‚úÖ **Lower barrier to entry:** No Fortran knowledge needed

**Disadvantages:**
- ‚ùå **Less customization:** Harder to create dataset-specific workflows
- ‚ùå **Runtime overhead:** Virtual function calls, dynamic dispatch
- ‚ùå **Complex codebase:** Abstract types, polymorphism
- ‚ùå **Memory pool constraints:** Harder to experiment with custom layouts
- ‚ùå **Potential performance loss:** ~5-10% slower (estimate)

---

### **Recommendation: Do Both!** üéØ

**Hybrid Approach:**
1. **Keep current structure for research/experimentation**
   - Researchers can still modify `.cuf` files directly
   - Full control over training loops
   - Maximum performance

2. **Add generic binary for production/ease-of-use**
   - Casual users get easy command-line interface
   - Quick hyperparameter sweeps
   - Better for community contributions

**Implementation:**
- Generic binary reuses common modules
- Datasets remain as separate implementations
- User chooses which workflow suits their needs

**Analogy:**
- Current structure = **Compiling from source** (maximum control)
- Generic binary = **Pre-built executable** (maximum convenience)

---

## Dataset Expansion Plans

### Quick Wins (< 2 hours each)
- **MNIST:** 99%+ accuracy expected (copy Fashion-MNIST, change to 1-channel)
- **KMNIST:** Japanese characters (same as MNIST)
- **EMNIST:** Extended MNIST (letters + digits)

### Medium Effort (< 1 day)
- **STL-10:** 96√ó96 images (needs architecture adjustment)
- **Caltech-101/256:** Variable-size images (needs resizing)

### Large Projects (> 1 week)
- **ImageNet:** 224√ó224, 1000 classes (needs architecture redesign)
- **COCO:** Object detection (completely different architecture)

---

## Oxford Flowers 102 Integration

### Current Situation
- Implemented in `v28_baseline` (not `v28_baseline_plus_export`)
- Dense layer only (1280 ‚Üí 102) on pre-extracted MobileNetV2 features
- 78.94% accuracy, 1.2s training time
- No export functionality, no inference notebook

### Proposed Approach (Choose One)

#### **Option A: Full Migration (Recommended)**
**Effort:** 2-3 days

1. Migrate to `v28_baseline_plus_export/datasets/oxford_flowers/`
2. Create modular structure:
   - `oxford_flowers_config.cuf` - Feature loading
   - `oxford_flowers_main.cuf` - Dense layer training
   - `prepare_oxford_flowers.py` - MobileNetV2 feature extraction
3. Add export functionality (simpler than CNN - just 2 files):
   - `fc_weights.bin` - (102, 1280)
   - `fc_bias.bin` - (102)
4. Create `model_loader.py` extension for dense layers
5. Create `oxford_flowers_inference.ipynb`

**Advantages:**
- ‚úÖ Consistent with other datasets
- ‚úÖ Complete export pipeline
- ‚úÖ Jupyter notebook inference
- ‚úÖ Documentation and examples

---

#### **Option B: Minimal Adapter**
**Effort:** 1 day

1. Keep Oxford Flowers in `v28_baseline`
2. Create simple export script in existing code
3. Create standalone inference notebook
4. Add cross-reference documentation

**Advantages:**
- ‚úÖ Faster to implement
- ‚úÖ Less code duplication

**Disadvantages:**
- ‚ùå Inconsistent structure
- ‚ùå Not part of main framework

---

#### **Option C: Document as Future Work**
**Effort:** 1 hour

1. Add Oxford Flowers to roadmap
2. Document current status
3. Focus on more impactful features first

---

## Community Contributions

### Ways to Encourage Contributions

1. **Dataset Challenges:**
   - "Add your favorite dataset in < 2 hours using our guide!"
   - Hall of fame for contributors
   - Benchmark leaderboard

2. **Architecture Zoo:**
   - Implement ResNet, VGG, MobileNet
   - Performance comparison matrix
   - Best practices documentation

3. **Optimization Contest:**
   - Can you beat our 4√ó PyTorch speedup?
   - Novel GPU kernel implementations
   - Memory optimization techniques

4. **Export Targets:**
   - TensorFlow export
   - ONNX improvements
   - TensorRT integration

### Infrastructure Needed
- Contribution guidelines (CONTRIBUTING.md)
- Issue templates
- Automated testing (compilation + accuracy checks)
- Code review checklist

---

## Implementation Timeline (Proposed)

### Phase 1: Production Features (4-6 weeks)
**Week 1-2:**
- ‚úÖ Data augmentation
- ‚úÖ Learning rate scheduling

**Week 3-4:**
- ‚úÖ Model checkpointing & validation
- ‚úÖ YAML configuration

**Week 5-6:**
- ‚úÖ Oxford Flowers integration (Option A)
- ‚úÖ Documentation updates

### Phase 2: Advanced Features (6-8 weeks)
**Week 7-10:**
- ‚úÖ Generic training binary
- ‚úÖ Mixed precision (FP16)

**Week 11-14:**
- ‚úÖ ONNX export
- ‚úÖ Additional datasets (MNIST, STL-10)

### Phase 3: Architecture Expansion (12+ weeks)
**Week 15+:**
- ‚úÖ ResNet implementation
- ‚úÖ MobileNet implementation
- ‚úÖ Architecture comparison study

---

## Success Metrics

### Technical Metrics
- **Accuracy:** Match or exceed PyTorch baselines
- **Speed:** Maintain 2-4√ó speedup over PyTorch
- **Memory:** < 1GB GPU memory for all current datasets
- **Compilation:** < 60s compilation time per dataset

### Community Metrics
- **Dataset Coverage:** 10+ datasets by Q2 2026
- **Contributors:** 5+ external contributors
- **GitHub Stars:** 100+ stars
- **Documentation:** 95%+ code coverage in docs

---

## Questions to Resolve

1. **Oxford Flowers:** Which option (A/B/C) should we pursue?
2. **Generic Binary:** Priority level (high/medium/low)?
3. **Architectures:** Start with ResNet or other?
4. **FP16:** Is numerical instability acceptable for 2-3√ó speedup?
5. **Community:** Do we want to actively encourage external contributions?

---

## Summary

**Current Status:**
- ‚úÖ 4 datasets with complete export pipeline
- ‚úÖ 4√ó faster than PyTorch
- ‚úÖ Production-ready modular framework

**Missing Production Features:**
- ‚ùå Data augmentation
- ‚ùå Checkpointing & validation
- ‚ùå Learning rate scheduling
- ‚ùå Oxford Flowers notebook

**Proposed Next Steps:**
1. Add production-critical features (augmentation, checkpointing, LR scheduling)
2. Integrate Oxford Flowers (Option A recommended)
3. Add YAML configs for better UX
4. Evaluate generic binary (can coexist with current structure)
5. Expand dataset coverage (MNIST, STL-10)
6. Explore advanced features (FP16, ONNX, new architectures)

**Philosophy:**
- **Keep what works:** Modular structure, performance optimizations
- **Add what's missing:** Production features, better UX
- **Enable innovation:** Let users choose their workflow (Fortran vs binary)
- **Encourage community:** Make it easy to contribute new datasets/features

---

**Last Updated:** 2025-11-20
**Next Review:** After implementing Phase 1 features
