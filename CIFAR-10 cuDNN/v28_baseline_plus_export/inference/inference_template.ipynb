{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion-MNIST Inference with v28 Trained Model\n",
    "\n",
    "This notebook demonstrates how to load a trained v28 CUDA Fortran model and use it for inference.\n",
    "\n",
    "**Features:**\n",
    "- Load exported model weights\n",
    "- Make predictions on test images\n",
    "- Visualize predictions with confidence scores\n",
    "- Generate confusion matrix\n",
    "- Analyze misclassified examples\n",
    "- Per-class accuracy analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required libraries and set up visualization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\nfrom pathlib import Path\n\n# Import our model loader (same directory)\nfrom model_loader import load_v28_model, load_metadata\n\n# Set up matplotlib\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 10\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Trained Model\n",
    "\n",
    "Load the v28 trained model from exported binary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model directory\n",
    "MODEL_DIR = '../saved_models/fashion_mnist/'\n",
    "\n",
    "# Load model\n",
    "print(\"Loading v28 Fashion-MNIST model...\")\n",
    "print(\"=\" * 70)\n",
    "model = load_v28_model(MODEL_DIR,\n",
    "                       in_channels=1,      # Grayscale\n",
    "                       num_classes=10,     # 10 fashion categories\n",
    "                       input_size=28,      # 28x28 images\n",
    "                       device=device)\n",
    "\n",
    "# Load metadata\n",
    "metadata = load_metadata(MODEL_DIR)\n",
    "if metadata:\n",
    "    print(\"\\nModel Metadata:\")\n",
    "    print(f\"  Dataset: {metadata.get('dataset', 'N/A')}\")\n",
    "    print(f\"  Training Accuracy: {metadata.get('accuracy', 'N/A')}%\")\n",
    "    print(f\"  Epochs: {metadata.get('epochs', 'N/A')}\")\n",
    "    print(f\"  Export Date: {metadata.get('export_date', 'N/A')}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset\n",
    "\n",
    "Load the Fashion-MNIST test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST class names\n",
    "CLASS_NAMES = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "# Load test dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "])\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False,\n",
    "                                     download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
    "                                          shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} images\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single Image Prediction\n",
    "\n",
    "Let's make a prediction on a single test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample image\n",
    "sample_idx = 0  # Try different indices: 0, 10, 100, etc.\n",
    "image, true_label = test_dataset[sample_idx]\n",
    "\n",
    "# Make prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    image_batch = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    output = model(image_batch)\n",
    "    probabilities = torch.softmax(output, dim=1)\n",
    "    prediction = torch.argmax(output, dim=1).item()\n",
    "    confidence = probabilities[0, prediction].item()\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Show image\n",
    "ax1.imshow(image.squeeze(), cmap='gray')\n",
    "ax1.axis('off')\n",
    "ax1.set_title(f'True: {CLASS_NAMES[true_label]}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Show prediction probabilities\n",
    "probs = probabilities[0].cpu().numpy()\n",
    "colors = ['green' if i == prediction else 'gray' for i in range(10)]\n",
    "ax2.barh(CLASS_NAMES, probs, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Confidence', fontsize=12)\n",
    "ax2.set_title(f'Prediction: {CLASS_NAMES[prediction]} ({confidence:.1%})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.set_xlim([0, 1])\n",
    "\n",
    "# Add grid\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTrue Label: {CLASS_NAMES[true_label]}\")\n",
    "print(f\"Prediction: {CLASS_NAMES[prediction]}\")\n",
    "print(f\"Confidence: {confidence:.2%}\")\n",
    "print(f\"Correct: {prediction == true_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Evaluation\n",
    "\n",
    "Evaluate the model on the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_confidences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        # Store results\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Get confidence for each prediction\n",
    "        confidences = probabilities[range(len(predictions)), predictions]\n",
    "        all_confidences.extend(confidences.cpu().numpy())\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = 100.0 * correct / total\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Correct: {correct}/{total}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_confidences = np.array(all_confidences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix\n",
    "\n",
    "Visualize which classes are confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "# Normalize by row (true labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "            cbar_kws={'label': 'Normalized Frequency'})\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Confusion Matrix (Test Accuracy: {accuracy:.2f}%)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMost Confused Pairs:\")\n",
    "print(\"=\" * 70)\n",
    "# Find top confusions (excluding diagonal)\n",
    "cm_no_diag = cm_normalized.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "top_confusions = np.argsort(cm_no_diag.flatten())[-5:][::-1]\n",
    "\n",
    "for idx in top_confusions:\n",
    "    i, j = idx // 10, idx % 10\n",
    "    if cm_no_diag[i, j] > 0.05:  # Only show significant confusions\n",
    "        print(f\"  {CLASS_NAMES[i]:15s} ‚Üí {CLASS_NAMES[j]:15s}: {cm_no_diag[i,j]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-Class Accuracy\n",
    "\n",
    "Analyze accuracy for each fashion category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class accuracy\n",
    "class_correct = []\n",
    "class_total = []\n",
    "\n",
    "for i in range(10):\n",
    "    mask = all_labels == i\n",
    "    class_total.append(mask.sum())\n",
    "    class_correct.append((all_predictions[mask] == i).sum())\n",
    "\n",
    "class_accuracy = [100.0 * c / t if t > 0 else 0 \n",
    "                 for c, t in zip(class_correct, class_total)]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['green' if acc > accuracy else 'orange' for acc in class_accuracy]\n",
    "bars = ax.barh(CLASS_NAMES, class_accuracy, color=colors, alpha=0.7)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, acc) in enumerate(zip(bars, class_accuracy)):\n",
    "    ax.text(acc + 1, i, f'{acc:.1f}%', va='center', fontweight='bold')\n",
    "\n",
    "# Add overall accuracy line\n",
    "ax.axvline(accuracy, color='red', linestyle='--', linewidth=2, \n",
    "          label=f'Overall Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "ax.set_xlabel('Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim([0, 105])\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-Class Results:\")\n",
    "print(\"=\" * 70)\n",
    "for i, name in enumerate(CLASS_NAMES):\n",
    "    print(f\"{name:15s}: {class_accuracy[i]:6.2f}% ({class_correct[i]:4d}/{class_total[i]:4d})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confidence Distribution\n",
    "\n",
    "Analyze prediction confidence for correct vs incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate confidences for correct and incorrect predictions\n",
    "correct_mask = all_predictions == all_labels\n",
    "correct_confidences = all_confidences[correct_mask]\n",
    "incorrect_confidences = all_confidences[~correct_mask]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(correct_confidences, bins=50, alpha=0.7, label='Correct', color='green')\n",
    "axes[0].hist(incorrect_confidences, bins=50, alpha=0.7, label='Incorrect', color='red')\n",
    "axes[0].set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot([correct_confidences, incorrect_confidences],\n",
    "               labels=['Correct', 'Incorrect'],\n",
    "               patch_artist=True,\n",
    "               boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "axes[1].set_ylabel('Confidence', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Confidence Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfidence Statistics:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Correct Predictions:\")\n",
    "print(f\"  Mean confidence: {correct_confidences.mean():.2%}\")\n",
    "print(f\"  Median confidence: {np.median(correct_confidences):.2%}\")\n",
    "print(f\"\\nIncorrect Predictions:\")\n",
    "print(f\"  Mean confidence: {incorrect_confidences.mean():.2%}\")\n",
    "print(f\"  Median confidence: {np.median(incorrect_confidences):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Misclassified Examples\n",
    "\n",
    "Look at examples where the model made mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_indices = np.where(all_predictions != all_labels)[0]\n",
    "\n",
    "# Sort by confidence (most confident mistakes first)\n",
    "sorted_indices = misclassified_indices[np.argsort(-all_confidences[misclassified_indices])]\n",
    "\n",
    "# Show top 12 most confident mistakes\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(sorted_indices[:12]):\n",
    "    image, true_label = test_dataset[idx]\n",
    "    pred_label = all_predictions[idx]\n",
    "    confidence = all_confidences[idx]\n",
    "    \n",
    "    axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'True: {CLASS_NAMES[true_label]}\\n'\n",
    "                     f'Pred: {CLASS_NAMES[pred_label]} ({confidence:.1%})',\n",
    "                     fontsize=9, color='red', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Most Confident Misclassifications', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal Misclassifications: {len(misclassified_indices)}\")\n",
    "print(f\"Error Rate: {100.0 * len(misclassified_indices) / len(all_labels):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Most Confident Correct Predictions\n",
    "\n",
    "Show examples where the model is very confident and correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find correctly classified examples\n",
    "correct_indices = np.where(all_predictions == all_labels)[0]\n",
    "\n",
    "# Sort by confidence (most confident correct predictions)\n",
    "sorted_indices = correct_indices[np.argsort(-all_confidences[correct_indices])]\n",
    "\n",
    "# Show top 12 most confident correct predictions\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, idx in enumerate(sorted_indices[:12]):\n",
    "    image, true_label = test_dataset[idx]\n",
    "    confidence = all_confidences[idx]\n",
    "    \n",
    "    axes[i].imshow(image.squeeze(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'{CLASS_NAMES[true_label]}\\nConfidence: {confidence:.1%}',\n",
    "                     fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Most Confident Correct Predictions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"  Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Correct: {correct} / {total}\")\n",
    "print(f\"  Errors: {total - correct}\")\n",
    "\n",
    "print(f\"\\nConfidence Analysis:\")\n",
    "print(f\"  Overall mean confidence: {all_confidences.mean():.2%}\")\n",
    "print(f\"  Correct predictions confidence: {correct_confidences.mean():.2%}\")\n",
    "print(f\"  Incorrect predictions confidence: {incorrect_confidences.mean():.2%}\")\n",
    "\n",
    "print(f\"\\nBest Performing Classes:\")\n",
    "best_classes = np.argsort(class_accuracy)[-3:][::-1]\n",
    "for i in best_classes:\n",
    "    print(f\"  {CLASS_NAMES[i]:15s}: {class_accuracy[i]:.2f}%\")\n",
    "\n",
    "print(f\"\\nWorst Performing Classes:\")\n",
    "worst_classes = np.argsort(class_accuracy)[:3]\n",
    "for i in worst_classes:\n",
    "    print(f\"  {CLASS_NAMES[i]:15s}: {class_accuracy[i]:.2f}%\")\n",
    "\n",
    "if metadata:\n",
    "    print(f\"\\nModel Training Info:\")\n",
    "    print(f\"  Dataset: {metadata.get('dataset', 'N/A')}\")\n",
    "    print(f\"  Training Accuracy: {metadata.get('accuracy', 'N/A')}%\")\n",
    "    print(f\"  Epochs: {metadata.get('epochs', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## üìù Adapting to Other Datasets\n\nTo use this notebook with other datasets, modify these parameters:\n\n### CIFAR-10\n```python\nMODEL_DIR = '../saved_models/cifar10/'\nmodel = load_v28_model(MODEL_DIR, in_channels=3, num_classes=10, input_size=32)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\nCLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n               'dog', 'frog', 'horse', 'ship', 'truck']\n```\n\n### CIFAR-100\n```python\nMODEL_DIR = '../saved_models/cifar100/'\nmodel = load_v28_model(MODEL_DIR, in_channels=3, num_classes=100, input_size=32)\ntest_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n# CLASS_NAMES = [...] # 100 classes\n```\n\n### SVHN\n```python\nMODEL_DIR = '../saved_models/svhn/'\nmodel = load_v28_model(MODEL_DIR, in_channels=3, num_classes=10, input_size=32)\ntest_dataset = datasets.SVHN(root='./data', split='test', download=True, transform=transform)\nCLASS_NAMES = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n```\n\n---\n\n## üí° Note on Imports\n\nThis notebook imports directly from the same directory:\n```python\nfrom model_loader import load_v28_model, load_metadata\n```\n\nIf you're running Python scripts from a different directory, adjust the import:\n```python\n# From v28_baseline/ directory\nfrom inference.model_loader import load_v28_model, load_metadata\n\n# From anywhere else\nimport sys\nsys.path.append('/path/to/v28_baseline')\nfrom inference.model_loader import load_v28_model, load_metadata\n```\n\n---\n\n**v28 Baseline Framework**  \n**Date**: 2025-11-17  \n**Status**: Production Ready"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}