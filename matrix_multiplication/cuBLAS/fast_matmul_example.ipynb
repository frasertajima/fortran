{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper is needed to enable use of the Fortran cuBLAS matmul program to run as a shared library in python. Once loaded, you can run gpu_matmul(a,b) and get 12Tflops vs 4Tflops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "# Load the Fortran shared library\n",
    "_lib = ctypes.CDLL('./fast_matmul.so')\n",
    "\n",
    "# Define the function prototype\n",
    "_lib.tf32_matmul.argtypes = [\n",
    "    ctypes.c_void_p,  # A\n",
    "    ctypes.c_void_p,  # B\n",
    "    ctypes.c_void_p,  # C\n",
    "    ctypes.c_int,     # M\n",
    "    ctypes.c_int,     # N\n",
    "    ctypes.c_int      # K\n",
    "]\n",
    "\n",
    "def gpu_matmul(a, b):\n",
    "    \"\"\"\n",
    "    Fast matrix multiplication using cuBLAS TF32.\n",
    "    \"\"\"\n",
    "    # Convert inputs to float32 if needed\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = cp.asarray(a, dtype=np.float32)\n",
    "    if isinstance(b, np.ndarray):\n",
    "        b = cp.asarray(b, dtype=np.float32)\n",
    "    \n",
    "    # Ensure float32\n",
    "    a = cp.asarray(a, dtype=np.float32)\n",
    "    b = cp.asarray(b, dtype=np.float32)\n",
    "    \n",
    "    # Get dimensions\n",
    "    M, K = a.shape\n",
    "    K2, N = b.shape\n",
    "    assert K == K2, \"Inner dimensions must match\"\n",
    "    \n",
    "    # Create output array\n",
    "    c = cp.empty((M, N), dtype=np.float32)\n",
    "    \n",
    "    # Call Fortran function\n",
    "    _lib.tf32_matmul(\n",
    "        ctypes.c_void_p(a.data.ptr),\n",
    "        ctypes.c_void_p(b.data.ptr),\n",
    "        ctypes.c_void_p(c.data.ptr),\n",
    "        M, N, K\n",
    "    )\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 169.0122\n",
      "Fast matmul: 175.51 ms\n",
      "CuPy matmul: 456.13 ms\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create test matrices\n",
    "    a = np.random.rand(10000, 10000).astype(np.float32)\n",
    "    b = np.random.rand(10000, 10000).astype(np.float32)\n",
    "    \n",
    "    # Using our fast matmul\n",
    "    c_fast = gpu_matmul(a, b)\n",
    "    \n",
    "    # Compare with cupy\n",
    "    a_cp = cp.asarray(a)\n",
    "    b_cp = cp.asarray(b)\n",
    "    c_cp = cp.matmul(a_cp, b_cp)\n",
    "    \n",
    "    # Check results\n",
    "    print(\"Max difference:\", cp.max(cp.abs(c_fast - c_cp)))\n",
    "    \n",
    "    # Benchmark\n",
    "    import time\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(5):\n",
    "        c_fast = gpu_matmul(a_cp, b_cp)\n",
    "        c_cp = cp.matmul(a_cp, b_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Time our implementation\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        c_fast = gpu_matmul(a_cp, b_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Fast matmul: {(t1-t0)/10*1000:.2f} ms\")\n",
    "    \n",
    "    # Time cupy\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        c_cp = cp.matmul(a_cp, b_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"CuPy matmul: {(t1-t0)/10*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cupy version: 13.3.0\n",
      "CUDA version: 12060\n",
      "Device: NVIDIA RTX A1000 Laptop GPU\n",
      "cuBLAS handle obtained\n",
      "\n",
      "Testing different matmul approaches:\n",
      "cupy.matmul: 4240.08 GFLOPS\n",
      "gpu_matmul: 12288.51 GFLOPS\n",
      "\n",
      "Max difference between implementations: 118.8031005859375\n",
      "\n",
      "Benchmarking cupy...\n",
      "Run 1: 4103.73 GFLOPS\n",
      "Run 2: 4133.01 GFLOPS\n",
      "Run 3: 4143.74 GFLOPS\n",
      "Run 4: 4161.88 GFLOPS\n",
      "Run 5: 4113.75 GFLOPS\n",
      "Run 6: 4077.17 GFLOPS\n",
      "Run 7: 4083.26 GFLOPS\n",
      "Run 8: 3992.93 GFLOPS\n",
      "Run 9: 3953.48 GFLOPS\n",
      "Run 10: 3955.06 GFLOPS\n",
      "\n",
      "Benchmarking gpu_matmul...\n",
      "Run 1: 11983.20 GFLOPS\n",
      "Run 2: 11986.29 GFLOPS\n",
      "Run 3: 11982.33 GFLOPS\n",
      "Run 4: 11984.53 GFLOPS\n",
      "Run 5: 12006.31 GFLOPS\n",
      "Run 6: 12046.60 GFLOPS\n",
      "Run 7: 12075.94 GFLOPS\n",
      "Run 8: 11935.67 GFLOPS\n",
      "Run 9: 11939.36 GFLOPS\n",
      "Run 10: 11926.43 GFLOPS\n",
      "\n",
      "Performance Results for cupy:\n",
      "  Minimum: 3953.48 GFLOPS\n",
      "  Maximum: 4161.88 GFLOPS\n",
      "  Average: 4071.80 GFLOPS\n",
      "  Std Dev: 73.37 GFLOPS\n",
      "\n",
      "Performance Results for gpu_matmul:\n",
      "  Minimum: 11926.43 GFLOPS\n",
      "  Maximum: 12075.94 GFLOPS\n",
      "  Average: 11986.66 GFLOPS\n",
      "  Std Dev: 45.22 GFLOPS\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from fast_matmul_wrapper import gpu_matmul\n",
    "import time\n",
    "\n",
    "# Print cupy configuration\n",
    "print(f\"cupy version: {cp.__version__}\")\n",
    "print(f\"CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "print(f\"Device: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}\")\n",
    "\n",
    "# Try to get cuBLAS configuration\n",
    "try:\n",
    "    handle = cp.cuda.device.get_cublas_handle()\n",
    "    print(\"cuBLAS handle obtained\")\n",
    "except:\n",
    "    print(\"Could not get cuBLAS handle\")\n",
    "\n",
    "# Create test matrices; make sure not to normalise matricies or cupy will drop to 100Gflops!!\n",
    "a = np.random.rand(5120, 5120).astype(np.float32)\n",
    "b = np.random.rand(5120, 5120).astype(np.float32)\n",
    "\n",
    "# Convert to cupy\n",
    "a_cp = cp.asarray(a)\n",
    "b_cp = cp.asarray(b)\n",
    "\n",
    "# Function to run a single timed matmul\n",
    "def timed_matmul(func, a, b, name=\"\"):\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    c = func(a, b)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    exec_time = end - start\n",
    "    flops = 2.0 * float(5120**3)\n",
    "    gflops = (flops / 1e9) / exec_time\n",
    "    return gflops, c\n",
    "\n",
    "# Try different ways of calling matmul\n",
    "print(\"\\nTesting different matmul approaches:\")\n",
    "\n",
    "# 1. Regular cupy matmul\n",
    "gflops, c1 = timed_matmul(cp.matmul, a_cp, b_cp, \"cupy.matmul\")\n",
    "print(f\"cupy.matmul: {gflops:.2f} GFLOPS\")\n",
    "\n",
    "# 2. Our gpu_matmul\n",
    "gflops, c2 = timed_matmul(gpu_matmul, a_cp, b_cp, \"gpu_matmul\")\n",
    "print(f\"gpu_matmul: {gflops:.2f} GFLOPS\")\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\nMax difference between implementations: {cp.max(cp.abs(c1 - c2))}\")\n",
    "\n",
    "# Now do full benchmark\n",
    "def run_benchmark(func, name, warmup=5, runs=10):\n",
    "    print(f\"\\nBenchmarking {name}...\")\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = func(a_cp, b_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Timing runs\n",
    "    times = []\n",
    "    for i in range(runs):\n",
    "        gflops, _ = timed_matmul(func, a_cp, b_cp)\n",
    "        times.append(gflops)\n",
    "        print(f\"Run {i+1}: {gflops:.2f} GFLOPS\")\n",
    "    return times\n",
    "\n",
    "# Run benchmarks\n",
    "times_cupy = run_benchmark(cp.matmul, \"cupy\")\n",
    "times_custom = run_benchmark(gpu_matmul, \"gpu_matmul\")\n",
    "\n",
    "# Print statistics\n",
    "def print_stats(name, times):\n",
    "    print(f\"\\nPerformance Results for {name}:\")\n",
    "    print(f\"  Minimum: {min(times):.2f} GFLOPS\")\n",
    "    print(f\"  Maximum: {max(times):.2f} GFLOPS\")\n",
    "    print(f\"  Average: {sum(times)/len(times):.2f} GFLOPS\")\n",
    "    print(f\"  Std Dev: {np.std(times):.2f} GFLOPS\")\n",
    "\n",
    "print_stats(\"cupy\", times_cupy)\n",
    "print_stats(\"gpu_matmul\", times_custom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing non-normalized matrices:\n",
      "GFLOPS: 3941.02\n",
      "Max value in result: 1371.0440673828125\n",
      "\n",
      "Testing normalized matrices:\n",
      "GFLOPS: 106.10\n",
      "Max value in result: 0.26630081527617694\n"
     ]
    }
   ],
   "source": [
    "# normalising values kills cupy performanc from 4,000gflops to only 100gflops!!\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Test both normal and normalized matrices\n",
    "def test_both_versions():\n",
    "    # Version 1: No normalization\n",
    "    a1 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    b1 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    a1_cp = cp.asarray(a1)\n",
    "    b1_cp = cp.asarray(b1)\n",
    "\n",
    "    # Version 2: With normalization\n",
    "    a2 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    b2 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    a2 = a2 / np.sqrt(a2.shape[1])\n",
    "    b2 = b2 / np.sqrt(b2.shape[0])\n",
    "    a2_cp = cp.asarray(a2)\n",
    "    b2_cp = cp.asarray(b2)\n",
    "\n",
    "    def benchmark(a, b, name):\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        c = cp.matmul(a, b)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        exec_time = end - start\n",
    "        flops = 2.0 * float(5120**3)\n",
    "        gflops = (flops / 1e9) / exec_time\n",
    "        return gflops, c\n",
    "\n",
    "    print(\"Testing non-normalized matrices:\")\n",
    "    gflops1, c1 = benchmark(a1_cp, b1_cp, \"non-normalized\")\n",
    "    print(f\"GFLOPS: {gflops1:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c1))}\")\n",
    "\n",
    "    print(\"\\nTesting normalized matrices:\")\n",
    "    gflops2, c2 = benchmark(a2_cp, b2_cp, \"normalized\")\n",
    "    print(f\"GFLOPS: {gflops2:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c2))}\")\n",
    "\n",
    "test_both_versions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slowdown in the cuBLAS kernel is not as noticable (it is there but still running at 10Tflops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with cupy.matmul:\n",
      "Non-normalized matrices:\n",
      "GFLOPS: 4229.82\n",
      "Max value in result: 1372.2125244140625\n",
      "\n",
      "Normalized matrices:\n",
      "GFLOPS: 106.35\n",
      "Max value in result: 0.2683828339836041\n",
      "\n",
      "Testing with gpu_matmul:\n",
      "Non-normalized matrices:\n",
      "GFLOPS: 14651.67\n",
      "Max value in result: 1358.61376953125\n",
      "\n",
      "Normalized matrices:\n",
      "GFLOPS: 10492.84\n",
      "Max value in result: 0.26880133152008057\n",
      "\n",
      "Verifying results:\n",
      "Non-normalized max difference: 122.855225\n",
      "Normalized max difference: 0.024820299532119394\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "from fast_matmul_wrapper import gpu_matmul\n",
    "\n",
    "# Test both normal and normalized matrices\n",
    "def test_both_versions():\n",
    "    # Version 1: No normalization\n",
    "    a1 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    b1 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    a1_cp = cp.asarray(a1)\n",
    "    b1_cp = cp.asarray(b1)\n",
    "\n",
    "    # Version 2: With normalization\n",
    "    a2 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    b2 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    a2 = a2 / np.sqrt(a2.shape[1])\n",
    "    b2 = b2 / np.sqrt(b2.shape[0])\n",
    "    a2_cp = cp.asarray(a2)\n",
    "    b2_cp = cp.asarray(b2)\n",
    "\n",
    "    def benchmark_cupy(a, b):\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        c = cp.matmul(a, b)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        exec_time = end - start\n",
    "        flops = 2.0 * float(5120**3)\n",
    "        gflops = (flops / 1e9) / exec_time\n",
    "        return gflops, c\n",
    "\n",
    "    def benchmark_gpu_matmul(a, b):\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        c = gpu_matmul(a, b)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        exec_time = end - start\n",
    "        flops = 2.0 * float(5120**3)\n",
    "        gflops = (flops / 1e9) / exec_time\n",
    "        return gflops, c\n",
    "\n",
    "    # Test cupy.matmul\n",
    "    print(\"Testing with cupy.matmul:\")\n",
    "    print(\"Non-normalized matrices:\")\n",
    "    gflops1, c1 = benchmark_cupy(a1_cp, b1_cp)\n",
    "    print(f\"GFLOPS: {gflops1:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c1))}\")\n",
    "\n",
    "    print(\"\\nNormalized matrices:\")\n",
    "    gflops2, c2 = benchmark_cupy(a2_cp, b2_cp)\n",
    "    print(f\"GFLOPS: {gflops2:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c2))}\")\n",
    "\n",
    "    # Test gpu_matmul\n",
    "    print(\"\\nTesting with gpu_matmul:\")\n",
    "    print(\"Non-normalized matrices:\")\n",
    "    gflops3, c3 = benchmark_gpu_matmul(a1_cp, b1_cp)\n",
    "    print(f\"GFLOPS: {gflops3:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c3))}\")\n",
    "\n",
    "    print(\"\\nNormalized matrices:\")\n",
    "    gflops4, c4 = benchmark_gpu_matmul(a2_cp, b2_cp)\n",
    "    print(f\"GFLOPS: {gflops4:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c4))}\")\n",
    "\n",
    "    # Verify results match between implementations\n",
    "    print(\"\\nVerifying results:\")\n",
    "    print(\"Non-normalized max difference:\", cp.max(cp.abs(c1 - c3)))\n",
    "    print(\"Normalized max difference:\", cp.max(cp.abs(c2 - c4)))\n",
    "\n",
    "test_both_versions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
