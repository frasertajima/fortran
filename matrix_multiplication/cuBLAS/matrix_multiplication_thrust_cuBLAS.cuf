module matrix_mul
  use cudafor
  implicit none

  type :: gpu_params
    character(len=64)         :: device_name
    integer                   :: major, minor
    integer                   :: sm_count
    integer                   :: max_threads_per_sm
    integer                   :: shared_mem_per_block
  end type gpu_params

  character(len=*), parameter :: COLOR_RED = char(27)//'[31m'
  character(len=*), parameter :: COLOR_GREEN = char(27)//'[32m'
  character(len=*), parameter :: COLOR_RESET = char(27)//'[0m'

contains

  attributes(global) subroutine MatrixMulKernel(A, B, C, M, N, K, TILE_DIM, BLOCK_SIZE)
    implicit none
    real(8), device           :: A(M,K), B(K,N), C(M,N)
    integer, value            :: M, N, K, TILE_DIM, BLOCK_SIZE

    ! Thread block parameters
    integer, parameter        :: THREAD_ITEMS_PER_THREAD = 4

    ! Shared memory tiles - using float instead of double to reduce shared memory usage
    real(4), shared           :: As0(0:63, 0:63), As1(0:63, 0:63)
    real(4), shared           :: Bs0(0:63, 0:63), Bs1(0:63, 0:63)

    ! Thread-local storage for better register reuse
    real(8)                   :: Csub(THREAD_ITEMS_PER_THREAD, THREAD_ITEMS_PER_THREAD)
    real(8)                   :: Areg(THREAD_ITEMS_PER_THREAD), Breg(THREAD_ITEMS_PER_THREAD)

    ! Thread and block indices
    integer                   :: tx, ty, bx, by
    integer                   :: row, col, kk, tile
    integer                   :: curr_buff, next_buff
    integer                   :: i, j

    ! Get thread and block indices
    tx = threadidx%x - 1
    ty = threadidx%y - 1
    bx = (blockidx%x - 1) * TILE_DIM
    by = (blockidx%y - 1) * TILE_DIM

    ! Calculate global row and column indices
    row = by + ty
    col = bx + tx

    ! Initialize Csub to zero
    do j = 1, THREAD_ITEMS_PER_THREAD
      do i = 1, THREAD_ITEMS_PER_THREAD
        Csub(i,j) = 0.0d0
      end do
    end do

    curr_buff = 0

    ! Load first tile into shared memory buffer 0
    if (row < M .and. tx < K) then
      As0(ty,tx) = real(A(row, tx), 4)
    else
      As0(ty,tx) = 0.0
    end if

    if (col < N .and. ty < K) then
      Bs0(ty,tx) = real(B(ty, col), 4)
    else
      Bs0(ty,tx) = 0.0
    end if

    call syncthreads()

    ! Main loop over tiles
    do tile = 0, (K-1)/TILE_DIM
      ! Load next tile into next buffer while computing on current buffer
      next_buff = 1 - curr_buff

      ! Load next tile of A into shared memory
      if (row < M .and. (tile*TILE_DIM + tx) < K) then
        if (curr_buff == 0) then
          As1(ty,tx) = real(A(row, tile*TILE_DIM + tx), 4)
        else
          As0(ty,tx) = real(A(row, tile*TILE_DIM + tx), 4)
        end if
      else
        if (curr_buff == 0) then
          As1(ty,tx) = 0.0
        else
          As0(ty,tx) = 0.0
        end if
      end if

      ! Load next tile of B into shared memory
      if (col < N .and. (tile*TILE_DIM + ty) < K) then
        if (curr_buff == 0) then
          Bs1(ty,tx) = real(B(tile*TILE_DIM + ty, col), 4)
        else
          Bs0(ty,tx) = real(B(tile*TILE_DIM + ty, col), 4)
        end if
      else
        if (curr_buff == 0) then
          Bs1(ty,tx) = 0.0
        else
          Bs0(ty,tx) = 0.0
        end if
      end if

      ! Compute using current buffer while loading completes
      if (curr_buff == 0) then
        do kk = 0, TILE_DIM-1
          ! Load values into registers
          do i = 1, THREAD_ITEMS_PER_THREAD
            Areg(i) = real(As0(ty+i-1,kk), 8)
            Breg(i) = real(Bs0(kk,tx+i-1), 8)
          end do

          ! Compute outer product
          do j = 1, THREAD_ITEMS_PER_THREAD
            do i = 1, THREAD_ITEMS_PER_THREAD
              Csub(i,j) = Csub(i,j) + Areg(i) * Breg(j)
            end do
          end do
        end do
      else
        do kk = 0, TILE_DIM-1
          ! Load values into registers
          do i = 1, THREAD_ITEMS_PER_THREAD
            Areg(i) = real(As1(ty+i-1,kk), 8)
            Breg(i) = real(Bs1(kk,tx+i-1), 8)
          end do

          ! Compute outer product
          do j = 1, THREAD_ITEMS_PER_THREAD
            do i = 1, THREAD_ITEMS_PER_THREAD
              Csub(i,j) = Csub(i,j) + Areg(i) * Breg(j)
            end do
          end do
        end do
      end if

      ! Switch buffers
      curr_buff = next_buff

      ! Synchronize before next iteration
      call syncthreads()
    end do

    ! Write results to global memory
    do j = 1, THREAD_ITEMS_PER_THREAD
      do i = 1, THREAD_ITEMS_PER_THREAD
        if ((row+i-1) < M .and. (col+j-1) < N) then
          C(row+i-1,col+j-1) = Csub(i,j)
        end if
      end do
    end do

  end subroutine MatrixMulKernel

  subroutine get_optimal_params(params, tile_dim, block_size)
    type(gpu_params), intent(out)   :: params
    integer, intent(out)            :: tile_dim, block_size
    type(cudaDeviceProp)            :: prop
    integer                         :: istat

    ! Get device properties
    istat = cudaGetDeviceProperties(prop, 0)

    ! Store device parameters
    params%device_name = prop%name
    params%major = prop%major
    params%minor = prop%minor
    params%sm_count = prop%multiProcessorCount
    params%max_threads_per_sm = prop%maxThreadsPerMultiProcessor
    params%shared_mem_per_block = prop%sharedMemPerBlock

    ! Set optimal parameters based on device capabilities
    tile_dim = 64  ! This seems to work well for most cases
    block_size = 16  ! This gives us 16x16 = 256 threads per block
  end subroutine get_optimal_params

  attributes(global) subroutine VerifyKernel(A, B, diff, M, N)
      implicit none
      real(8), device                  :: A(M,N), B(M,N), diff(M,N)  ! Declare as 2D arrays
      integer, value                   :: M, N
      integer                          :: row, col

      ! Calculate row and column indices
      row = (blockIdx%x - 1) * blockDim%x + threadIdx%x
      col = (blockIdx%y - 1) * blockDim%y + threadIdx%y

      if (row <= M .and. col <= N) then
          diff(row,col) = abs(A(row,col) - B(row,col))
      endif
  end subroutine VerifyKernel

  subroutine verify_results(A, B, M, N, max_diff, results_match)
      implicit none
      real(8), device                   :: A(M,N), B(M,N)  ! Declare as 2D arrays
      integer, intent(in)               :: M, N
      real(8), intent(out)              :: max_diff
      logical, intent(out)              :: results_match

      real(8), device, allocatable      :: diff(:,:)  ! Make it 2D
      type(dim3)                        :: grid, block
      real(8), parameter                :: tolerance = 1.0d-10
      integer                           :: istat

      ! Allocate difference array
      allocate(diff(M,N))

      ! Configure kernel launch parameters for 2D grid
      block = dim3(16, 16, 1)  ! 16x16 thread block
      grid = dim3((M+15)/16, (N+15)/16, 1)  ! Ceiling division for grid size

      ! Launch verification kernel
      call VerifyKernel<<<grid, block>>>(A, B, diff, M, N)
      istat = cudaDeviceSynchronize()

      ! Find maximum difference
      max_diff = maxval(diff)

      ! Check if results match within tolerance
      results_match = (max_diff <= tolerance)

      deallocate(diff)
  end subroutine verify_results

  subroutine verify_and_print_results(A, B, M, N, version_name)
      implicit none
      real(8), device                   :: A(M,N), B(M,N)
      integer, intent(in)               :: M, N
      character(len=*), intent(in)      :: version_name
      real(8)                          :: max_difference
      logical                          :: results_match

      ! Use existing verify_results
      call verify_results(A, B, M, N, max_difference, results_match)

      ! Print results in standard format with colors
      print *, ""
      print *, "Verification Results for ", trim(version_name), ":"
      if (results_match) then
          print *, COLOR_GREEN//"Results match!"//COLOR_RESET
          write(*,'(A,E12.5)') COLOR_GREEN//"  Maximum difference: "//COLOR_RESET, max_difference
      else
          print *, COLOR_RED//"WARNING: Results differ!"//COLOR_RESET
          write(*,'(A,E12.5)') COLOR_RED//"  Maximum difference: "//COLOR_RESET, max_difference
      endif
  end subroutine verify_and_print_results

  subroutine print_statistics(version_name, gflops_arr, n_runs, peak_theoretical)
    character(len=*), intent(in) :: version_name
    real(8), intent(in) :: gflops_arr(n_runs)
    integer, intent(in) :: n_runs
    real(8), intent(in) :: peak_theoretical
    real(8) :: min_gflops, max_gflops, avg_gflops, std_dev

    min_gflops = minval(gflops_arr)
    max_gflops = maxval(gflops_arr)
    avg_gflops = sum(gflops_arr) / real(n_runs)
    std_dev = sqrt(sum((gflops_arr - avg_gflops)**2) / real(n_runs))

    print *, ""
    print *, "Performance Results for ", trim(version_name), " (", n_runs, "runs ):"
    write(*,'(A,F12.2,A)') "  Minimum: ", min_gflops, " GFLOPS"
    write(*,'(A,F12.2,A)') "  Maximum: ", max_gflops, " GFLOPS"
    write(*,'(A,F12.2,A)') "  Average: ", avg_gflops, " GFLOPS"
    write(*,'(A,F12.2,A)') "  Std Dev: ", std_dev, " GFLOPS"
    print *, ""
    print *, "GPU Utilization:"
    write(*,'(A,F12.2,A)') "  Peak Theoretical:  ", peak_theoretical, " GFLOPS"
    write(*,'(A,F8.2,A)') "  Average Achieved:   ", (avg_gflops / peak_theoretical) * 100.0d0, "%"
    write(*,'(A,F8.2,A)') "  Best Achieved:      ", (max_gflops / peak_theoretical) * 100.0d0, "%"
  end subroutine print_statistics

end module matrix_mul

program main
  use matrix_mul  ! For custom kernel
  use thrust      ! For Thrust version
  use cudafor
  use cublas_v2
  implicit none

  integer, parameter                                    :: matrix_size = 5120
  integer, parameter                                    :: M = matrix_size, N = matrix_size, K = matrix_size
  integer, parameter                                    :: NUM_RUNS = 10
  integer, parameter                                    :: NUM_WARMUP = 5

  real(8), allocatable, dimension(:,:)                  :: A, B, C
  real(8), device, allocatable, target, dimension(:,:)  :: d_A, d_B, d_C
  real(8), device, allocatable, target, dimension(:,:)  :: d_C_thrust, d_C_custom  ! New arrays for comparison
  real(8)                                               :: start, finish, exec_time
  real(8)                                               :: FLOP, FLOPS, GFLOPS, min_gflops, max_gflops, avg_gflops, peak_gflops
  real(8), dimension(NUM_RUNS)                          :: run_times, gflops_array
  integer                                               :: istat, run
  type(cudaEvent)                                       :: startEvent, stopEvent
  real                                                  :: eventTime
  type(dim3)                                            :: grid, block
  real(8)                                               :: max_difference
  logical                                               :: results_match

  ! GPU parameters
  type(gpu_params)                                      :: params
  integer                                               :: tile_dim, block_size

  ! Add cuBLAS variables
  type(cublasHandle)                                    :: handle
  real(4), allocatable                                  :: a_sp(:,:), b_sp(:,:), c_sp(:,:)
  real(4), device                                       :: a_d_sp(M,K), b_d_sp(K,N), c_d_sp(M,N)
  real(8), dimension(NUM_RUNS)                          :: gflops_array_fp32, gflops_array_tf32
  real(8), device, allocatable, dimension(:,:)          :: d_C_cublas
  real(8), allocatable                                  :: cref(:,:)
  real(8), allocatable, dimension(:,:)                  :: c_thrust  ! For host verification

  ! Get optimal parameters for current GPU
  call get_optimal_params(params, tile_dim, block_size)

  ! Print GPU information
  print *, "GPU Information:"
  print *, " Device Name:", trim(params%device_name)
  write(*,'(A,I13,A,I12)') "  Compute Capability: ", params%major, " .", params%minor
  write(*,'(A,I12)') "  Number of SMs:        ", params%sm_count
  write(*,'(A,I12)') "  Max Threads per SM:     ", params%max_threads_per_sm
  write(*,'(A,I12,A)') "  Shared Memory per Block: ", params%shared_mem_per_block, " bytes"
  write(*,'(A,I12)') "  Selected tile size:   ", tile_dim
  write(*,'(A,I12)') "  Selected block size:  ", block_size

  ! Allocate and initialize matrices
  allocate(A(M,K), B(K,N), C(M,N))
  call random_number(A)
  call random_number(B)
  A = A / sqrt(real(K))  ! Normalize by sqrt(K) to keep results ~O(1)
  B = B / sqrt(real(K))
  allocate(d_A(M,K), d_B(K,N), d_C(M,N))
  allocate(d_C_thrust(M,N), d_C_custom(M,N))  ! Allocate comparison arrays
  allocate(d_C_cublas(M,N))
  allocate(cref(M,N))

  print *, "Running reference matmul..."
  cref = matmul(A,B)  ! Double precision reference

  ! Create CUDA events
  istat = cudaEventCreate(startEvent)
  istat = cudaEventCreate(stopEvent)

  ! Set up execution configuration for custom kernel
  block = dim3(block_size, block_size, 1)
  grid = dim3((M-1)/tile_dim + 1, (N-1)/tile_dim + 1, 1)


  ! THRUST VERSION TIMING RUNS -------------------------------------------------------
  print *, ""
  print *, "Performing Thrust timing runs..."
  ! Print matrix dimensions once at the start
  call thrust_print_matrix_info(M, N, K)

  ! Test Thrust functionality first
  call thrust_test()
  print *, "Thrust test completed successfully"

  do run = 1, NUM_RUNS
    ! Record start time
    istat = cudaEventRecord(startEvent, 0)

    ! Copy input matrices to device
    d_A = A
    d_B = B

    ! Launch Thrust kernel
    call thrust_matrix_multiply(d_A, d_B, d_C, M, N, K)

    ! Copy result back
    d_C_thrust = d_C

    ! Record stop time and synchronize
    istat = cudaEventRecord(stopEvent, 0)
    istat = cudaEventSynchronize(stopEvent)

    ! Get timing
    istat = cudaEventElapsedTime(eventTime, startEvent, stopEvent)
    exec_time = eventTime / 1000.0d0

    ! Calculate GFLOPS
    FLOP = 2.0d0 * real(M) * real(N) * real(K)
    GFLOPS = (FLOP / 1.0d9) / exec_time
    gflops_array(run) = GFLOPS

    print *, "Thrust Run", run, ":", GFLOPS, "GFLOPS"
  end do

  c_thrust = d_C_thrust  ! Copy device to host

  ! Calculate and print statistics for Thrust version
  peak_gflops = real(params%sm_count) * 128.0d0 * (1.35d3 / 1.0d3)
  call print_statistics("Thrust", gflops_array, NUM_RUNS, peak_gflops)


  ! CUSTOM KERNEL TIMING RUNS -------------------------------------------------------
  print *, "Performing Custom Kernel warmup runs..."
  do run = 1, NUM_WARMUP
    call MatrixMulKernel<<<grid, block>>>(d_A, d_B, d_C, M, N, K, tile_dim, block_size)
  end do
  istat = cudaDeviceSynchronize()

  print *, "Performing Custom Kernel timing runs..."
  do run = 1, NUM_RUNS
    ! Record start time
    istat = cudaEventRecord(startEvent, 0)

    ! Copy input matrices to device
    d_A = A
    d_B = B

    ! Launch custom kernel
    call MatrixMulKernel<<<grid, block>>>(d_A, d_B, d_C, M, N, K, tile_dim, block_size)

    ! Copy result back
    C = d_C

    ! Record stop time and synchronize
    istat = cudaEventRecord(stopEvent, 0)
    istat = cudaEventSynchronize(stopEvent)

    ! Get timing
    istat = cudaEventElapsedTime(eventTime, startEvent, stopEvent)
    exec_time = eventTime / 1000.0d0

    ! Calculate GFLOPS
    FLOP = 2.0d0 * real(M) * real(N) * real(K)
    GFLOPS = (FLOP / 1.0d9) / exec_time
    gflops_array(run) = GFLOPS

    print *, "Custom Run", run, ":", GFLOPS, "GFLOPS"
  end do

  ! Before verification
  istat = cudaDeviceSynchronize()  ! Ensure all previous operations are complete
  call verify_and_print_results(d_C_thrust, d_C, M, N, "Thrust vs Custom Kernel")
  print *, "Custom maxval(abs(cref-c)): ", maxval(abs(real(cref,4) - real(C,4)))

  ! Calculate statistics for custom kernel
  call print_statistics("Custom Kernel", gflops_array, NUM_RUNS, peak_gflops)



  ! cuBLAS VERSION TIMING RUNS -------------------------------------------------------
  print *, ""
  print *, "Performing cuBLAS timing runs..."

  ! Initialize cuBLAS
  istat = cublasCreate(handle)
  if (istat /= CUBLAS_STATUS_SUCCESS) then
    print *, "cuBLAS initialization failed"
    stop
  endif

  ! Allocate and initialize SP arrays
  allocate(a_sp(M,K), b_sp(K,N), c_sp(M,N))
  a_sp = real(A, 4)  ! Convert from DP to SP
  b_sp = real(B, 4)

  ! Copy to device
  a_d_sp = a_sp
  b_d_sp = b_sp

  ! 1. First run: Standard FP32 SGEMM
  print *, ""
  print *, "Running standard FP32 SGEMM..."
  istat = cublasSetMathMode(handle, CUBLAS_DEFAULT_MATH)

  do run = 1, NUM_RUNS
    c_d_sp = 0.0
    istat = cudaEventRecord(startEvent, 0)

    istat = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, &
                        M, N, K, 1.0, a_d_sp, M, b_d_sp, K, 0.0, c_d_sp, M)

    istat = cudaEventRecord(stopEvent, 0)
    istat = cudaEventSynchronize(stopEvent)
    istat = cudaEventElapsedTime(eventTime, startEvent, stopEvent)

    exec_time = eventTime / 1000.0d0
    GFLOPS = (FLOP / 1.0d9) / exec_time
    gflops_array_fp32(run) = GFLOPS

    print *, "FP32 Run", run, ":", GFLOPS, "GFLOPS"
  end do

  ! Get FP32 results and compare
  c_sp = c_d_sp
  d_C_cublas = real(c_sp, 8)  ! Store FP32 results
  call verify_and_print_results(d_C_thrust, d_C_cublas, M, N, "Thrust vs FP32 SGEMM")
  call print_statistics("cuBLAS FP32 SGEMM", gflops_array_fp32, NUM_RUNS, peak_gflops)
  print *, "FP32 SGEMM maxval(abs(cref-c)): ", maxval(abs(real(cref,4) - c_sp))

  ! 2. Second run: TF32 Tensor Core SGEMM
  print *, ""
  print *, "Running TF32 Tensor Core SGEMM..."
  istat = cublasSetMathMode(handle, CUBLAS_TF32_TENSOR_OP_MATH)

  do run = 1, NUM_RUNS
    c_d_sp = 0.0
    istat = cudaEventRecord(startEvent, 0)

    istat = cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, &
                        M, N, K, 1.0, a_d_sp, M, b_d_sp, K, 0.0, c_d_sp, M)

    istat = cudaEventRecord(stopEvent, 0)
    istat = cudaEventSynchronize(stopEvent)
    istat = cudaEventElapsedTime(eventTime, startEvent, stopEvent)

    exec_time = eventTime / 1000.0d0
    GFLOPS = (FLOP / 1.0d9) / exec_time
    gflops_array_tf32(run) = GFLOPS

    print *, "TF32 Run", run, ":", GFLOPS, "GFLOPS"
  end do

  ! Get TF32 results and compare
  c_sp = c_d_sp
  d_C_cublas = real(c_sp, 8)  ! Store TF32 results
  call verify_and_print_results(d_C_thrust, d_C_cublas, M, N, "Thrust vs TF32 Tensor Core")
  call print_statistics("cuBLAS TF32 Tensor Core SGEMM", gflops_array_tf32, NUM_RUNS, peak_gflops)
  print *, "TF32 SGEMM maxval(abs(cref-c)): ", maxval(abs(real(cref,4) - c_sp))

  ! Cleanup
  deallocate(a_sp, b_sp, c_sp)
  istat = cublasDestroy(handle)
  istat = cudaEventDestroy(startEvent)
  istat = cudaEventDestroy(stopEvent)
  deallocate(A, B, C, d_A, d_B, d_C, d_C_thrust, d_C_custom, d_C_cublas)

end program main
! after optimisations, hitting 2,780Gflops (from 350Gflops), 65% of theoretical peak!!
! nvcc -c thrust.cu -o thrust.C.o --extended-lambda
! nvfortran -o3 thrust.cuf matrix_multiplication_thrust.cuf thrust.C.o -c++libs -o matrix_multiplication_thrust
! added comparison with thrust version: thrust is super slow at 66 GFLOPS vs 2,780 GFLOPS on RTX 4060!!
! added verification kernel to compare results between thrust and custom kernel; they match.
! so this really is a very fast matrix multiplication kernel as compared to thrust
