{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wrapper is needed to enable use of the Fortran cuBLAS matmul program to run as a shared library in python. Once loaded, you can run tf32_matmul(a,b) and get 12Tflops vs 4Tflops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "# Load the Fortran shared library\n",
    "_lib = ctypes.CDLL('./fast_matmul.so')\n",
    "\n",
    "# Define the function prototype\n",
    "_lib.tf32_matmul.argtypes = [\n",
    "    ctypes.c_void_p,  # A\n",
    "    ctypes.c_void_p,  # B\n",
    "    ctypes.c_void_p,  # C\n",
    "    ctypes.c_int,     # M\n",
    "    ctypes.c_int,     # N\n",
    "    ctypes.c_int      # K\n",
    "]\n",
    "\n",
    "def tf32_matmul(a, b):\n",
    "    \"\"\"\n",
    "    Fast matrix multiplication using cuBLAS TF32.\n",
    "    \"\"\"\n",
    "    # Convert inputs to float32 if needed\n",
    "    if isinstance(a, np.ndarray):\n",
    "        a = cp.asarray(a, dtype=np.float32)\n",
    "    if isinstance(b, np.ndarray):\n",
    "        b = cp.asarray(b, dtype=np.float32)\n",
    "    \n",
    "    # Ensure float32\n",
    "    a = cp.asarray(a, dtype=np.float32)\n",
    "    b = cp.asarray(b, dtype=np.float32)\n",
    "    \n",
    "    # Get dimensions\n",
    "    M, K = a.shape\n",
    "    K2, N = b.shape\n",
    "    assert K == K2, \"Inner dimensions must match\"\n",
    "    \n",
    "    # Create output array\n",
    "    c = cp.empty((M, N), dtype=np.float32)\n",
    "    \n",
    "    # Call Fortran function\n",
    "    _lib.tf32_matmul(\n",
    "        ctypes.c_void_p(a.data.ptr),\n",
    "        ctypes.c_void_p(b.data.ptr),\n",
    "        ctypes.c_void_p(c.data.ptr),\n",
    "        M, N, K\n",
    "    )\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference: 168.52344\n",
      "Fast matmul: 177.15 ms\n",
      "CuPy matmul: 464.35 ms\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create test matrices\n",
    "    a = np.random.rand(10000, 10000).astype(np.float32)\n",
    "    b = np.random.rand(10000, 10000).astype(np.float32)\n",
    "    \n",
    "    # Using our fast matmul\n",
    "    c_fast = tf32_matmul(a, b)\n",
    "    \n",
    "    # Compare with cupy\n",
    "    a_cp = cp.asarray(a)\n",
    "    b_cp = cp.asarray(b)\n",
    "    c_cp = cp.matmul(a_cp, b_cp)\n",
    "    \n",
    "    # Check results\n",
    "    print(\"Max difference:\", cp.max(cp.abs(c_fast - c_cp)))\n",
    "    \n",
    "    # Benchmark\n",
    "    import time\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(5):\n",
    "        c_fast = tf32_matmul(a_cp, b_cp)\n",
    "        c_cp = cp.matmul(a_cp, b_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Time our implementation\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        c_fast = tf32_matmul(a_cp, b_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Fast matmul: {(t1-t0)/10*1000:.2f} ms\")\n",
    "    \n",
    "    # Time cupy\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(10):\n",
    "        c_cp = cp.matmul(a_cp, b_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"CuPy matmul: {(t1-t0)/10*1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cupy is much faster on RTX 4060 vs A1000 (only 2x difference vs 3-4x):\n",
    "Max difference: 196.97314\n",
    "Fast matmul: 146.03 ms\n",
    "CuPy matmul: 264.85 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cupy version: 13.3.0\n",
      "CUDA version: 12060\n",
      "Device: NVIDIA RTX A1000 Laptop GPU\n",
      "cuBLAS handle obtained\n",
      "\n",
      "Testing different matmul approaches:\n",
      "cupy.matmul: 3870.59 GFLOPS\n",
      "tf32_matmul: 11330.21 GFLOPS\n",
      "\n",
      "Max difference between implementations: 129.4381103515625\n",
      "\n",
      "Benchmarking cupy...\n",
      "Run 1: 3932.05 GFLOPS\n",
      "Run 2: 3947.16 GFLOPS\n",
      "Run 3: 3942.15 GFLOPS\n",
      "Run 4: 3947.49 GFLOPS\n",
      "Run 5: 3939.81 GFLOPS\n",
      "Run 6: 3894.70 GFLOPS\n",
      "Run 7: 3815.17 GFLOPS\n",
      "Run 8: 3815.58 GFLOPS\n",
      "Run 9: 3829.82 GFLOPS\n",
      "Run 10: 3854.40 GFLOPS\n",
      "\n",
      "Benchmarking tf32_matmul...\n",
      "Run 1: 11743.99 GFLOPS\n",
      "Run 2: 11760.96 GFLOPS\n",
      "Run 3: 11752.08 GFLOPS\n",
      "Run 4: 11757.11 GFLOPS\n",
      "Run 5: 11750.89 GFLOPS\n",
      "Run 6: 11743.22 GFLOPS\n",
      "Run 7: 11746.86 GFLOPS\n",
      "Run 8: 11712.84 GFLOPS\n",
      "Run 9: 11713.53 GFLOPS\n",
      "Run 10: 11762.01 GFLOPS\n",
      "\n",
      "Performance Results for cupy:\n",
      "  Minimum: 3815.17 GFLOPS\n",
      "  Maximum: 3947.49 GFLOPS\n",
      "  Average: 3891.83 GFLOPS\n",
      "  Std Dev: 54.37 GFLOPS\n",
      "\n",
      "Performance Results for tf32_matmul:\n",
      "  Minimum: 11712.84 GFLOPS\n",
      "  Maximum: 11762.01 GFLOPS\n",
      "  Average: 11744.35 GFLOPS\n",
      "  Std Dev: 16.75 GFLOPS\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from fast_matmul_wrapper import tf32_matmul\n",
    "import time\n",
    "\n",
    "# Print cupy configuration\n",
    "print(f\"cupy version: {cp.__version__}\")\n",
    "print(f\"CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "print(f\"Device: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}\")\n",
    "\n",
    "# Try to get cuBLAS configuration\n",
    "try:\n",
    "    handle = cp.cuda.device.get_cublas_handle()\n",
    "    print(\"cuBLAS handle obtained\")\n",
    "except:\n",
    "    print(\"Could not get cuBLAS handle\")\n",
    "\n",
    "# Create test matrices; make sure not to normalise matricies or cupy will drop to 100Gflops!!\n",
    "a = np.random.rand(5120, 5120).astype(np.float32)\n",
    "b = np.random.rand(5120, 5120).astype(np.float32)\n",
    "\n",
    "# Convert to cupy\n",
    "a_cp = cp.asarray(a)\n",
    "b_cp = cp.asarray(b)\n",
    "\n",
    "# Function to run a single timed matmul\n",
    "def timed_matmul(func, a, b, name=\"\"):\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    c = func(a, b)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    exec_time = end - start\n",
    "    flops = 2.0 * float(5120**3)\n",
    "    gflops = (flops / 1e9) / exec_time\n",
    "    return gflops, c\n",
    "\n",
    "# Try different ways of calling matmul\n",
    "print(\"\\nTesting different matmul approaches:\")\n",
    "\n",
    "# 1. Regular cupy matmul\n",
    "gflops, c1 = timed_matmul(cp.matmul, a_cp, b_cp, \"cupy.matmul\")\n",
    "print(f\"cupy.matmul: {gflops:.2f} GFLOPS\")\n",
    "\n",
    "# 2. Our tf32_matmul\n",
    "gflops, c2 = timed_matmul(tf32_matmul, a_cp, b_cp, \"tf32_matmul\")\n",
    "print(f\"tf32_matmul: {gflops:.2f} GFLOPS\")\n",
    "\n",
    "# Compare results\n",
    "print(f\"\\nMax difference between implementations: {cp.max(cp.abs(c1 - c2))}\")\n",
    "\n",
    "# Now do full benchmark\n",
    "def run_benchmark(func, name, warmup=5, runs=10):\n",
    "    print(f\"\\nBenchmarking {name}...\")\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = func(a_cp, b_cp)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    \n",
    "    # Timing runs\n",
    "    times = []\n",
    "    for i in range(runs):\n",
    "        gflops, _ = timed_matmul(func, a_cp, b_cp)\n",
    "        times.append(gflops)\n",
    "        print(f\"Run {i+1}: {gflops:.2f} GFLOPS\")\n",
    "    return times\n",
    "\n",
    "# Run benchmarks\n",
    "times_cupy = run_benchmark(cp.matmul, \"cupy\")\n",
    "times_custom = run_benchmark(tf32_matmul, \"tf32_matmul\")\n",
    "\n",
    "# Print statistics\n",
    "def print_stats(name, times):\n",
    "    print(f\"\\nPerformance Results for {name}:\")\n",
    "    print(f\"  Minimum: {min(times):.2f} GFLOPS\")\n",
    "    print(f\"  Maximum: {max(times):.2f} GFLOPS\")\n",
    "    print(f\"  Average: {sum(times)/len(times):.2f} GFLOPS\")\n",
    "    print(f\"  Std Dev: {np.std(times):.2f} GFLOPS\")\n",
    "\n",
    "print_stats(\"cupy\", times_cupy)\n",
    "print_stats(\"tf32_matmul\", times_custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cupy version: 13.3.0\n",
    "CUDA version: 12060\n",
    "Device: NVIDIA GeForce RTX 4060\n",
    "cuBLAS handle obtained\n",
    "\n",
    "Testing different matmul approaches:\n",
    "cupy.matmul: 8678.26 GFLOPS\n",
    "tf32_matmul: 12975.45 GFLOPS\n",
    "\n",
    "Max difference between implementations: 130.9873046875\n",
    "\n",
    "Benchmarking cupy...\n",
    "Run 1: 7368.19 GFLOPS\n",
    "Run 2: 7264.97 GFLOPS\n",
    "Run 3: 7343.90 GFLOPS\n",
    "Run 4: 7289.49 GFLOPS\n",
    "Run 5: 7249.27 GFLOPS\n",
    "Run 6: 7283.76 GFLOPS\n",
    "Run 7: 7248.43 GFLOPS\n",
    "Run 8: 7171.27 GFLOPS\n",
    "Run 9: 7401.01 GFLOPS\n",
    "Run 10: 7240.99 GFLOPS\n",
    "\n",
    "Benchmarking tf32_matmul...\n",
    "Run 1: 13926.38 GFLOPS\n",
    "Run 2: 13990.53 GFLOPS\n",
    "Run 3: 13856.14 GFLOPS\n",
    "Run 4: 14022.57 GFLOPS\n",
    "Run 5: 13802.79 GFLOPS\n",
    "Run 6: 13854.66 GFLOPS\n",
    "Run 7: 13866.73 GFLOPS\n",
    "Run 8: 13848.80 GFLOPS\n",
    "Run 9: 13809.18 GFLOPS\n",
    "Run 10: 13646.37 GFLOPS\n",
    "\n",
    "Performance Results for cupy:\n",
    "  Minimum: 7171.27 GFLOPS\n",
    "  Maximum: 7401.01 GFLOPS\n",
    "  Average: 7286.13 GFLOPS\n",
    "  Std Dev: 64.61 GFLOPS\n",
    "\n",
    "Performance Results for tf32_matmul:\n",
    "  Minimum: 13646.37 GFLOPS\n",
    "  Maximum: 14022.57 GFLOPS\n",
    "  Average: 13862.41 GFLOPS\n",
    "  Std Dev: 99.76 GFLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing non-normalized matrices:\n",
      "GFLOPS: 4231.76\n",
      "Max value in result: 1368.1602783203125\n",
      "\n",
      "Testing normalized matrices:\n",
      "GFLOPS: 105.38\n",
      "Max value in result: 0.2668110103148127\n"
     ]
    }
   ],
   "source": [
    "# normalising values kills cupy performanc from 4,000gflops to only 100gflops!!\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Test both normal and normalized matrices\n",
    "def test_both_versions():\n",
    "    # Version 1: No normalization\n",
    "    a1 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    b1 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    a1_cp = cp.asarray(a1)\n",
    "    b1_cp = cp.asarray(b1)\n",
    "\n",
    "    # Version 2: With normalization\n",
    "    a2 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    b2 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    a2 = a2 / np.sqrt(a2.shape[1])\n",
    "    b2 = b2 / np.sqrt(b2.shape[0])\n",
    "    a2_cp = cp.asarray(a2)\n",
    "    b2_cp = cp.asarray(b2)\n",
    "\n",
    "    def benchmark(a, b, name):\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        c = cp.matmul(a, b)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        exec_time = end - start\n",
    "        flops = 2.0 * float(5120**3)\n",
    "        gflops = (flops / 1e9) / exec_time\n",
    "        return gflops, c\n",
    "\n",
    "    print(\"Testing non-normalized matrices:\")\n",
    "    gflops1, c1 = benchmark(a1_cp, b1_cp, \"non-normalized\")\n",
    "    print(f\"GFLOPS: {gflops1:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c1))}\")\n",
    "\n",
    "    print(\"\\nTesting normalized matrices:\")\n",
    "    gflops2, c2 = benchmark(a2_cp, b2_cp, \"normalized\")\n",
    "    print(f\"GFLOPS: {gflops2:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c2))}\")\n",
    "\n",
    "test_both_versions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slowdown in the cuBLAS kernel is not as noticable (it is there but still running at 10Tflops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with cupy.matmul:\n",
      "Non-normalized matrices:\n",
      "GFLOPS: 4239.47\n",
      "Max value in result: 1364.2012939453125\n",
      "\n",
      "Normalized matrices:\n",
      "GFLOPS: 106.19\n",
      "Max value in result: 0.26554513465560065\n",
      "\n",
      "Testing with tf32_matmul:\n",
      "Non-normalized matrices:\n",
      "GFLOPS: 14639.32\n",
      "Max value in result: 1360.1336669921875\n",
      "\n",
      "Normalized matrices:\n",
      "GFLOPS: 10620.87\n",
      "Max value in result: 0.2670082747936249\n",
      "\n",
      "Verifying results:\n",
      "Non-normalized max difference: 121.69861\n",
      "Normalized max difference: 0.023368133077295183\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import time\n",
    "from fast_matmul_wrapper import tf32_matmul\n",
    "\n",
    "# Test both normal and normalized matrices\n",
    "def test_both_versions():\n",
    "    # Version 1: No normalization\n",
    "    a1 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    b1 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    a1_cp = cp.asarray(a1)\n",
    "    b1_cp = cp.asarray(b1)\n",
    "\n",
    "    # Version 2: With normalization\n",
    "    a2 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    b2 = np.random.rand(5120, 5120).astype(np.float32)\n",
    "    a2 = a2 / np.sqrt(a2.shape[1])\n",
    "    b2 = b2 / np.sqrt(b2.shape[0])\n",
    "    a2_cp = cp.asarray(a2)\n",
    "    b2_cp = cp.asarray(b2)\n",
    "\n",
    "    def benchmark_cupy(a, b):\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        c = cp.matmul(a, b)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        exec_time = end - start\n",
    "        flops = 2.0 * float(5120**3)\n",
    "        gflops = (flops / 1e9) / exec_time\n",
    "        return gflops, c\n",
    "\n",
    "    def benchmark_tf32_matmul(a, b):\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        c = tf32_matmul(a, b)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        exec_time = end - start\n",
    "        flops = 2.0 * float(5120**3)\n",
    "        gflops = (flops / 1e9) / exec_time\n",
    "        return gflops, c\n",
    "\n",
    "    # Test cupy.matmul\n",
    "    print(\"Testing with cupy.matmul:\")\n",
    "    print(\"Non-normalized matrices:\")\n",
    "    gflops1, c1 = benchmark_cupy(a1_cp, b1_cp)\n",
    "    print(f\"GFLOPS: {gflops1:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c1))}\")\n",
    "\n",
    "    print(\"\\nNormalized matrices:\")\n",
    "    gflops2, c2 = benchmark_cupy(a2_cp, b2_cp)\n",
    "    print(f\"GFLOPS: {gflops2:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c2))}\")\n",
    "\n",
    "    # Test tf32_matmul\n",
    "    print(\"\\nTesting with tf32_matmul:\")\n",
    "    print(\"Non-normalized matrices:\")\n",
    "    gflops3, c3 = benchmark_tf32_matmul(a1_cp, b1_cp)\n",
    "    print(f\"GFLOPS: {gflops3:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c3))}\")\n",
    "\n",
    "    print(\"\\nNormalized matrices:\")\n",
    "    gflops4, c4 = benchmark_tf32_matmul(a2_cp, b2_cp)\n",
    "    print(f\"GFLOPS: {gflops4:.2f}\")\n",
    "    print(f\"Max value in result: {cp.max(cp.abs(c4))}\")\n",
    "\n",
    "    # Verify results match between implementations\n",
    "    print(\"\\nVerifying results:\")\n",
    "    print(\"Non-normalized max difference:\", cp.max(cp.abs(c1 - c3)))\n",
    "    print(\"Normalized max difference:\", cp.max(cp.abs(c2 - c4)))\n",
    "\n",
    "test_both_versions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RTX 4060 has much slower cuBLAS performance for normalised matrices and much higher cupy\n",
    "\n",
    "Testing with cupy.matmul:\n",
    "Non-normalized matrices:\n",
    "GFLOPS: 8001.40\n",
    "Max value in result: 1361.484619140625\n",
    "\n",
    "Normalized matrices:\n",
    "GFLOPS: 203.98\n",
    "Max value in result: 0.26614665120703523\n",
    "\n",
    "Testing with tf32_matmul:\n",
    "Non-normalized matrices:\n",
    "GFLOPS: 14230.39\n",
    "Max value in result: 1362.4464111328125\n",
    "\n",
    "Normalized matrices:\n",
    "GFLOPS: 3094.93\n",
    "Max value in result: 0.2662442624568939\n",
    "\n",
    "Verifying results:\n",
    "Non-normalized max difference: 122.25574\n",
    "Normalized max difference: 0.02260998545646506\n",
    "\n",
    "A1000 tf32_matmul suffers less of a slowdown with normalised:\n",
    "\n",
    "Testing with cupy.matmul:\n",
    "Non-normalized matrices:\n",
    "GFLOPS: 4239.47\n",
    "Max value in result: 1364.2012939453125\n",
    "\n",
    "Normalized matrices:\n",
    "GFLOPS: 106.19\n",
    "Max value in result: 0.26554513465560065\n",
    "\n",
    "Testing with tf32_matmul:\n",
    "Non-normalized matrices:\n",
    "GFLOPS: 14639.32\n",
    "Max value in result: 1360.1336669921875\n",
    "\n",
    "Normalized matrices:\n",
    "GFLOPS: 10620.87\n",
    "Max value in result: 0.2670082747936249\n",
    "\n",
    "Verifying results:\n",
    "Non-normalized max difference: 121.69861\n",
    "Normalized max difference: 0.023368133077295183"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
