{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by numpy:  11.670009851455688\n",
      "Time taken by cupy:  9.9381263256073\n"
     ]
    }
   ],
   "source": [
    "# original test from cupy\n",
    "import time  \n",
    "import cupy as cp  \n",
    "import numpy as np\n",
    "\n",
    "# create a random numpy array of size 1000x1000  \n",
    "a = np.random.rand(1000, 1000)\n",
    "\n",
    "# create a random cupy array of size 1000x1000  \n",
    "b = cp.random.rand(1000, 1000)\n",
    "\n",
    "# start timer  \n",
    "start = time.time()\n",
    "\n",
    "# perform some random operations on numpy array  \n",
    "for i in range(1000):  \n",
    "    a = np.dot(a, a)\n",
    "\n",
    "# print time taken  \n",
    "print(\"Time taken by numpy: \", time.time() - start)\n",
    "\n",
    "# start timer  \n",
    "start = time.time()\n",
    "\n",
    "# perform some random operations on cupy array  \n",
    "for i in range(1000):  \n",
    "    b = cp.dot(b, b)\n",
    "\n",
    "# print time taken  \n",
    "print(\"Time taken by cupy: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CUDA Device 0>\n"
     ]
    }
   ],
   "source": [
    "print(b.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA Fortran kernel using cuBLAS tested for same accuracy as cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise fortran library\n",
    "import numpy as np\n",
    "from numpy.ctypeslib import ndpointer\n",
    "import ctypes\n",
    "import time\n",
    "\n",
    "# Load the shared library\n",
    "lib = ctypes.CDLL('./libcudamatmul_double.so')\n",
    "\n",
    "# Define argument types\n",
    "lib.py_matrix_dot.argtypes = [\n",
    "    ndpointer(dtype=np.float64, flags='C_CONTIGUOUS'),\n",
    "    ndpointer(dtype=np.float64, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int\n",
    "]\n",
    "\n",
    "def cuda_dot_double(a, iterations=1000):\n",
    "    \"\"\"Fast matrix power using CUDA Fortran with binary exponentiation\"\"\"\n",
    "    if not isinstance(a, np.ndarray):\n",
    "        raise TypeError(\"Input must be a numpy array\")\n",
    "    \n",
    "    if a.dtype != np.float64:\n",
    "        a = np.asarray(a, dtype=np.float64)\n",
    "    \n",
    "    if a.shape[0] != a.shape[1]:\n",
    "        raise ValueError(\"Matrix must be square\")\n",
    "    \n",
    "    if not a.flags['C_CONTIGUOUS']:\n",
    "        a = np.ascontiguousarray(a)\n",
    "    \n",
    "    n = a.shape[0]\n",
    "    result = np.empty_like(a)\n",
    "    \n",
    "    lib.py_matrix_dot(a, result, n, iterations)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by NumPy: 12.796 seconds\n",
      "Time taken by CuPy: 20.359 seconds\n",
      "Time taken by CUDA Fortran: 20.328 seconds\n",
      "\n",
      "Checking results (first few elements):\n",
      "NumPy result:\n",
      "[[3.08320013e-66 3.09295878e-66]\n",
      " [3.08024097e-66 3.08999025e-66]]\n",
      "\n",
      "CuPy result:\n",
      "[[3.08320013e-66 3.09295878e-66]\n",
      " [3.08024097e-66 3.08999025e-66]]\n",
      "\n",
      "CUDA Fortran result:\n",
      "[[3.08320013e-66 3.09295878e-66]\n",
      " [3.08024097e-66 3.08999025e-66]]\n",
      "\n",
      "Checking for infinities or NaNs:\n",
      "NumPy contains inf: False\n",
      "NumPy contains NaN: False\n",
      "CuPy contains inf: False\n",
      "CuPy contains NaN: False\n",
      "CUDA contains inf: False\n",
      "CUDA contains NaN: False\n",
      "\n",
      "Accuracy comparison (maximum absolute differences):\n",
      "NumPy vs CuPy:         1.05e-80\n",
      "NumPy vs CUDA Fortran: 1.21e-80\n",
      "CuPy vs CUDA Fortran:  1.37e-80\n",
      "\n",
      "Speedup ratios:\n",
      "CUDA Fortran vs NumPy: 0.6x faster\n",
      "CUDA Fortran vs CuPy:  1.0x faster\n",
      "CuPy vs NumPy:         0.6x faster\n"
     ]
    }
   ],
   "source": [
    "import time  \n",
    "import cupy as cp  \n",
    "import numpy as np\n",
    "\n",
    "# create a random numpy array of size 1000x1000 and scale it\n",
    "a_np = np.random.rand(1000, 1000)\n",
    "# Modified scaling\n",
    "a_np = np.random.rand(1000, 1000)\n",
    "scale_factor = np.linalg.norm(a_np)\n",
    "a_np = a_np / (scale_factor * np.power(1.1, 1/1000))  # More gradual scaling\n",
    "\n",
    "a_cp = cp.array(a_np)  # same scaled data for CuPy\n",
    "a_cuda = a_np.copy()   # same scaled data for CUDA Fortran\n",
    "\n",
    "# NumPy test\n",
    "start = time.time()\n",
    "result_numpy = a_np.copy()\n",
    "for i in range(1000):  \n",
    "    result_numpy = np.dot(result_numpy, a_np)\n",
    "numpy_time = time.time() - start\n",
    "print(f\"Time taken by NumPy: {numpy_time:.3f} seconds\")\n",
    "\n",
    "# CuPy test\n",
    "start = time.time()\n",
    "result_cupy = a_cp.copy()\n",
    "for i in range(1000):  \n",
    "    result_cupy = cp.dot(result_cupy, a_cp)\n",
    "result_cupy = cp.asnumpy(result_cupy)\n",
    "cupy_time = time.time() - start\n",
    "print(f\"Time taken by CuPy: {cupy_time:.3f} seconds\")\n",
    "\n",
    "# CUDA Fortran test\n",
    "start = time.time()\n",
    "result_cuda = cuda_dot_double(a_cuda, 1000)\n",
    "cuda_time = time.time() - start\n",
    "print(f\"Time taken by CUDA Fortran: {cuda_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nChecking results (first few elements):\")\n",
    "print(\"NumPy result:\")\n",
    "print(result_numpy[0:2, 0:2])\n",
    "print(\"\\nCuPy result:\")\n",
    "print(result_cupy[0:2, 0:2])\n",
    "print(\"\\nCUDA Fortran result:\")\n",
    "print(result_cuda[0:2, 0:2])\n",
    "\n",
    "print(\"\\nChecking for infinities or NaNs:\")\n",
    "print(f\"NumPy contains inf: {np.isinf(result_numpy).any()}\")\n",
    "print(f\"NumPy contains NaN: {np.isnan(result_numpy).any()}\")\n",
    "print(f\"CuPy contains inf: {np.isinf(result_cupy).any()}\")\n",
    "print(f\"CuPy contains NaN: {np.isnan(result_cupy).any()}\")\n",
    "print(f\"CUDA contains inf: {np.isinf(result_cuda).any()}\")\n",
    "print(f\"CUDA contains NaN: {np.isnan(result_cuda).any()}\")\n",
    "\n",
    "if not (np.isinf(result_numpy).any() or np.isinf(result_cupy).any() or np.isinf(result_cuda).any()):\n",
    "    print(\"\\nAccuracy comparison (maximum absolute differences):\")\n",
    "    print(f\"NumPy vs CuPy:         {np.max(np.abs(result_numpy - result_cupy)):.2e}\")\n",
    "    print(f\"NumPy vs CUDA Fortran: {np.max(np.abs(result_numpy - result_cuda)):.2e}\")\n",
    "    print(f\"CuPy vs CUDA Fortran:  {np.max(np.abs(result_cupy - result_cuda)):.2e}\")\n",
    "\n",
    "print(\"\\nSpeedup ratios:\")\n",
    "print(f\"CUDA Fortran vs NumPy: {numpy_time/cuda_time:.1f}x faster\")\n",
    "print(f\"CUDA Fortran vs CuPy:  {cupy_time/cuda_time:.1f}x faster\")\n",
    "print(f\"CuPy vs NumPy:         {numpy_time/cupy_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy configuration:\n",
      "OS                           : Linux-6.12.9-200.fc41.x86_64-x86_64-with-glibc2.40\n",
      "Python Version               : 3.12.7\n",
      "CuPy Version                 : 13.3.0\n",
      "CuPy Platform                : NVIDIA CUDA\n",
      "NumPy Version                : 2.2.1\n",
      "SciPy Version                : 1.14.1\n",
      "Cython Build Version         : 0.29.36\n",
      "Cython Runtime Version       : None\n",
      "CUDA Root                    : /var/home/fraser/nvidia/hpc_sdk/Linux_x86_64/24.11/cuda\n",
      "nvcc PATH                    : /var/home/fraser/nvidia/hpc_sdk/Linux_x86_64/24.11/cuda/bin/nvcc\n",
      "CUDA Build Version           : 12060\n",
      "CUDA Driver Version          : 12070\n",
      "CUDA Runtime Version         : 12060 (linked to CuPy) / 12060 (locally installed)\n",
      "CUDA Extra Include Dirs      : []\n",
      "cuBLAS Version               : (available)\n",
      "cuFFT Version                : 11300\n",
      "cuRAND Version               : 10307\n",
      "cuSOLVER Version             : (11, 7, 1)\n",
      "cuSPARSE Version             : (available)\n",
      "NVRTC Version                : (12, 6)\n",
      "Thrust Version               : 200600\n",
      "CUB Build Version            : 200600\n",
      "Jitify Build Version         : <unknown>\n",
      "cuDNN Build Version          : (not loaded; try `import cupy.cuda.cudnn` first)\n",
      "cuDNN Version                : (not loaded; try `import cupy.cuda.cudnn` first)\n",
      "NCCL Build Version           : (not loaded; try `import cupy.cuda.nccl` first)\n",
      "NCCL Runtime Version         : (not loaded; try `import cupy.cuda.nccl` first)\n",
      "cuTENSOR Version             : 20002\n",
      "cuSPARSELt Build Version     : None\n",
      "Device 0 Name                : NVIDIA RTX A1000 Laptop GPU\n",
      "Device 0 Compute Capability  : 86\n",
      "Device 0 PCI Bus ID          : 0000:01:00.0\n",
      "None\n",
      "\n",
      "Initial matrix first few elements:\n",
      "NumPy: [[0.00064838 0.00164582]\n",
      " [0.00032049 0.00093811]]\n",
      "CuPy: [[0.00064838 0.00164582]\n",
      " [0.00032049 0.00093811]]\n",
      "CUDA input: [[0.00064838 0.00164582]\n",
      " [0.00032049 0.00093811]]\n",
      "\n",
      "After first multiplication:\n",
      "NumPy: [[0.000725   0.00072679]\n",
      " [0.00074655 0.00073909]]\n",
      "CuPy: [[0.000725   0.00072679]\n",
      " [0.00074655 0.00073909]]\n",
      "CUDA Fortran: [[0.000725   0.00072679]\n",
      " [0.00074655 0.00073909]]\n",
      "\n",
      "Running full tests...\n",
      "Time taken by NumPy: 10.281 seconds\n",
      "Time taken by CuPy: 20.219 seconds\n",
      "Time taken by CUDA Fortran: 20.265 seconds\n",
      "\n",
      "Final results (first few elements):\n",
      "NumPy result:\n",
      "[[3.35143386e-66 3.34763332e-66]\n",
      " [3.46536238e-66 3.46143265e-66]]\n",
      "\n",
      "CuPy result:\n",
      "[[3.35143386e-66 3.34763332e-66]\n",
      " [3.46536238e-66 3.46143265e-66]]\n",
      "\n",
      "CUDA Fortran result:\n",
      "[[3.35143386e-66 3.34763332e-66]\n",
      " [3.46536238e-66 3.46143265e-66]]\n",
      "\n",
      "Results after 1 iterations:\n",
      "NumPy[0,0]: 7.250007e-04\n",
      "CuPy[0,0]:  7.250007e-04\n",
      "CUDA[0,0]:  7.250007e-04\n",
      "Ratios:\n",
      "CUDA/NumPy: 1.000000\n",
      "CUDA/CuPy:  1.000000\n",
      "\n",
      "Results after 10 iterations:\n",
      "NumPy[0,0]: 1.985871e-04\n",
      "CuPy[0,0]:  1.985871e-04\n",
      "CUDA[0,0]:  1.985871e-04\n",
      "Ratios:\n",
      "CUDA/NumPy: 1.000000\n",
      "CUDA/CuPy:  1.000000\n",
      "\n",
      "Results after 100 iterations:\n",
      "NumPy[0,0]: 4.811153e-10\n",
      "CuPy[0,0]:  4.811153e-10\n",
      "CUDA[0,0]:  4.811153e-10\n",
      "Ratios:\n",
      "CUDA/NumPy: 1.000000\n",
      "CUDA/CuPy:  1.000000\n"
     ]
    }
   ],
   "source": [
    "import time  \n",
    "import cupy as cp  \n",
    "import numpy as np\n",
    "#from numba import cuda\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print CuPy configuration\n",
    "print(\"CuPy configuration:\")\n",
    "print(cp.show_config())\n",
    "\n",
    "# Create a random numpy array of size 1000x1000 and scale it\n",
    "a_np = np.random.rand(1000, 1000)\n",
    "scale_factor = np.linalg.norm(a_np)\n",
    "a_np = a_np / (scale_factor * np.power(1.1, 1/1000))  # Scale to prevent overflow\n",
    "\n",
    "# Use the same input data for all three methods\n",
    "a_cp = cp.array(a_np)  # Copy to GPU for CuPy\n",
    "a_cuda = a_np.copy()   # Copy for CUDA Fortran\n",
    "\n",
    "print(\"\\nInitial matrix first few elements:\")\n",
    "print(\"NumPy:\", a_np[0:2, 0:2])\n",
    "print(\"CuPy:\", cp.asnumpy(a_cp)[0:2, 0:2])\n",
    "print(\"CUDA input:\", a_cuda[0:2, 0:2])\n",
    "\n",
    "# First multiplication test\n",
    "print(\"\\nAfter first multiplication:\")\n",
    "result_np = np.dot(a_np, a_np)\n",
    "print(\"NumPy:\", result_np[0:2, 0:2])\n",
    "\n",
    "result_cp = cp.dot(a_cp, a_cp)\n",
    "print(\"CuPy:\", cp.asnumpy(result_cp)[0:2, 0:2])\n",
    "\n",
    "result_cuda = cuda_dot_double(a_cuda, 1)\n",
    "print(\"CUDA Fortran:\", result_cuda[0:2, 0:2])\n",
    "\n",
    "print(\"\\nRunning full tests...\")\n",
    "\n",
    "# NumPy test\n",
    "start = time.time()\n",
    "result_numpy = a_np.copy()\n",
    "for i in range(1000):  \n",
    "    result_numpy = np.dot(result_numpy, a_np)\n",
    "numpy_time = time.time() - start\n",
    "print(f\"Time taken by NumPy: {numpy_time:.3f} seconds\")\n",
    "\n",
    "# CuPy test\n",
    "start = time.time()\n",
    "result_cupy = a_cp.copy()\n",
    "for i in range(1000):  \n",
    "    result_cupy = cp.dot(result_cupy, a_cp)\n",
    "result_cupy = cp.asnumpy(result_cupy)\n",
    "cupy_time = time.time() - start\n",
    "print(f\"Time taken by CuPy: {cupy_time:.3f} seconds\")\n",
    "\n",
    "# CUDA Fortran test\n",
    "start = time.time()\n",
    "result_cuda = cuda_dot_double(a_cuda, 1000)\n",
    "cuda_time = time.time() - start\n",
    "print(f\"Time taken by CUDA Fortran: {cuda_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nFinal results (first few elements):\")\n",
    "print(\"NumPy result:\")\n",
    "print(result_numpy[0:2, 0:2])\n",
    "print(\"\\nCuPy result:\")\n",
    "print(result_cupy[0:2, 0:2])\n",
    "print(\"\\nCUDA Fortran result:\")\n",
    "print(result_cuda[0:2, 0:2])\n",
    "\n",
    "# Compare results at specific iterations\n",
    "def run_specific_iterations(n_iters):\n",
    "    result_np = a_np.copy()\n",
    "    result_cp = a_cp.copy()\n",
    "    result_cu = cuda_dot_double(a_cuda, n_iters)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        result_np = np.dot(result_np, a_np)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        result_cp = cp.dot(result_cp, a_cp)\n",
    "    \n",
    "    return result_np, cp.asnumpy(result_cp), result_cu\n",
    "\n",
    "# Check accuracy at different iterations\n",
    "for iters in [1, 10, 100]:\n",
    "    print(f\"\\nResults after {iters} iterations:\")\n",
    "    res_np, res_cp, res_cu = run_specific_iterations(iters)\n",
    "    print(f\"NumPy[0,0]: {res_np[0,0]:.6e}\")\n",
    "    print(f\"CuPy[0,0]:  {res_cp[0,0]:.6e}\")\n",
    "    print(f\"CUDA[0,0]:  {res_cu[0,0]:.6e}\")\n",
    "    print(f\"Ratios:\")\n",
    "    print(f\"CUDA/NumPy: {res_cu[0,0]/res_np[0,0]:.6f}\")\n",
    "    print(f\"CUDA/CuPy:  {res_cu[0,0]/res_cp[0,0]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise fortran library for tensor core version\n",
    "import numpy as np\n",
    "from numpy.ctypeslib import ndpointer\n",
    "import ctypes\n",
    "import time\n",
    "\n",
    "# Load the shared library\n",
    "lib = ctypes.CDLL('./libcudamatmul_tensor.so')\n",
    "\n",
    "# Define argument types\n",
    "lib.py_matrix_dot.argtypes = [\n",
    "    ndpointer(dtype=np.float64, flags='C_CONTIGUOUS'),\n",
    "    ndpointer(dtype=np.float64, flags='C_CONTIGUOUS'),\n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int\n",
    "]\n",
    "\n",
    "def cuda_dot_tensor(a, iterations=1000):\n",
    "    \"\"\"Fast matrix power using CUDA Fortran with binary exponentiation\"\"\"\n",
    "    if not isinstance(a, np.ndarray):\n",
    "        raise TypeError(\"Input must be a numpy array\")\n",
    "    \n",
    "    if a.dtype != np.float64:\n",
    "        a = np.asarray(a, dtype=np.float64)\n",
    "    \n",
    "    if a.shape[0] != a.shape[1]:\n",
    "        raise ValueError(\"Matrix must be square\")\n",
    "    \n",
    "    if not a.flags['C_CONTIGUOUS']:\n",
    "        a = np.ascontiguousarray(a)\n",
    "    \n",
    "    n = a.shape[0]\n",
    "    result = np.empty_like(a)\n",
    "    \n",
    "    lib.py_matrix_dot(a, result, n, iterations)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by NumPy: 8.676 seconds\n",
      "Time taken by CuPy: 20.319 seconds\n",
      "Time taken by CUDA Fortran: 0.509 seconds\n",
      "\n",
      "Checking results (first few elements):\n",
      "NumPy result:\n",
      "[[2.60460916e-66 2.62017611e-66]\n",
      " [2.59865694e-66 2.61418831e-66]]\n",
      "\n",
      "CuPy result:\n",
      "[[2.60460916e-66 2.62017611e-66]\n",
      " [2.59865694e-66 2.61418831e-66]]\n",
      "\n",
      "CUDA Fortran result:\n",
      "[[1.02058017e-07 1.02058017e-07]\n",
      " [1.01911084e-07 1.01911084e-07]]\n",
      "\n",
      "Checking for infinities or NaNs:\n",
      "NumPy contains inf: False\n",
      "NumPy contains NaN: False\n",
      "CuPy contains inf: False\n",
      "CuPy contains NaN: False\n",
      "CUDA contains inf: False\n",
      "CUDA contains NaN: False\n",
      "\n",
      "Accuracy comparison (maximum absolute differences):\n",
      "NumPy vs CuPy:         9.49e-81\n",
      "NumPy vs CUDA Fortran: 1.10e-07\n",
      "CuPy vs CUDA Fortran:  1.10e-07\n",
      "\n",
      "Speedup ratios:\n",
      "CUDA Fortran vs NumPy: 17.1x faster\n",
      "CUDA Fortran vs CuPy:  39.9x faster\n",
      "CuPy vs NumPy:         0.4x faster\n"
     ]
    }
   ],
   "source": [
    "import time  \n",
    "import cupy as cp  \n",
    "import numpy as np\n",
    "\n",
    "# create a random numpy array of size 1000x1000 and scale it\n",
    "a_np = np.random.rand(1000, 1000)\n",
    "# Modified scaling\n",
    "a_np = np.random.rand(1000, 1000)\n",
    "scale_factor = np.linalg.norm(a_np)\n",
    "a_np = a_np / (scale_factor * np.power(1.1, 1/1000))  # More gradual scaling\n",
    "\n",
    "a_cp = cp.array(a_np)  # same scaled data for CuPy\n",
    "a_cuda = a_np.copy()   # same scaled data for CUDA Fortran\n",
    "\n",
    "# NumPy test\n",
    "start = time.time()\n",
    "result_numpy = a_np.copy()\n",
    "for i in range(1000):  \n",
    "    result_numpy = np.dot(result_numpy, a_np)\n",
    "numpy_time = time.time() - start\n",
    "print(f\"Time taken by NumPy: {numpy_time:.3f} seconds\")\n",
    "\n",
    "# CuPy test\n",
    "start = time.time()\n",
    "result_cupy = a_cp.copy()\n",
    "for i in range(1000):  \n",
    "    result_cupy = cp.dot(result_cupy, a_cp)\n",
    "result_cupy = cp.asnumpy(result_cupy)\n",
    "cupy_time = time.time() - start\n",
    "print(f\"Time taken by CuPy: {cupy_time:.3f} seconds\")\n",
    "\n",
    "# CUDA Fortran test\n",
    "start = time.time()\n",
    "result_cuda = cuda_dot_tensor(a_cuda, 1000)\n",
    "cuda_time = time.time() - start\n",
    "print(f\"Time taken by CUDA Fortran: {cuda_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nChecking results (first few elements):\")\n",
    "print(\"NumPy result:\")\n",
    "print(result_numpy[0:2, 0:2])\n",
    "print(\"\\nCuPy result:\")\n",
    "print(result_cupy[0:2, 0:2])\n",
    "print(\"\\nCUDA Fortran result:\")\n",
    "print(result_cuda[0:2, 0:2])\n",
    "\n",
    "print(\"\\nChecking for infinities or NaNs:\")\n",
    "print(f\"NumPy contains inf: {np.isinf(result_numpy).any()}\")\n",
    "print(f\"NumPy contains NaN: {np.isnan(result_numpy).any()}\")\n",
    "print(f\"CuPy contains inf: {np.isinf(result_cupy).any()}\")\n",
    "print(f\"CuPy contains NaN: {np.isnan(result_cupy).any()}\")\n",
    "print(f\"CUDA contains inf: {np.isinf(result_cuda).any()}\")\n",
    "print(f\"CUDA contains NaN: {np.isnan(result_cuda).any()}\")\n",
    "\n",
    "if not (np.isinf(result_numpy).any() or np.isinf(result_cupy).any() or np.isinf(result_cuda).any()):\n",
    "    print(\"\\nAccuracy comparison (maximum absolute differences):\")\n",
    "    print(f\"NumPy vs CuPy:         {np.max(np.abs(result_numpy - result_cupy)):.2e}\")\n",
    "    print(f\"NumPy vs CUDA Fortran: {np.max(np.abs(result_numpy - result_cuda)):.2e}\")\n",
    "    print(f\"CuPy vs CUDA Fortran:  {np.max(np.abs(result_cupy - result_cuda)):.2e}\")\n",
    "\n",
    "print(\"\\nSpeedup ratios:\")\n",
    "print(f\"CUDA Fortran vs NumPy: {numpy_time/cuda_time:.1f}x faster\")\n",
    "print(f\"CUDA Fortran vs CuPy:  {cupy_time/cuda_time:.1f}x faster\")\n",
    "print(f\"CuPy vs NumPy:         {numpy_time/cupy_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy configuration:\n",
      "OS                           : Linux-6.12.9-200.fc41.x86_64-x86_64-with-glibc2.40\n",
      "Python Version               : 3.12.7\n",
      "CuPy Version                 : 13.3.0\n",
      "CuPy Platform                : NVIDIA CUDA\n",
      "NumPy Version                : 2.2.1\n",
      "SciPy Version                : 1.14.1\n",
      "Cython Build Version         : 0.29.36\n",
      "Cython Runtime Version       : None\n",
      "CUDA Root                    : /var/home/fraser/nvidia/hpc_sdk/Linux_x86_64/24.11/cuda\n",
      "nvcc PATH                    : /var/home/fraser/nvidia/hpc_sdk/Linux_x86_64/24.11/cuda/bin/nvcc\n",
      "CUDA Build Version           : 12060\n",
      "CUDA Driver Version          : 12070\n",
      "CUDA Runtime Version         : 12060 (linked to CuPy) / 12060 (locally installed)\n",
      "CUDA Extra Include Dirs      : []\n",
      "cuBLAS Version               : (available)\n",
      "cuFFT Version                : 11300\n",
      "cuRAND Version               : 10307\n",
      "cuSOLVER Version             : (11, 7, 1)\n",
      "cuSPARSE Version             : (available)\n",
      "NVRTC Version                : (12, 6)\n",
      "Thrust Version               : 200600\n",
      "CUB Build Version            : 200600\n",
      "Jitify Build Version         : <unknown>\n",
      "cuDNN Build Version          : (not loaded; try `import cupy.cuda.cudnn` first)\n",
      "cuDNN Version                : (not loaded; try `import cupy.cuda.cudnn` first)\n",
      "NCCL Build Version           : (not loaded; try `import cupy.cuda.nccl` first)\n",
      "NCCL Runtime Version         : (not loaded; try `import cupy.cuda.nccl` first)\n",
      "cuTENSOR Version             : 20002\n",
      "cuSPARSELt Build Version     : None\n",
      "Device 0 Name                : NVIDIA RTX A1000 Laptop GPU\n",
      "Device 0 Compute Capability  : 86\n",
      "Device 0 PCI Bus ID          : 0000:01:00.0\n",
      "None\n",
      "\n",
      "Initial matrix first few elements:\n",
      "NumPy: [[0.00064838 0.00164582]\n",
      " [0.00032049 0.00093811]]\n",
      "CuPy: [[0.00064838 0.00164582]\n",
      " [0.00032049 0.00093811]]\n",
      "CUDA input: [[0.00064838 0.00164582]\n",
      " [0.00032049 0.00093811]]\n",
      "\n",
      "After first multiplication:\n",
      "NumPy: [[0.000725   0.00072679]\n",
      " [0.00074655 0.00073909]]\n",
      "CuPy: [[0.000725   0.00072679]\n",
      " [0.00074655 0.00073909]]\n",
      "CUDA Fortran: [[0.00072499 0.00072678]\n",
      " [0.00074654 0.00073908]]\n",
      "\n",
      "Running full tests...\n",
      "Time taken by NumPy: 9.498 seconds\n",
      "Time taken by CuPy: 20.313 seconds\n",
      "Time taken by CUDA Fortran: 0.426 seconds\n",
      "\n",
      "Final results (first few elements):\n",
      "NumPy result:\n",
      "[[3.35143386e-66 3.34763332e-66]\n",
      " [3.46536238e-66 3.46143265e-66]]\n",
      "\n",
      "CuPy result:\n",
      "[[3.35143386e-66 3.34763332e-66]\n",
      " [3.46536238e-66 3.46143265e-66]]\n",
      "\n",
      "CUDA Fortran result:\n",
      "[[1.01173541e-07 1.01173541e-07]\n",
      " [1.04631418e-07 1.04631418e-07]]\n",
      "\n",
      "Results after 1 iterations:\n",
      "NumPy[0,0]: 7.250007e-04\n",
      "CuPy[0,0]:  7.250007e-04\n",
      "CUDA[0,0]:  7.249885e-04\n",
      "Ratios:\n",
      "CUDA/NumPy: 0.999983\n",
      "CUDA/CuPy:  0.999983\n",
      "\n",
      "Results after 10 iterations:\n",
      "NumPy[0,0]: 1.985871e-04\n",
      "CuPy[0,0]:  1.985871e-04\n",
      "CUDA[0,0]:  1.985755e-04\n",
      "Ratios:\n",
      "CUDA/NumPy: 0.999941\n",
      "CUDA/CuPy:  0.999941\n",
      "\n",
      "Results after 100 iterations:\n",
      "NumPy[0,0]: 4.811153e-10\n",
      "CuPy[0,0]:  4.811153e-10\n",
      "CUDA[0,0]:  1.011735e-07\n",
      "Ratios:\n",
      "CUDA/NumPy: 210.289597\n",
      "CUDA/CuPy:  210.289597\n"
     ]
    }
   ],
   "source": [
    "import time  \n",
    "import cupy as cp  \n",
    "import numpy as np\n",
    "#from numba import cuda\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Print CuPy configuration\n",
    "print(\"CuPy configuration:\")\n",
    "print(cp.show_config())\n",
    "\n",
    "# Create a random numpy array of size 1000x1000 and scale it\n",
    "a_np = np.random.rand(1000, 1000)\n",
    "scale_factor = np.linalg.norm(a_np)\n",
    "a_np = a_np / (scale_factor * np.power(1.1, 1/1000))  # Scale to prevent overflow\n",
    "\n",
    "# Use the same input data for all three methods\n",
    "a_cp = cp.array(a_np)  # Copy to GPU for CuPy\n",
    "a_cuda = a_np.copy()   # Copy for CUDA Fortran\n",
    "\n",
    "print(\"\\nInitial matrix first few elements:\")\n",
    "print(\"NumPy:\", a_np[0:2, 0:2])\n",
    "print(\"CuPy:\", cp.asnumpy(a_cp)[0:2, 0:2])\n",
    "print(\"CUDA input:\", a_cuda[0:2, 0:2])\n",
    "\n",
    "# First multiplication test\n",
    "print(\"\\nAfter first multiplication:\")\n",
    "result_np = np.dot(a_np, a_np)\n",
    "print(\"NumPy:\", result_np[0:2, 0:2])\n",
    "\n",
    "result_cp = cp.dot(a_cp, a_cp)\n",
    "print(\"CuPy:\", cp.asnumpy(result_cp)[0:2, 0:2])\n",
    "\n",
    "result_cuda = cuda_dot_tensor(a_cuda, 1)\n",
    "print(\"CUDA Fortran:\", result_cuda[0:2, 0:2])\n",
    "\n",
    "print(\"\\nRunning full tests...\")\n",
    "\n",
    "# NumPy test\n",
    "start = time.time()\n",
    "result_numpy = a_np.copy()\n",
    "for i in range(1000):  \n",
    "    result_numpy = np.dot(result_numpy, a_np)\n",
    "numpy_time = time.time() - start\n",
    "print(f\"Time taken by NumPy: {numpy_time:.3f} seconds\")\n",
    "\n",
    "# CuPy test\n",
    "start = time.time()\n",
    "result_cupy = a_cp.copy()\n",
    "for i in range(1000):  \n",
    "    result_cupy = cp.dot(result_cupy, a_cp)\n",
    "result_cupy = cp.asnumpy(result_cupy)\n",
    "cupy_time = time.time() - start\n",
    "print(f\"Time taken by CuPy: {cupy_time:.3f} seconds\")\n",
    "\n",
    "# CUDA Fortran test\n",
    "start = time.time()\n",
    "result_cuda = cuda_dot_tensor(a_cuda, 1000)\n",
    "cuda_time = time.time() - start\n",
    "print(f\"Time taken by CUDA Fortran: {cuda_time:.3f} seconds\")\n",
    "\n",
    "print(\"\\nFinal results (first few elements):\")\n",
    "print(\"NumPy result:\")\n",
    "print(result_numpy[0:2, 0:2])\n",
    "print(\"\\nCuPy result:\")\n",
    "print(result_cupy[0:2, 0:2])\n",
    "print(\"\\nCUDA Fortran result:\")\n",
    "print(result_cuda[0:2, 0:2])\n",
    "\n",
    "# Compare results at specific iterations\n",
    "def run_specific_iterations(n_iters):\n",
    "    result_np = a_np.copy()\n",
    "    result_cp = a_cp.copy()\n",
    "    result_cu = cuda_dot_tensor(a_cuda, n_iters)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        result_np = np.dot(result_np, a_np)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        result_cp = cp.dot(result_cp, a_cp)\n",
    "    \n",
    "    return result_np, cp.asnumpy(result_cp), result_cu\n",
    "\n",
    "# Check accuracy at different iterations\n",
    "for iters in [1, 10, 100]:\n",
    "    print(f\"\\nResults after {iters} iterations:\")\n",
    "    res_np, res_cp, res_cu = run_specific_iterations(iters)\n",
    "    print(f\"NumPy[0,0]: {res_np[0,0]:.6e}\")\n",
    "    print(f\"CuPy[0,0]:  {res_cp[0,0]:.6e}\")\n",
    "    print(f\"CUDA[0,0]:  {res_cu[0,0]:.6e}\")\n",
    "    print(f\"Ratios:\")\n",
    "    print(f\"CUDA/NumPy: {res_cu[0,0]/res_np[0,0]:.6f}\")\n",
    "    print(f\"CUDA/CuPy:  {res_cu[0,0]/res_cp[0,0]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first multiplication is extremely accurate (within ~0.002% difference), and the results diverge more over iterations. This makes sense because:\n",
    "\n",
    "1. First iteration: very close match\n",
    "   - NumPy/CuPy: 0.000725\n",
    "   - CUDA: 0.00072499\n",
    "   - Ratio: 0.999983\n",
    "\n",
    "2. After 10 iterations: still very close\n",
    "   - NumPy/CuPy: 1.985871e-04\n",
    "   - CUDA: 1.985755e-04\n",
    "   - Ratio: 0.999941\n",
    "\n",
    "3. After 100 iterations: divergence grows\n",
    "   - NumPy/CuPy: 4.811153e-10\n",
    "   - CUDA: 1.011735e-07\n",
    "   - Ratio: 210.289597\n",
    "\n",
    "This means that the CUDA tensor core dot() replacement is \n",
    "Accuracy comparison (maximum absolute differences):\n",
    "NumPy vs CuPy:         9.49e-81\n",
    "NumPy vs CUDA Fortran: 1.10e-07\n",
    "CuPy vs CUDA Fortran:  1.10e-07\n",
    "\n",
    "Speedup ratios:\n",
    "CUDA Fortran vs NumPy: 69.0x faster\n",
    "CUDA Fortran vs CuPy:  39.4x faster\n",
    "CuPy vs NumPy:         1.8x faster\n",
    "\n",
    "39x faster than cupy with negligible accuracy loss could be very useful in most situations. Also, the 210x ratio is with super tiny numbers of 4.8e-10 vs 1.01e-7. Not likely to affect normal matrix dot scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
