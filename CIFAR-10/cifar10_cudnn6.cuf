module cifar10_data_module
    use cudafor
    use iso_c_binding
    !use cifar10_cnn_module
    implicit none

    ! Add these to cifar10_data_module:
    real(4), device, allocatable, public :: gpu_train_data(:,:)      ! (50000, 3072)
    integer, device, allocatable, public :: gpu_train_labels(:)     ! (50000)
    real(4), device, allocatable, public :: gpu_test_data(:,:)      ! (10000, 3072)
    integer, device, allocatable, public :: gpu_test_labels(:)      ! (10000)

    ! ========================================================================
    ! CNN ARCHITECTURE PARAMETERS
    ! ========================================================================
    integer, parameter :: INPUT_CHANNELS = 3
    integer, parameter :: INPUT_HEIGHT = 32, INPUT_WIDTH = 32
    integer, parameter :: CONV1_FILTERS = 32, CONV2_FILTERS = 64, CONV3_FILTERS = 128
    integer, parameter :: KERNEL_SIZE = 3, PADDING = 1, STRIDE = 1
    integer, parameter :: POOL_SIZE = 2, POOL_STRIDE = 2

    ! ========================================================================
    ! CIFAR-10 DATASET PARAMETERS
    ! ========================================================================
    integer, parameter :: train_samples = 50000
    integer, parameter :: test_samples = 10000
    integer, parameter :: input_size = 3072  ! 3*32*32
    integer, parameter :: num_classes = 10

    ! CIFAR-10 normalization parameters (match Python exactly)
    real(8), parameter :: mean_r = 0.4914d0, mean_g = 0.4822d0, mean_b = 0.4465d0
    real(8), parameter :: std_r = 0.2023d0, std_g = 0.1994d0, std_b = 0.2010d0

    ! Module state
    logical :: data_loaded = .false.

    public :: load_cifar10_data, denormalize_cifar10_sample, is_data_loaded
    public :: train_samples, test_samples
    public :: input_size, num_classes
contains

    ! ========================================================================
    ! MAIN DATA LOADING FUNCTION
    ! ========================================================================
    subroutine load_cifar10_data()
        ! Load CIFAR-10 data with EXACT verification against Python version
        ! FIXED: Added missing persistent memory initialization calls
        real(4), allocatable :: temp_x_train(:), temp_x_test(:)
        real(4), allocatable :: temp_y_train(:), temp_y_test(:)

        ! Host temporary arrays (will be deallocated after copying to GPU pool)
        real(8), allocatable :: x_train_host(:,:,:,:), y_train_host(:,:)
        real(8), allocatable :: x_test_host(:,:,:,:), y_test_host(:,:)

        integer :: i, j, k, l, label, stat, idx
        real(8) :: sample_sum, sample_mean, sample_min, sample_max
        logical :: verification_passed

        print *, "ðŸ“ Loading CIFAR-10 with PYTHON VERIFICATION..."
        verification_passed = .false.

        ! Allocate host arrays (temporary - will be cleaned up)
        allocate(x_train_host(train_samples, input_channels, input_height, input_width))
        allocate(y_train_host(num_classes, train_samples))
        allocate(x_test_host(test_samples, input_channels, input_height, input_width))
        allocate(y_test_host(num_classes, test_samples))

        ! Allocate temporary flat arrays for binary file loading
        allocate(temp_x_train(input_size * train_samples))
        allocate(temp_y_train(train_samples))
        allocate(temp_x_test(input_size * test_samples))
        allocate(temp_y_test(test_samples))

        ! Load VERIFIED binary files (from Python export)
        print *, "ðŸ“¥ Loading verified binary files..."
        open(unit=10, file='cifar10_train_x_verified.bin', form='unformatted', access='stream', status='old', iostat=stat)
        if (stat /= 0) then
            print *, "âŒ Error: Cannot open cifar10_train_x_verified.bin"
            print *, "   Run the Python export script first!"
            stop
        endif
        read(10) temp_x_train
        close(10)

        open(unit=10, file='cifar10_train_y_verified.bin', form='unformatted', access='stream', status='old')
        read(10) temp_y_train
        close(10)

        open(unit=10, file='cifar10_test_x_verified.bin', form='unformatted', access='stream', status='old')
        read(10) temp_x_test
        close(10)

        open(unit=10, file='cifar10_test_y_verified.bin', form='unformatted', access='stream', status='old')
        read(10) temp_y_test
        close(10)

        print *, "âœ… Verified binary files loaded"

        ! ========================================================================
        ! RESHAPE TRAINING DATA: 1D â†’ 4D (NCHW format)
        ! ========================================================================
        print *, "ðŸ”„ Reshaping training data to 4D tensor (NCHW)..."
        do i = 1, train_samples
            do j = 1, input_channels
                do k = 1, input_height
                    do l = 1, input_width
                        idx = (i-1)*input_size + (j-1)*1024 + (k-1)*32 + l
                        x_train_host(i, j, k, l) = real(temp_x_train(idx), 8)
                    end do
                end do
            end do
        end do

        ! ========================================================================
        ! RESHAPE TEST DATA: 1D â†’ 4D (NCHW format)
        ! ========================================================================
        print *, "ðŸ”„ Reshaping test data to 4D tensor (NCHW)..."
        do i = 1, test_samples
            do j = 1, input_channels
                do k = 1, input_height
                    do l = 1, input_width
                        idx = (i-1)*input_size + (j-1)*1024 + (k-1)*32 + l
                        x_test_host(i, j, k, l) = real(temp_x_test(idx), 8)
                    end do
                end do
            end do
        end do

        ! ========================================================================
        ! CONVERT LABELS: Sparse â†’ One-hot encoding
        ! ========================================================================
        print *, "ðŸ”„ Converting labels to one-hot encoding..."

        ! Initialize to zero
        y_train_host = 0.0d0
        y_test_host = 0.0d0

        ! Training labels
        do i = 1, train_samples
            label = int(temp_y_train(i)) + 1  ! Convert 0-9 to 1-10
            if (label >= 1 .and. label <= num_classes) then
                y_train_host(label, i) = 1.0d0
            endif
        end do

        ! Test labels
        do i = 1, test_samples
            label = int(temp_y_test(i)) + 1  ! Convert 0-9 to 1-10
            if (label >= 1 .and. label <= num_classes) then
                y_test_host(label, i) = 1.0d0
            endif
        end do

        ! ========================================================================
        ! VERIFICATION: Check against Python version
        ! ========================================================================
        print *, "ðŸ” Verifying data against Python version..."

        ! Check training data statistics
        sample_sum = 0.0d0
        sample_min = x_train_host(1,1,1,1)
        sample_max = x_train_host(1,1,1,1)

        do i = 1, min(1000, train_samples)  ! Check first 1000 samples
            do j = 1, input_channels
                do k = 1, input_height
                    do l = 1, input_width
                        sample_sum = sample_sum + x_train_host(i,j,k,l)
                        sample_min = min(sample_min, x_train_host(i,j,k,l))
                        sample_max = max(sample_max, x_train_host(i,j,k,l))
                    end do
                end do
            end do
        end do

        sample_mean = sample_sum / (1000.0d0 * input_size)

        print *, "   Training data stats:"
        print *, "     Mean:", sample_mean
        print *, "     Min:", sample_min
        print *, "     Max:", sample_max

        ! Verify reasonable ranges (normalized CIFAR-10)
        if (sample_mean > -1.0d0 .and. sample_mean < 1.0d0 .and. &
            sample_min > -3.0d0 .and. sample_max < 3.0d0) then
            verification_passed = .true.
            print *, "âœ… Data verification PASSED - matches Python normalization"
        else
            print *, "âŒ Data verification FAILED - check Python export"
            stop
        endif

        ! ========================================================================
        ! TRANSFER TO PERSISTENT GPU MEMORY
        ! ========================================================================
        print *, "ðŸ”„ Transferring data to persistent GPU memory..."
        call load_dataset_to_gpu_persistent(x_train_host, y_train_host, x_test_host, y_test_host)

        ! ========================================================================
        ! CLEANUP
        ! ========================================================================
        print *, "ðŸ§¹ Cleaning up temporary host arrays..."

        ! Cleanup temporary host arrays
        deallocate(x_train_host, y_train_host, x_test_host, y_test_host)
        deallocate(temp_x_train, temp_y_train, temp_x_test, temp_y_test)

        data_loaded = .true.
        print *, "âœ… CIFAR-10 data loading completed successfully"
        print *, "   Training samples:", train_samples
        print *, "   Test samples:", test_samples
        print *, "   Data verified against Python version"
    end subroutine load_cifar10_data

    ! ========================================================================
    ! DENORMALIZE CIFAR (equivalent to Python denormalize_cifar10)
    ! ========================================================================
    subroutine denormalize_cifar10_sample(normalized_sample, denormalized_sample)
        ! Denormalize a single CIFAR-10 sample for visualization
        ! Reverses the normalization: (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)
        real(8), device, intent(in) :: normalized_sample(:,:,:)    ! (3, 32, 32)
        real(8), device, intent(out) :: denormalized_sample(:,:,:) ! (3, 32, 32)

        integer :: h, w

        !$cuf kernel do(2)
        do w = 1, 32
            do h = 1, 32
                ! Denormalize: x = (normalized * std) + mean
                denormalized_sample(1, h, w) = (normalized_sample(1, h, w) * std_r) + mean_r
                denormalized_sample(2, h, w) = (normalized_sample(2, h, w) * std_g) + mean_g
                denormalized_sample(3, h, w) = (normalized_sample(3, h, w) * std_b) + mean_b

                ! Clamp to [0, 1] range
                denormalized_sample(1, h, w) = max(0.0d0, min(1.0d0, denormalized_sample(1, h, w)))
                denormalized_sample(2, h, w) = max(0.0d0, min(1.0d0, denormalized_sample(2, h, w)))
                denormalized_sample(3, h, w) = max(0.0d0, min(1.0d0, denormalized_sample(3, h, w)))
            end do
        end do
    end subroutine denormalize_cifar10_sample

    ! ========================================================================
    ! DATA MODULE ACCESS FUNCTIONS
    ! ========================================================================
    function is_data_loaded() result(loaded)
        ! Check if data is loaded and ready
        logical :: loaded
        loaded = data_loaded
    end function is_data_loaded

    ! ========================================================================
    ! CRITICAL FIX: REAL GPU DATA LOADING
    ! ========================================================================
    subroutine load_dataset_to_gpu_persistent(x_train_host, y_train_host, x_test_host, y_test_host)
        real(8), intent(in) :: x_train_host(:,:,:,:), y_train_host(:,:)
        real(8), intent(in) :: x_test_host(:,:,:,:), y_test_host(:,:)
        integer :: stat

        print *, "ðŸ”„ ACTUALLY transferring data to GPU memory..."

        ! Allocate GPU arrays for training data
        if (.not. allocated(gpu_train_data)) then
            allocate(gpu_train_data(train_samples, INPUT_CHANNELS * INPUT_HEIGHT * INPUT_WIDTH), stat=stat)
            if (stat /= 0) then
                print *, "âŒ Failed to allocate GPU training data"
                return
            endif
        endif

        if (.not. allocated(gpu_train_labels)) then
            allocate(gpu_train_labels(train_samples), stat=stat)
            if (stat /= 0) then
                print *, "âŒ Failed to allocate GPU training labels"
                return
            endif
        endif

        ! Allocate GPU arrays for test data
        if (.not. allocated(gpu_test_data)) then
            allocate(gpu_test_data(test_samples, INPUT_CHANNELS * INPUT_HEIGHT * INPUT_WIDTH), stat=stat)
            if (stat /= 0) then
                print *, "âŒ Failed to allocate GPU test data"
                return
            endif
        endif

        if (.not. allocated(gpu_test_labels)) then
            allocate(gpu_test_labels(test_samples), stat=stat)
            if (stat /= 0) then
                print *, "âŒ Failed to allocate GPU test labels"
                return
            endif
        endif

        ! Flatten and copy training data: (50000, 3, 32, 32) â†’ (50000, 3072)
        call flatten_and_copy_to_gpu(x_train_host, gpu_train_data, train_samples)

        ! Copy training labels: convert from one-hot (10, 50000) to sparse (50000)
        call convert_labels_to_sparse(y_train_host, gpu_train_labels, train_samples, num_classes)

        ! Flatten and copy test data: (10000, 3, 32, 32) â†’ (10000, 3072)
        call flatten_and_copy_to_gpu(x_test_host, gpu_test_data, test_samples)

        ! Copy test labels: convert from one-hot (10, 10000) to sparse (10000)
        call convert_labels_to_sparse(y_test_host, gpu_test_labels, test_samples, num_classes)

        print *, "âœ… Data ACTUALLY transferred to GPU persistent memory"
        print *, "   Training data shape: (", train_samples, ",", INPUT_CHANNELS * INPUT_HEIGHT * INPUT_WIDTH, ")"
        print *, "   Test data shape: (", test_samples, ",", INPUT_CHANNELS * INPUT_HEIGHT * INPUT_WIDTH, ")"
    end subroutine load_dataset_to_gpu_persistent

    ! ========================================================================
    ! HELPER FUNCTIONS FOR REAL DATA TRANSFER
    ! ========================================================================
    subroutine flatten_and_copy_to_gpu(host_4d, gpu_2d, num_samples)
        real(8), intent(in) :: host_4d(:,:,:,:)  ! (samples, channels, height, width)
        real(4), device, intent(out) :: gpu_2d(:,:)  ! (samples, flattened)
        integer, intent(in) :: num_samples

        real(4), allocatable :: temp_flattened(:,:)
        integer :: i, c, h, w, idx

        ! Create temporary host array for flattening
        allocate(temp_flattened(num_samples, INPUT_CHANNELS * INPUT_HEIGHT * INPUT_WIDTH))

        ! Flatten: (samples, 3, 32, 32) â†’ (samples, 3072)
        do i = 1, num_samples
            idx = 1
            do c = 1, INPUT_CHANNELS
                do h = 1, INPUT_HEIGHT
                    do w = 1, INPUT_WIDTH
                        temp_flattened(i, idx) = real(host_4d(i, c, h, w), 4)
                        idx = idx + 1
                    end do
                end do
            end do
        end do

        ! Copy flattened data to GPU
        gpu_2d = temp_flattened

        deallocate(temp_flattened)
        print *, "âœ… Flattened and copied", num_samples, "samples to GPU"
    end subroutine flatten_and_copy_to_gpu

    subroutine convert_labels_to_sparse(host_onehot, gpu_sparse, num_samples, num_classes_param)
        real(8), intent(in) :: host_onehot(:,:)  ! (classes, samples) - one-hot format
        integer, device, intent(out) :: gpu_sparse(:)  ! (samples) - sparse format
        integer, intent(in) :: num_samples, num_classes_param

        integer, allocatable :: temp_sparse(:)
        integer :: i, c

        allocate(temp_sparse(num_samples))

        ! Convert one-hot to sparse: find the class with value 1.0
        do i = 1, num_samples
            temp_sparse(i) = 0  ! Default to class 0
            do c = 1, num_classes_param
                if (host_onehot(c, i) > 0.5) then  ! Found the 1.0 value
                    temp_sparse(i) = c - 1  ! Convert to 0-based indexing
                    exit
                endif
            end do
        end do

        ! Copy to GPU
        gpu_sparse = temp_sparse

        deallocate(temp_sparse)
        print *, "âœ… Converted labels to sparse format:", num_samples, "labels"
    end subroutine convert_labels_to_sparse
end module cifar10_data_module

! CIFAR-10 CNN Training using Pure cuDNN Intrinsics
module cudnn_cifar10
    use cudafor
    use iso_c_binding
    use ieee_arithmetic
    implicit none

    ! cuDNN constants
    integer(c_int), parameter :: CUDNN_STATUS_SUCCESS = 0
    integer(c_int), parameter :: CUDNN_TENSOR_NCHW = 0
    integer(c_int), parameter :: CUDNN_DATA_FLOAT = 0
    integer(c_int), parameter :: CUDNN_CROSS_CORRELATION = 0
    integer(c_int), parameter :: CUDNN_POOLING_MAX = 0
    integer(c_int), parameter :: CUDNN_ACTIVATION_RELU = 1
    integer(c_int), parameter :: CUDNN_PROPAGATE_NAN = 0
    integer(c_int), parameter :: CUDNN_SOFTMAX_ACCURATE = 0
    integer(c_int), parameter :: CUDNN_SOFTMAX_MODE_CHANNEL = 1
    integer(c_int), parameter :: CUDNN_BATCHNORM_SPATIAL = 1

    ! Algorithm constants
    integer(c_int), parameter :: CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM = 0
    integer(c_int), parameter :: CUDNN_CONVOLUTION_BWD_DATA_ALGO_1 = 1
    integer(c_int), parameter :: CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1 = 1

    ! Network architecture
    integer, parameter :: BATCH_SIZE = 128
    integer, parameter :: CNN_NUM_CLASSES = 10  ! Renamed to avoid conflict
    integer, parameter :: CNN_INPUT_SIZE = 3 * 32 * 32  ! Renamed to avoid conflict

    ! Layer dimensions
    integer, parameter :: CONV1_FILTERS = 32
    integer, parameter :: CONV2_FILTERS = 64
    integer, parameter :: CONV3_FILTERS = 128
    integer, parameter :: FC1_SIZE = 512
    integer, parameter :: FC2_SIZE = 256
    integer, parameter :: FC3_SIZE = 10

    logical, save :: debug_nan_checks = .true.  ! Set to .false. after debugging

    ! Global handles
    type(c_ptr) :: cudnn_handle = c_null_ptr
    type(c_ptr) :: cublas_handle = c_null_ptr

    ! Adam optimizer hyperparameters (matching PyTorch)
    real(4), parameter :: adam_beta1 = 0.9
    real(4), parameter :: adam_beta2 = 0.999
    real(4), parameter :: adam_epsilon = 1e-8
    real(4), parameter :: weight_decay = 1e-4

    ! ========================================================================
    ! CNN Layer Structure - holds everything for one model
    ! ========================================================================
    type :: cnn_model
        ! Descriptors
        type(c_ptr) :: input_desc, conv1_desc, conv2_desc, conv3_desc
        type(c_ptr) :: pool1_desc, pool2_desc, pool3_desc
        type(c_ptr) :: fc1_desc, fc2_desc, fc3_desc, output_desc
        type(c_ptr) :: conv1_filter_desc, conv2_filter_desc, conv3_filter_desc
        type(c_ptr) :: conv1_bias_desc, conv2_bias_desc, conv3_bias_desc
        type(c_ptr) :: conv1_conv_desc, conv2_conv_desc, conv3_conv_desc
        type(c_ptr) :: pooling_desc, activation_desc
        type(c_ptr) :: bn1_desc, bn2_desc, bn3_desc

        ! Layer outputs (forward pass)
        real(4), device, allocatable :: input(:,:,:,:)
        real(4), device, allocatable :: conv1_out(:,:,:,:)
        real(4), device, allocatable :: bn1_out(:,:,:,:)
        real(4), device, allocatable :: relu1_out(:,:,:,:)
        real(4), device, allocatable :: pool1_out(:,:,:,:)
        real(4), device, allocatable :: conv2_out(:,:,:,:)
        real(4), device, allocatable :: bn2_out(:,:,:,:)
        real(4), device, allocatable :: relu2_out(:,:,:,:)
        real(4), device, allocatable :: pool2_out(:,:,:,:)
        real(4), device, allocatable :: conv3_out(:,:,:,:)
        real(4), device, allocatable :: bn3_out(:,:,:,:)
        real(4), device, allocatable :: relu3_out(:,:,:,:)
        real(4), device, allocatable :: pool3_out(:,:,:,:)

        ! FC layers - forward pass states
        real(4), device, allocatable :: flatten(:,:)         ! (batch, 2048)
        real(4), device, allocatable :: fc1_out(:,:)         ! (batch, 512)
        real(4), device, allocatable :: fc1_relu(:,:)        ! (batch, 512)
        real(4), device, allocatable :: fc1_dropout(:,:)     ! (batch, 512)
        real(4), device, allocatable :: fc2_out(:,:)         ! (batch, 256)
        real(4), device, allocatable :: fc2_relu(:,:)        ! (batch, 256)
        real(4), device, allocatable :: fc2_dropout(:,:)     ! (batch, 256)
        real(4), device, allocatable :: fc3_out(:,:)         ! (batch, 10)
        real(4), device, allocatable :: softmax_out(:,:)     ! (batch, 10)

        ! Weights and biases
        real(4), device, allocatable :: conv1_weights(:,:,:,:), conv1_bias(:)
        real(4), device, allocatable :: conv2_weights(:,:,:,:), conv2_bias(:)
        real(4), device, allocatable :: conv3_weights(:,:,:,:), conv3_bias(:)
        real(4), device, allocatable :: fc1_weights(:,:), fc1_bias(:)
        real(4), device, allocatable :: fc2_weights(:,:), fc2_bias(:)
        real(4), device, allocatable :: fc3_weights(:,:), fc3_bias(:)

        ! Batch normalization parameters
        real(4), device, allocatable :: bn1_scale(:), bn1_bias(:)
        real(4), device, allocatable :: bn1_running_mean(:), bn1_running_var(:)
        real(4), device, allocatable :: bn1_saved_mean(:), bn1_saved_inv_var(:)
        real(4), device, allocatable :: bn2_scale(:), bn2_bias(:)
        real(4), device, allocatable :: bn2_running_mean(:), bn2_running_var(:)
        real(4), device, allocatable :: bn2_saved_mean(:), bn2_saved_inv_var(:)
        real(4), device, allocatable :: bn3_scale(:), bn3_bias(:)
        real(4), device, allocatable :: bn3_running_mean(:), bn3_running_var(:)
        real(4), device, allocatable :: bn3_saved_mean(:), bn3_saved_inv_var(:)

        ! Conv gradients (backward pass)
        real(4), device, allocatable :: grad_input(:,:,:,:)
        real(4), device, allocatable :: grad_conv1(:,:,:,:), grad_bn1(:,:,:,:), grad_relu1(:,:,:,:)
        real(4), device, allocatable :: grad_pool1(:,:,:,:), grad_conv2(:,:,:,:), grad_bn2(:,:,:,:)
        real(4), device, allocatable :: grad_relu2(:,:,:,:), grad_pool2(:,:,:,:), grad_conv3(:,:,:,:)
        real(4), device, allocatable :: grad_bn3(:,:,:,:), grad_relu3(:,:,:,:), grad_pool3(:,:,:,:)

        ! âœ… FC gradients - INCLUDING intermediate states for dropout/relu backward
        real(4), device, allocatable :: grad_fc1(:,:)            ! (batch, 512)
        real(4), device, allocatable :: grad_fc1_dropout(:,:)    ! (batch, 512)
        real(4), device, allocatable :: grad_fc1_relu(:,:)       ! (batch, 512)
        real(4), device, allocatable :: grad_fc2(:,:)            ! (batch, 256)
        real(4), device, allocatable :: grad_fc2_dropout(:,:)    ! (batch, 256)
        real(4), device, allocatable :: grad_fc2_relu(:,:)       ! (batch, 256)
        real(4), device, allocatable :: grad_fc3(:,:)            ! (batch, 10)

        ! Weight gradients
        real(4), device, allocatable :: grad_conv1_weights(:,:,:,:), grad_conv1_bias(:)
        real(4), device, allocatable :: grad_conv2_weights(:,:,:,:), grad_conv2_bias(:)
        real(4), device, allocatable :: grad_conv3_weights(:,:,:,:), grad_conv3_bias(:)
        real(4), device, allocatable :: grad_fc1_weights(:,:), grad_fc1_bias(:)
        real(4), device, allocatable :: grad_fc2_weights(:,:), grad_fc2_bias(:)
        real(4), device, allocatable :: grad_fc3_weights(:,:), grad_fc3_bias(:)
        real(4), device, allocatable :: grad_bn1_scale(:), grad_bn1_bias(:)
        real(4), device, allocatable :: grad_bn2_scale(:), grad_bn2_bias(:)
        real(4), device, allocatable :: grad_bn3_scale(:), grad_bn3_bias(:)

        ! Dropout descriptor and states
        type(c_ptr) :: dropout_desc = c_null_ptr
        real(4), device, allocatable :: dropout_states(:)
        integer(c_size_t) :: dropout_state_size = 0

        ! Workspace for convolution algorithms
        real(4), device, allocatable :: workspace(:)
        integer(c_size_t) :: workspace_size = 0

        ! âœ… ADD: Adam optimizer state
        ! First moments (m)
        real(4), device, allocatable :: m_conv1_weights(:,:,:,:), m_conv1_bias(:)
        real(4), device, allocatable :: m_conv2_weights(:,:,:,:), m_conv2_bias(:)
        real(4), device, allocatable :: m_conv3_weights(:,:,:,:), m_conv3_bias(:)
        real(4), device, allocatable :: m_fc1_weights(:,:), m_fc1_bias(:)
        real(4), device, allocatable :: m_fc2_weights(:,:), m_fc2_bias(:)
        real(4), device, allocatable :: m_fc3_weights(:,:), m_fc3_bias(:)
        real(4), device, allocatable :: m_bn1_scale(:), m_bn1_bias(:)
        real(4), device, allocatable :: m_bn2_scale(:), m_bn2_bias(:)
        real(4), device, allocatable :: m_bn3_scale(:), m_bn3_bias(:)

        ! Second moments (v)
        real(4), device, allocatable :: v_conv1_weights(:,:,:,:), v_conv1_bias(:)
        real(4), device, allocatable :: v_conv2_weights(:,:,:,:), v_conv2_bias(:)
        real(4), device, allocatable :: v_conv3_weights(:,:,:,:), v_conv3_bias(:)
        real(4), device, allocatable :: v_fc1_weights(:,:), v_fc1_bias(:)
        real(4), device, allocatable :: v_fc2_weights(:,:), v_fc2_bias(:)
        real(4), device, allocatable :: v_fc3_weights(:,:), v_fc3_bias(:)
        real(4), device, allocatable :: v_bn1_scale(:), v_bn1_bias(:)
        real(4), device, allocatable :: v_bn2_scale(:), v_bn2_bias(:)
        real(4), device, allocatable :: v_bn3_scale(:), v_bn3_bias(:)

        ! Timestep counter
        integer :: adam_timestep = 0
    end type cnn_model

    ! ========================================================================
    ! cuDNN C Interface - Direct bindings
    ! ========================================================================
    interface
        function cudnnCreate(handle) bind(c, name='cudnnCreate')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: handle
            integer(c_int) :: cudnnCreate
        end function

        function cudnnDestroy(handle) bind(c, name='cudnnDestroy')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle
            integer(c_int) :: cudnnDestroy
        end function

        function cudnnCreateTensorDescriptor(desc) bind(c, name='cudnnCreateTensorDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: desc
            integer(c_int) :: cudnnCreateTensorDescriptor
        end function

        function cudnnSetTensor4dDescriptor(desc, format, datatype, n, c, h, w) &
                bind(c, name='cudnnSetTensor4dDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int), value :: format, datatype, n, c, h, w
            integer(c_int) :: cudnnSetTensor4dDescriptor
        end function

        function cudnnDestroyTensorDescriptor(desc) bind(c, name='cudnnDestroyTensorDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int) :: cudnnDestroyTensorDescriptor
        end function

        function cudnnCreateFilterDescriptor(desc) bind(c, name='cudnnCreateFilterDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: desc
            integer(c_int) :: cudnnCreateFilterDescriptor
        end function

        function cudnnSetFilter4dDescriptor(desc, datatype, format, k, c, h, w) &
                bind(c, name='cudnnSetFilter4dDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int), value :: datatype, format, k, c, h, w
            integer(c_int) :: cudnnSetFilter4dDescriptor
        end function

        function cudnnDestroyFilterDescriptor(desc) bind(c, name='cudnnDestroyFilterDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int) :: cudnnDestroyFilterDescriptor
        end function

        function cudnnCreateConvolutionDescriptor(desc) bind(c, name='cudnnCreateConvolutionDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: desc
            integer(c_int) :: cudnnCreateConvolutionDescriptor
        end function

        function cudnnSetConvolution2dDescriptor(desc, pad_h, pad_w, u, v, dilation_h, dilation_w, mode, datatype) &
                bind(c, name='cudnnSetConvolution2dDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int), value :: pad_h, pad_w, u, v, dilation_h, dilation_w, mode, datatype
            integer(c_int) :: cudnnSetConvolution2dDescriptor
        end function

        function cudnnDestroyConvolutionDescriptor(desc) bind(c, name='cudnnDestroyConvolutionDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int) :: cudnnDestroyConvolutionDescriptor
        end function

        function cudnnCreatePoolingDescriptor(desc) bind(c, name='cudnnCreatePoolingDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: desc
            integer(c_int) :: cudnnCreatePoolingDescriptor
        end function

        function cudnnSetPooling2dDescriptor(desc, mode, nan_opt, window_h, window_w, pad_h, pad_w, stride_h, stride_w) &
                bind(c, name='cudnnSetPooling2dDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int), value :: mode, nan_opt, window_h, window_w, pad_h, pad_w, stride_h, stride_w
            integer(c_int) :: cudnnSetPooling2dDescriptor
        end function

        function cudnnDestroyPoolingDescriptor(desc) bind(c, name='cudnnDestroyPoolingDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int) :: cudnnDestroyPoolingDescriptor
        end function

        function cudnnCreateActivationDescriptor(desc) bind(c, name='cudnnCreateActivationDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: desc
            integer(c_int) :: cudnnCreateActivationDescriptor
        end function

        function cudnnSetActivationDescriptor(desc, mode, relu_nan_opt, coef) &
                bind(c, name='cudnnSetActivationDescriptor')
            import :: c_ptr, c_int, c_double
            type(c_ptr), value :: desc
            integer(c_int), value :: mode, relu_nan_opt
            real(c_double), value :: coef
            integer(c_int) :: cudnnSetActivationDescriptor
        end function

        function cudnnDestroyActivationDescriptor(desc) bind(c, name='cudnnDestroyActivationDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: desc
            integer(c_int) :: cudnnDestroyActivationDescriptor
        end function

        ! Forward operations
        function cudnnConvolutionForward(handle, alpha, x_desc, x, w_desc, w, conv_desc, algo, &
                workspace, workspace_size, beta, y_desc, y) bind(c, name='cudnnConvolutionForward')
            import :: c_ptr, c_int, c_size_t
            type(c_ptr), value :: handle, alpha, x_desc, x, w_desc, w, conv_desc, y_desc, y, workspace, beta
            integer(c_int), value :: algo
            integer(c_size_t), value :: workspace_size
            integer(c_int) :: cudnnConvolutionForward
        end function

        function cudnnAddTensor(handle, alpha, a_desc, a, beta, c_desc, c) bind(c, name='cudnnAddTensor')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle, alpha, a_desc, a, beta, c_desc, c
            integer(c_int) :: cudnnAddTensor
        end function

        function cudnnActivationForward(handle, activation_desc, alpha, x_desc, x, beta, y_desc, y) &
                bind(c, name='cudnnActivationForward')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle, activation_desc, alpha, x_desc, x, beta, y_desc, y
            integer(c_int) :: cudnnActivationForward
        end function

        function cudnnPoolingForward(handle, pool_desc, alpha, x_desc, x, beta, y_desc, y) &
                bind(c, name='cudnnPoolingForward')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle, pool_desc, alpha, x_desc, x, beta, y_desc, y
            integer(c_int) :: cudnnPoolingForward
        end function

        function cudnnBatchNormalizationForwardTraining(handle, mode, alpha, beta, x_desc, x, y_desc, y, &
                bn_scale_bias_mean_var_desc, bn_scale, bn_bias, exponential_average_factor, &
                result_running_mean, result_running_variance, epsilon, result_save_mean, result_save_inv_variance) &
                bind(c, name='cudnnBatchNormalizationForwardTraining')
            import :: c_ptr, c_int, c_double
            type(c_ptr), value :: handle, alpha, beta, x_desc, x, y_desc, y, bn_scale_bias_mean_var_desc
            type(c_ptr), value :: bn_scale, bn_bias, result_running_mean, result_running_variance
            type(c_ptr), value :: result_save_mean, result_save_inv_variance
            integer(c_int), value :: mode
            real(c_double), value :: exponential_average_factor, epsilon
            integer(c_int) :: cudnnBatchNormalizationForwardTraining
        end function

        function cudnnBatchNormalizationForwardInference(handle, mode, alpha, beta, x_desc, x, y_desc, y, &
                bn_scale_bias_mean_var_desc, bn_scale, bn_bias, &
                estimated_mean, estimated_variance, epsilon) &
                bind(c, name='cudnnBatchNormalizationForwardInference')
            import :: c_ptr, c_int, c_double
            type(c_ptr), value :: handle, alpha, beta, x_desc, x, y_desc, y, bn_scale_bias_mean_var_desc
            type(c_ptr), value :: bn_scale, bn_bias, estimated_mean, estimated_variance
            integer(c_int), value :: mode
            real(c_double), value :: epsilon
            integer(c_int) :: cudnnBatchNormalizationForwardInference
        end function

        function cudnnSoftmaxForward(handle, algo, mode, alpha, x_desc, x, beta, y_desc, y) &
                bind(c, name='cudnnSoftmaxForward')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle, alpha, x_desc, x, beta, y_desc, y
            integer(c_int), value :: algo, mode
            integer(c_int) :: cudnnSoftmaxForward
        end function

        ! Backward operations
        function cudnnConvolutionBackwardData(handle, alpha, w_desc, w, dy_desc, dy, conv_desc, algo, &
                workspace, workspace_size, beta, dx_desc, dx) bind(c, name='cudnnConvolutionBackwardData')
            import :: c_ptr, c_int, c_size_t
            type(c_ptr), value :: handle, alpha, w_desc, w, dy_desc, dy, conv_desc, dx_desc, dx, workspace, beta
            integer(c_int), value :: algo
            integer(c_size_t), value :: workspace_size
            integer(c_int) :: cudnnConvolutionBackwardData
        end function

        function cudnnConvolutionBackwardFilter(handle, alpha, x_desc, x, dy_desc, dy, conv_desc, algo, &
                workspace, workspace_size, beta, dw_desc, dw) bind(c, name='cudnnConvolutionBackwardFilter')
            import :: c_ptr, c_int, c_size_t
            type(c_ptr), value :: handle, alpha, x_desc, x, dy_desc, dy, conv_desc, dw_desc, dw, workspace, beta
            integer(c_int), value :: algo
            integer(c_size_t), value :: workspace_size
            integer(c_int) :: cudnnConvolutionBackwardFilter
        end function

        function cudnnConvolutionBackwardBias(handle, alpha, dy_desc, dy, beta, db_desc, db) &
                bind(c, name='cudnnConvolutionBackwardBias')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle, alpha, dy_desc, dy, beta, db_desc, db
            integer(c_int) :: cudnnConvolutionBackwardBias
        end function

        function cudnnActivationBackward(handle, activation_desc, alpha, y_desc, y, dy_desc, dy, &
                x_desc, x, beta, dx_desc, dx) bind(c, name='cudnnActivationBackward')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle, activation_desc, alpha, y_desc, y, dy_desc, dy, x_desc, x, beta, dx_desc, dx
            integer(c_int) :: cudnnActivationBackward
        end function

        function cudnnPoolingBackward(handle, pool_desc, alpha, y_desc, y, dy_desc, dy, x_desc, x, &
                beta, dx_desc, dx) bind(c, name='cudnnPoolingBackward')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle, pool_desc, alpha, y_desc, y, dy_desc, dy, x_desc, x, beta, dx_desc, dx
            integer(c_int) :: cudnnPoolingBackward
        end function

        function cudnnBatchNormalizationBackward(handle, mode, alpha_data, beta_data, alpha_param, beta_param, &
                x_desc, x, dy_desc, dy, dx_desc, dx, bn_scale_bias_diff_desc, bn_scale, &
                result_bn_scale_diff, result_bn_bias_diff, epsilon, saved_mean, saved_inv_variance) &
                bind(c, name='cudnnBatchNormalizationBackward')
            import :: c_ptr, c_int, c_double
            type(c_ptr), value :: handle, alpha_data, beta_data, alpha_param, beta_param
            type(c_ptr), value :: x_desc, x, dy_desc, dy, dx_desc, dx, bn_scale_bias_diff_desc
            type(c_ptr), value :: bn_scale, result_bn_scale_diff, result_bn_bias_diff, saved_mean, saved_inv_variance
            integer(c_int), value :: mode
            real(c_double), value :: epsilon
            integer(c_int) :: cudnnBatchNormalizationBackward
        end function

        function cudnnSoftmaxBackward(handle, algo, mode, alpha, y_desc, y, dy_desc, dy, beta, dx_desc, dx) &
                bind(c, name='cudnnSoftmaxBackward')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle, alpha, y_desc, y, dy_desc, dy, beta, dx_desc, dx
            integer(c_int), value :: algo, mode
            integer(c_int) :: cudnnSoftmaxBackward
        end function

        ! DROPOUT FUNCTIONS
        function cudnnCreateDropoutDescriptor(dropout_desc) &
                bind(c, name='cudnnCreateDropoutDescriptor')
            import :: c_ptr, c_int
            type(c_ptr) :: dropout_desc
            integer(c_int) :: cudnnCreateDropoutDescriptor
        end function

        function cudnnDropoutGetStatesSize(handle, size_in_bytes) &
                bind(c, name='cudnnDropoutGetStatesSize')
            import :: c_ptr, c_int, c_size_t
            type(c_ptr), value :: handle
            integer(c_size_t) :: size_in_bytes
            integer(c_int) :: cudnnDropoutGetStatesSize
        end function

        function cudnnSetDropoutDescriptor(dropout_desc, handle, dropout, states, &
                state_size_in_bytes, seed) &
                bind(c, name='cudnnSetDropoutDescriptor')
            import :: c_ptr, c_int, c_float, c_size_t, c_long_long
            type(c_ptr), value :: dropout_desc, handle, states
            real(c_float), value :: dropout
            integer(c_size_t), value :: state_size_in_bytes
            integer(c_long_long), value :: seed
            integer(c_int) :: cudnnSetDropoutDescriptor
        end function

        function cudnnDropoutForward(handle, dropout_desc, x_desc, x, y_desc, y, &
                reserve_space, reserve_space_size_in_bytes) &
                bind(c, name='cudnnDropoutForward')
            import :: c_ptr, c_int, c_size_t
            type(c_ptr), value :: handle, dropout_desc, x_desc, x, y_desc, y, reserve_space
            integer(c_size_t), value :: reserve_space_size_in_bytes
            integer(c_int) :: cudnnDropoutForward
        end function

        function cudnnDropoutBackward(handle, dropout_desc, dy_desc, dy, dx_desc, dx, &
                reserve_space, reserve_space_size_in_bytes) &
                bind(c, name='cudnnDropoutBackward')
            import :: c_ptr, c_int, c_size_t
            type(c_ptr), value :: handle, dropout_desc, dy_desc, dy, dx_desc, dx, reserve_space
            integer(c_size_t), value :: reserve_space_size_in_bytes
            integer(c_int) :: cudnnDropoutBackward
        end function

        function cudnnDestroyDropoutDescriptor(dropout_desc) &
                bind(c, name='cudnnDestroyDropoutDescriptor')
            import :: c_ptr, c_int
            type(c_ptr), value :: dropout_desc
            integer(c_int) :: cudnnDestroyDropoutDescriptor
        end function
    end interface

    ! cuBLAS interface for FC layers
    interface
        function cublasCreate_v2(handle) bind(c, name='cublasCreate_v2')
            import :: c_ptr, c_int
            type(c_ptr), intent(out) :: handle
            integer(c_int) :: cublasCreate_v2
        end function

        function cublasDestroy_v2(handle) bind(c, name='cublasDestroy_v2')
            import :: c_ptr, c_int
            type(c_ptr), value :: handle
            integer(c_int) :: cublasDestroy_v2
        end function

        function cublasSgemm_v2(handle, transa, transb, m, n, k, alpha, a, lda, b, ldb, &
                beta, c, ldc) bind(c, name='cublasSgemm_v2')
            import :: c_ptr, c_int, c_float
            type(c_ptr), value :: handle, a, b, c
            integer(c_int), value :: transa, transb, m, n, k, lda, ldb, ldc
            real(c_float), intent(in) :: alpha, beta
            integer(c_int) :: cublasSgemm_v2
        end function

        function cublasSgemv_v2(handle, trans, m, n, alpha, a, lda, x, incx, beta, y, incy) &
                bind(c, name='cublasSgemv_v2')
            import :: c_ptr, c_int, c_float
            type(c_ptr), value :: handle, a, x, y
            integer(c_int), value :: trans, m, n, lda, incx, incy
            real(c_float), intent(in) :: alpha, beta
            integer(c_int) :: cublasSgemv_v2
        end function
    end interface
contains

    ! ========================================================================
    ! Initialize cuDNN and cuBLAS
    ! ========================================================================
    subroutine init_cudnn_cublas()
        integer :: stat

        stat = cudnnCreate(cudnn_handle)
        if (stat /= CUDNN_STATUS_SUCCESS) then
            print *, "Failed to create cuDNN handle"
            stop
        end if

        stat = cublasCreate_v2(cublas_handle)
        if (stat /= 0) then
            print *, "Failed to create cuBLAS handle"
            stop
        end if

        print *, "âœ… cuDNN and cuBLAS initialized"
    end subroutine init_cudnn_cublas

    ! ========================================================================
    ! Create and initialize model
    ! ========================================================================
    subroutine create_model(model, batch_size)
        type(cnn_model), intent(out) :: model
        integer, intent(in) :: batch_size
        integer :: stat
        integer(c_size_t) :: dropout_bytes
        real(4), allocatable :: temp(:,:,:,:)
        real(4) :: std_dev
        integer :: i, j, k, l
        real(4), parameter :: eps = 1.0e-4  ! âœ… ADD THIS LINE

        print *, "ðŸ”§ Creating CIFAR-10 CNN model..."

        ! Create tensor descriptors
        stat = cudnnCreateTensorDescriptor(model%input_desc)
        stat = cudnnSetTensor4dDescriptor(model%input_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         batch_size, 3, 32, 32)

        ! Conv1 descriptors
        stat = cudnnCreateTensorDescriptor(model%conv1_desc)
        stat = cudnnSetTensor4dDescriptor(model%conv1_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         batch_size, CONV1_FILTERS, 32, 32)

        stat = cudnnCreateFilterDescriptor(model%conv1_filter_desc)
        stat = cudnnSetFilter4dDescriptor(model%conv1_filter_desc, CUDNN_DATA_FLOAT, CUDNN_TENSOR_NCHW, &
                                         CONV1_FILTERS, 3, 3, 3)

        stat = cudnnCreateTensorDescriptor(model%conv1_bias_desc)
        stat = cudnnSetTensor4dDescriptor(model%conv1_bias_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         1, CONV1_FILTERS, 1, 1)

        stat = cudnnCreateConvolutionDescriptor(model%conv1_conv_desc)
        stat = cudnnSetConvolution2dDescriptor(model%conv1_conv_desc, 1, 1, 1, 1, 1, 1, &
                                              CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT)

        ! BN1 descriptor
        stat = cudnnCreateTensorDescriptor(model%bn1_desc)
        stat = cudnnSetTensor4dDescriptor(model%bn1_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         1, CONV1_FILTERS, 1, 1)

        ! Pool1 descriptors
        stat = cudnnCreateTensorDescriptor(model%pool1_desc)
        stat = cudnnSetTensor4dDescriptor(model%pool1_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         batch_size, CONV1_FILTERS, 16, 16)

        ! Conv2 descriptors
        stat = cudnnCreateTensorDescriptor(model%conv2_desc)
        stat = cudnnSetTensor4dDescriptor(model%conv2_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         batch_size, CONV2_FILTERS, 16, 16)

        stat = cudnnCreateFilterDescriptor(model%conv2_filter_desc)
        stat = cudnnSetFilter4dDescriptor(model%conv2_filter_desc, CUDNN_DATA_FLOAT, CUDNN_TENSOR_NCHW, &
                                         CONV2_FILTERS, CONV1_FILTERS, 3, 3)

        stat = cudnnCreateTensorDescriptor(model%conv2_bias_desc)
        stat = cudnnSetTensor4dDescriptor(model%conv2_bias_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         1, CONV2_FILTERS, 1, 1)

        stat = cudnnCreateConvolutionDescriptor(model%conv2_conv_desc)
        stat = cudnnSetConvolution2dDescriptor(model%conv2_conv_desc, 1, 1, 1, 1, 1, 1, &
                                              CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT)

        ! BN2 descriptor
        stat = cudnnCreateTensorDescriptor(model%bn2_desc)
        stat = cudnnSetTensor4dDescriptor(model%bn2_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         1, CONV2_FILTERS, 1, 1)

        ! Pool2 descriptors
        stat = cudnnCreateTensorDescriptor(model%pool2_desc)
        stat = cudnnSetTensor4dDescriptor(model%pool2_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         batch_size, CONV2_FILTERS, 8, 8)

        ! Conv3 descriptors
        stat = cudnnCreateTensorDescriptor(model%conv3_desc)
        stat = cudnnSetTensor4dDescriptor(model%conv3_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         batch_size, CONV3_FILTERS, 8, 8)

        stat = cudnnCreateFilterDescriptor(model%conv3_filter_desc)
        stat = cudnnSetFilter4dDescriptor(model%conv3_filter_desc, CUDNN_DATA_FLOAT, CUDNN_TENSOR_NCHW, &
                                         CONV3_FILTERS, CONV2_FILTERS, 3, 3)

        stat = cudnnCreateTensorDescriptor(model%conv3_bias_desc)
        stat = cudnnSetTensor4dDescriptor(model%conv3_bias_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         1, CONV3_FILTERS, 1, 1)

        stat = cudnnCreateConvolutionDescriptor(model%conv3_conv_desc)
        stat = cudnnSetConvolution2dDescriptor(model%conv3_conv_desc, 1, 1, 1, 1, 1, 1, &
                                              CUDNN_CROSS_CORRELATION, CUDNN_DATA_FLOAT)

        ! BN3 descriptor
        stat = cudnnCreateTensorDescriptor(model%bn3_desc)
        stat = cudnnSetTensor4dDescriptor(model%bn3_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         1, CONV3_FILTERS, 1, 1)

        ! Pool3 descriptors
        stat = cudnnCreateTensorDescriptor(model%pool3_desc)
        stat = cudnnSetTensor4dDescriptor(model%pool3_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         batch_size, CONV3_FILTERS, 4, 4)

        ! Pooling descriptor (shared for all pooling layers)
        stat = cudnnCreatePoolingDescriptor(model%pooling_desc)
        stat = cudnnSetPooling2dDescriptor(model%pooling_desc, CUDNN_POOLING_MAX, CUDNN_PROPAGATE_NAN, &
                                          2, 2, 0, 0, 2, 2)

        ! Activation descriptor (shared for all ReLU layers)
        stat = cudnnCreateActivationDescriptor(model%activation_desc)
        ! Change from 0.0d0 to 0.01d0
        stat = cudnnSetActivationDescriptor(model%activation_desc, CUDNN_ACTIVATION_RELU, CUDNN_PROPAGATE_NAN, 0.0d0)

        ! FC1 descriptor (batch, 512, 1, 1)
        stat = cudnnCreateTensorDescriptor(model%fc1_desc)
        stat = cudnnSetTensor4dDescriptor(model%fc1_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         int(batch_size, c_int), int(512, c_int), 1_c_int, 1_c_int)

        ! FC2 descriptor (batch, 256, 1, 1)
        stat = cudnnCreateTensorDescriptor(model%fc2_desc)
        stat = cudnnSetTensor4dDescriptor(model%fc2_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         int(batch_size, c_int), int(256, c_int), 1_c_int, 1_c_int)

        ! FC3 descriptor (batch, 10, 1, 1)
        stat = cudnnCreateTensorDescriptor(model%fc3_desc)
        stat = cudnnSetTensor4dDescriptor(model%fc3_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         int(batch_size, c_int), int(CNN_NUM_CLASSES, c_int), 1_c_int, 1_c_int)

        stat = cudnnCreateTensorDescriptor(model%output_desc)
        stat = cudnnSetTensor4dDescriptor(model%output_desc, CUDNN_TENSOR_NCHW, CUDNN_DATA_FLOAT, &
                                         batch_size, CNN_NUM_CLASSES, 1, 1)

        ! Allocate layer outputs
        allocate(model%input(batch_size, 3, 32, 32))
        allocate(model%conv1_out(batch_size, CONV1_FILTERS, 32, 32))
        allocate(model%bn1_out(batch_size, CONV1_FILTERS, 32, 32))
        allocate(model%relu1_out(batch_size, CONV1_FILTERS, 32, 32))
        allocate(model%pool1_out(batch_size, CONV1_FILTERS, 16, 16))
        allocate(model%conv2_out(batch_size, CONV2_FILTERS, 16, 16))
        allocate(model%bn2_out(batch_size, CONV2_FILTERS, 16, 16))
        allocate(model%relu2_out(batch_size, CONV2_FILTERS, 16, 16))
        allocate(model%pool2_out(batch_size, CONV2_FILTERS, 8, 8))
        allocate(model%conv3_out(batch_size, CONV3_FILTERS, 8, 8))
        allocate(model%bn3_out(batch_size, CONV3_FILTERS, 8, 8))
        allocate(model%relu3_out(batch_size, CONV3_FILTERS, 8, 8))
        allocate(model%pool3_out(batch_size, CONV3_FILTERS, 4, 4))
        allocate(model%flatten(batch_size, CONV3_FILTERS * 4 * 4))
        allocate(model%fc1_out(batch_size, 512))
        allocate(model%fc1_relu(batch_size, 512))
        allocate(model%fc1_dropout(batch_size, 512))
        allocate(model%fc2_out(batch_size, 256))
        allocate(model%fc2_relu(batch_size, 256))
        allocate(model%fc2_dropout(batch_size, 256))
        allocate(model%fc3_out(batch_size, CNN_NUM_CLASSES))
        allocate(model%softmax_out(batch_size, CNN_NUM_CLASSES))

        ! Allocate weights and biases
        allocate(model%conv1_weights(CONV1_FILTERS, 3, 3, 3))
        allocate(model%conv1_bias(CONV1_FILTERS))
        allocate(model%conv2_weights(CONV2_FILTERS, CONV1_FILTERS, 3, 3))
        allocate(model%conv2_bias(CONV2_FILTERS))
        allocate(model%conv3_weights(CONV3_FILTERS, CONV2_FILTERS, 3, 3))
        allocate(model%conv3_bias(CONV3_FILTERS))

        allocate(model%fc1_weights(512, CONV3_FILTERS * 4 * 4))
        allocate(model%fc1_bias(512))
        allocate(model%fc2_weights(256, 512))
        allocate(model%fc2_bias(256))
        allocate(model%fc3_weights(CNN_NUM_CLASSES, 256))
        allocate(model%fc3_bias(CNN_NUM_CLASSES))

        ! Allocate batch normalization parameters
        allocate(model%bn1_scale(CONV1_FILTERS))
        allocate(model%bn1_bias(CONV1_FILTERS))
        allocate(model%bn1_running_mean(CONV1_FILTERS))
        allocate(model%bn1_running_var(CONV1_FILTERS))
        allocate(model%bn1_saved_mean(CONV1_FILTERS))
        allocate(model%bn1_saved_inv_var(CONV1_FILTERS))

        allocate(model%bn2_scale(CONV2_FILTERS))
        allocate(model%bn2_bias(CONV2_FILTERS))
        allocate(model%bn2_running_mean(CONV2_FILTERS))
        allocate(model%bn2_running_var(CONV2_FILTERS))
        allocate(model%bn2_saved_mean(CONV2_FILTERS))
        allocate(model%bn2_saved_inv_var(CONV2_FILTERS))

        allocate(model%bn3_scale(CONV3_FILTERS))
        allocate(model%bn3_bias(CONV3_FILTERS))
        allocate(model%bn3_running_mean(CONV3_FILTERS))
        allocate(model%bn3_running_var(CONV3_FILTERS))
        allocate(model%bn3_saved_mean(CONV3_FILTERS))
        allocate(model%bn3_saved_inv_var(CONV3_FILTERS))

        ! Initialize weights with He initialization
        allocate(temp(CONV1_FILTERS, 3, 3, 3))
        std_dev = sqrt(2.0 / (3 * 3 * 3))
        call random_number(temp)
        temp = (temp - 0.5) * 2.0 * std_dev
        model%conv1_weights = temp
        deallocate(temp)
        model%conv1_bias = 0.0

        allocate(temp(CONV2_FILTERS, CONV1_FILTERS, 3, 3))
        std_dev = sqrt(2.0 / (CONV1_FILTERS * 3 * 3))
        call random_number(temp)
        temp = (temp - 0.5) * 2.0 * std_dev
        model%conv2_weights = temp
        deallocate(temp)
        model%conv2_bias = 0.0

        allocate(temp(CONV3_FILTERS, CONV2_FILTERS, 3, 3))
        std_dev = sqrt(2.0 / (CONV2_FILTERS * 3 * 3))
        call random_number(temp)
        temp = (temp - 0.5) * 2.0 * std_dev
        model%conv3_weights = temp
        deallocate(temp)
        model%conv3_bias = 0.0

        ! Initialize FC weights (He initialization)
        call random_normal_2d(model%fc1_weights, 0.0, sqrt(2.0 / real(CONV3_FILTERS * 4 * 4)))
        model%fc1_bias = 0.0

        call random_normal_2d(model%fc2_weights, 0.0, sqrt(2.0 / 512.0))
        model%fc2_bias = 0.0

        call random_normal_2d(model%fc3_weights, 0.0, sqrt(2.0 / 256.0))
        model%fc3_bias = 0.0

        ! Add after FC weight initialization
        block
            real(4) :: fc1_check(5), conv1_check(5)

            ! Check if weights are reasonable
            fc1_check = model%fc1_weights(1:5, 1)
            conv1_check = model%conv1_weights(1, 1, 1, 1:5)

            print *, "ðŸ” Weight initialization check:"
            print *, "   FC1 first 5 weights:", fc1_check
            print *, "   Conv1 first 5 weights:", conv1_check

            if (abs(fc1_check(1)) < 1e-6) then
                print *, "âŒ WARNING: Weights appear to be zero!"
            endif
        end block

        ! Initialize batch norm parameters
        model%bn1_scale = 1.0
        model%bn1_bias = 0.0
        model%bn1_running_mean = 0.0
        model%bn1_running_var = 1.0 + eps

        model%bn2_scale = 1.0
        model%bn2_bias = 0.0
        model%bn2_running_mean = 0.0
        model%bn2_running_var = 1.0 + eps

        model%bn3_scale = 1.0
        model%bn3_bias = 0.0
        model%bn3_running_mean = 0.0
        model%bn3_running_var = 1.0 + eps

        ! Allocate gradients
        allocate(model%grad_input(batch_size, 3, 32, 32))
        allocate(model%grad_conv1(batch_size, CONV1_FILTERS, 32, 32))
        allocate(model%grad_bn1(batch_size, CONV1_FILTERS, 32, 32))
        allocate(model%grad_relu1(batch_size, CONV1_FILTERS, 32, 32))
        allocate(model%grad_pool1(batch_size, CONV1_FILTERS, 16, 16))
        allocate(model%grad_conv2(batch_size, CONV2_FILTERS, 16, 16))
        allocate(model%grad_bn2(batch_size, CONV2_FILTERS, 16, 16))
        allocate(model%grad_relu2(batch_size, CONV2_FILTERS, 16, 16))
        allocate(model%grad_pool2(batch_size, CONV2_FILTERS, 8, 8))
        allocate(model%grad_conv3(batch_size, CONV3_FILTERS, 8, 8))
        allocate(model%grad_bn3(batch_size, CONV3_FILTERS, 8, 8))
        allocate(model%grad_relu3(batch_size, CONV3_FILTERS, 8, 8))
        allocate(model%grad_pool3(batch_size, CONV3_FILTERS, 4, 4))
        allocate(model%grad_fc1(batch_size, 512))
        allocate(model%grad_fc2(batch_size, 256))
        allocate(model%grad_fc3(batch_size, CNN_NUM_CLASSES))

        allocate(model%grad_conv1_weights(CONV1_FILTERS, 3, 3, 3))
        allocate(model%grad_conv1_bias(CONV1_FILTERS))
        allocate(model%grad_conv2_weights(CONV2_FILTERS, CONV1_FILTERS, 3, 3))
        allocate(model%grad_conv2_bias(CONV2_FILTERS))
        allocate(model%grad_conv3_weights(CONV3_FILTERS, CONV2_FILTERS, 3, 3))
        allocate(model%grad_conv3_bias(CONV3_FILTERS))
        allocate(model%grad_fc1_weights(512, CONV3_FILTERS * 4 * 4))
        allocate(model%grad_fc1_bias(512))
        allocate(model%grad_fc2_weights(256, 512))
        allocate(model%grad_fc2_bias(256))
        allocate(model%grad_fc3_weights(CNN_NUM_CLASSES, 256))
        allocate(model%grad_fc3_bias(CNN_NUM_CLASSES))
        allocate(model%grad_bn1_scale(CONV1_FILTERS))
        allocate(model%grad_bn1_bias(CONV1_FILTERS))
        allocate(model%grad_bn2_scale(CONV2_FILTERS))
        allocate(model%grad_bn2_bias(CONV2_FILTERS))
        allocate(model%grad_bn3_scale(CONV3_FILTERS))
        allocate(model%grad_bn3_bias(CONV3_FILTERS))

        ! ========================================================================
        ! âœ… ALLOCATE FC WEIGHTS (3 layers with correct dimensions)
        ! ========================================================================
        allocate(model%fc1_weights(512, CONV3_FILTERS * 4 * 4))  ! 512 x 2048
        allocate(model%fc1_bias(512))
        allocate(model%fc2_weights(256, 512))                     ! 256 x 512
        allocate(model%fc2_bias(256))
        allocate(model%fc3_weights(CNN_NUM_CLASSES, 256))        ! âœ… NEW: 10 x 256
        allocate(model%fc3_bias(CNN_NUM_CLASSES))                ! âœ… NEW: 10

        ! ========================================================================
        ! âœ… ALLOCATE FC OUTPUTS (with intermediate states)
        ! ========================================================================
        allocate(model%flatten(batch_size, CONV3_FILTERS * 4 * 4))
        allocate(model%fc1_out(batch_size, 512))
        allocate(model%fc1_relu(batch_size, 512))         ! âœ… NEW
        allocate(model%fc1_dropout(batch_size, 512))      ! âœ… NEW
        allocate(model%fc2_out(batch_size, 256))
        allocate(model%fc2_relu(batch_size, 256))         ! âœ… NEW
        allocate(model%fc2_dropout(batch_size, 256))      ! âœ… NEW
        allocate(model%fc3_out(batch_size, CNN_NUM_CLASSES))  ! âœ… NEW
        allocate(model%softmax_out(batch_size, CNN_NUM_CLASSES))

        ! ========================================================================
        ! âœ… ALLOCATE FC GRADIENTS (3 layers)
        ! ========================================================================
        allocate(model%grad_fc1_weights(512, CONV3_FILTERS * 4 * 4))
        allocate(model%grad_fc1_bias(512))
        allocate(model%grad_fc2_weights(256, 512))
        allocate(model%grad_fc2_bias(256))
        allocate(model%grad_fc3_weights(CNN_NUM_CLASSES, 256))  ! âœ… NEW
        allocate(model%grad_fc3_bias(CNN_NUM_CLASSES))          ! âœ… NEW


        ! Allocate FC gradient intermediate states
        allocate(model%grad_fc1(batch_size, 512))
        allocate(model%grad_fc1_dropout(batch_size, 512))
        allocate(model%grad_fc1_relu(batch_size, 512))
        allocate(model%grad_fc2(batch_size, 256))
        allocate(model%grad_fc2_dropout(batch_size, 256))
        allocate(model%grad_fc2_relu(batch_size, 256))
        allocate(model%grad_fc3(batch_size, CNN_NUM_CLASSES))

        ! ========================================================================
        ! ALLOCATE ADAM OPTIMIZER STATE
        ! ========================================================================
        print *, "ðŸ”§ Initializing Adam optimizer state..."

        ! Conv layers - first moments
        allocate(model%m_conv1_weights(CONV1_FILTERS, 3, 3, 3))
        allocate(model%m_conv1_bias(CONV1_FILTERS))
        allocate(model%m_conv2_weights(CONV2_FILTERS, CONV1_FILTERS, 3, 3))
        allocate(model%m_conv2_bias(CONV2_FILTERS))
        allocate(model%m_conv3_weights(CONV3_FILTERS, CONV2_FILTERS, 3, 3))
        allocate(model%m_conv3_bias(CONV3_FILTERS))

        ! Conv layers - second moments
        allocate(model%v_conv1_weights(CONV1_FILTERS, 3, 3, 3))
        allocate(model%v_conv1_bias(CONV1_FILTERS))
        allocate(model%v_conv2_weights(CONV2_FILTERS, CONV1_FILTERS, 3, 3))
        allocate(model%v_conv2_bias(CONV2_FILTERS))
        allocate(model%v_conv3_weights(CONV3_FILTERS, CONV2_FILTERS, 3, 3))
        allocate(model%v_conv3_bias(CONV3_FILTERS))

        ! FC layers - first moments
        allocate(model%m_fc1_weights(512, CONV3_FILTERS * 4 * 4))
        allocate(model%m_fc1_bias(512))
        allocate(model%m_fc2_weights(256, 512))
        allocate(model%m_fc2_bias(256))
        allocate(model%m_fc3_weights(CNN_NUM_CLASSES, 256))
        allocate(model%m_fc3_bias(CNN_NUM_CLASSES))

        ! FC layers - second moments
        allocate(model%v_fc1_weights(512, CONV3_FILTERS * 4 * 4))
        allocate(model%v_fc1_bias(512))
        allocate(model%v_fc2_weights(256, 512))
        allocate(model%v_fc2_bias(256))
        allocate(model%v_fc3_weights(CNN_NUM_CLASSES, 256))
        allocate(model%v_fc3_bias(CNN_NUM_CLASSES))

        ! Batch norm - first moments
        allocate(model%m_bn1_scale(CONV1_FILTERS))
        allocate(model%m_bn1_bias(CONV1_FILTERS))
        allocate(model%m_bn2_scale(CONV2_FILTERS))
        allocate(model%m_bn2_bias(CONV2_FILTERS))
        allocate(model%m_bn3_scale(CONV3_FILTERS))
        allocate(model%m_bn3_bias(CONV3_FILTERS))

        ! Batch norm - second moments
        allocate(model%v_bn1_scale(CONV1_FILTERS))
        allocate(model%v_bn1_bias(CONV1_FILTERS))
        allocate(model%v_bn2_scale(CONV2_FILTERS))
        allocate(model%v_bn2_bias(CONV2_FILTERS))
        allocate(model%v_bn3_scale(CONV3_FILTERS))
        allocate(model%v_bn3_bias(CONV3_FILTERS))

        ! Initialize all moments to zero
        model%m_conv1_weights = 0.0; model%v_conv1_weights = 0.0
        model%m_conv1_bias = 0.0; model%v_conv1_bias = 0.0
        model%m_conv2_weights = 0.0; model%v_conv2_weights = 0.0
        model%m_conv2_bias = 0.0; model%v_conv2_bias = 0.0
        model%m_conv3_weights = 0.0; model%v_conv3_weights = 0.0
        model%m_conv3_bias = 0.0; model%v_conv3_bias = 0.0
        model%m_fc1_weights = 0.0; model%v_fc1_weights = 0.0
        model%m_fc1_bias = 0.0; model%v_fc1_bias = 0.0
        model%m_fc2_weights = 0.0; model%v_fc2_weights = 0.0
        model%m_fc2_bias = 0.0; model%v_fc2_bias = 0.0
        model%m_fc3_weights = 0.0; model%v_fc3_weights = 0.0
        model%m_fc3_bias = 0.0; model%v_fc3_bias = 0.0
        model%m_bn1_scale = 0.0; model%v_bn1_scale = 0.0
        model%m_bn1_bias = 0.0; model%v_bn1_bias = 0.0
        model%m_bn2_scale = 0.0; model%v_bn2_scale = 0.0
        model%m_bn2_bias = 0.0; model%v_bn2_bias = 0.0
        model%m_bn3_scale = 0.0; model%v_bn3_scale = 0.0
        model%m_bn3_bias = 0.0; model%v_bn3_bias = 0.0

        model%adam_timestep = 0

        print *, "âœ… Adam optimizer initialized"

        ! ========================================================================
        ! âœ… INITIALIZE FC WEIGHTS (He initialization)
        ! ========================================================================
        ! FC1: 2048 â†’ 512
        call random_normal_2d(model%fc1_weights, 0.0, sqrt(2.0 / real(CONV3_FILTERS * 4 * 4)))
        model%fc1_bias = 0.0

        ! FC2: 512 â†’ 256
        call random_normal_2d(model%fc2_weights, 0.0, sqrt(2.0 / 512.0))
        model%fc2_bias = 0.0

        ! âœ… FC3: 256 â†’ 10
        call random_normal_2d(model%fc3_weights, 0.0, sqrt(2.0 / 256.0))
        model%fc3_bias = 0.0

        ! ========================================================================
        ! âœ… SETUP DROPOUT (cuDNN requires dropout descriptor and state)
        ! ========================================================================
        ! Create dropout descriptor
        stat = cudnnCreateDropoutDescriptor(model%dropout_desc)
        if (stat /= CUDNN_STATUS_SUCCESS) then
            print *, "âŒ Failed to create dropout descriptor:", stat
            return
        endif

        ! Get dropout state size
        stat = cudnnDropoutGetStatesSize(cudnn_handle, dropout_bytes)
        if (stat /= CUDNN_STATUS_SUCCESS) then
            print *, "âŒ Failed to get dropout state size:", stat
            return
        endif

        model%dropout_state_size = dropout_bytes

        ! Allocate dropout states
        allocate(model%dropout_states(int(dropout_bytes / 4)))  ! Size in float32

        ! Initialize dropout descriptor (dropout rate = 0.5, seed = 1234)
        stat = cudnnSetDropoutDescriptor(model%dropout_desc, cudnn_handle, &
                0.5, c_loc(model%dropout_states), dropout_bytes, 1234_8)
        if (stat /= CUDNN_STATUS_SUCCESS) then
            print *, "âŒ Failed to set dropout descriptor:", stat
            return
        endif

        print *, "âœ… Model created with 3-layer FC (512â†’256â†’10) and dropout"

        ! Allocate workspace (64MB should be enough)
        model%workspace_size = 64 * 1024 * 1024
        allocate(model%workspace(model%workspace_size / 4))

        print *, "âœ… Model created and initialized"
    end subroutine create_model

    subroutine clip_gradients(model, max_norm)
        type(cnn_model), intent(inout) :: model
        real(4), intent(in) :: max_norm
        real(4), device :: total_norm_device
        real(4) :: total_norm_host, clip_coef

        ! Initialize device variable
        total_norm_device = 0.0

        ! Compute squared norm on GPU using reduction kernels
        call compute_gradient_norm_squared(model, total_norm_device)

        ! Copy result to host and compute sqrt
        total_norm_host = sqrt(total_norm_device)

        ! Only clip if norm exceeds threshold
        if (total_norm_host > max_norm) then
            clip_coef = max_norm / (total_norm_host + 1e-6)
            print *, "âš ï¸  Clipping gradients: norm =", total_norm_host, "â†’", max_norm

            ! Scale all gradients on GPU uniformly (this is correct for clipping)
            call scale_all_gradients_uniform(model, clip_coef)
        endif
    end subroutine clip_gradients

    ! New function: scales all gradients by the same factor (for clipping only)
    subroutine scale_all_gradients_uniform(model, scale)
        type(cnn_model), intent(inout) :: model
        real(4), intent(in) :: scale
        integer :: i, j, k, l

        ! Scale Conv1 gradients
        associate(grad => model%grad_conv1_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            grad(i,j,k,l) = grad(i,j,k,l) * scale
                        end do
                    end do
                end do
            end do
        end associate

        associate(grad => model%grad_conv1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale Conv2 gradients
        associate(grad => model%grad_conv2_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            grad(i,j,k,l) = grad(i,j,k,l) * scale
                        end do
                    end do
                end do
            end do
        end associate

        associate(grad => model%grad_conv2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale Conv3 gradients
        associate(grad => model%grad_conv3_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            grad(i,j,k,l) = grad(i,j,k,l) * scale
                        end do
                    end do
                end do
            end do
        end associate

        associate(grad => model%grad_conv3_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale FC1 gradients
        associate(grad => model%grad_fc1_weights)
            !$cuf kernel do(2)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    grad(i,j) = grad(i,j) * scale
                end do
            end do
        end associate

        associate(grad => model%grad_fc1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale FC2 gradients
        associate(grad => model%grad_fc2_weights)
            !$cuf kernel do(2)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    grad(i,j) = grad(i,j) * scale
                end do
            end do
        end associate

        associate(grad => model%grad_fc2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale FC3 gradients
        associate(grad => model%grad_fc3_weights)
            !$cuf kernel do(2)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    grad(i,j) = grad(i,j) * scale
                end do
            end do
        end associate

        associate(grad => model%grad_fc3_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale BatchNorm gradients
        associate(grad => model%grad_bn1_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn2_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn3_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn3_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate
    end subroutine scale_all_gradients_uniform

    ! Compute gradient norm squared (all on GPU)
    subroutine compute_gradient_norm_squared(model, norm_sq)
        type(cnn_model), intent(in) :: model
        real(4), device, intent(inout) :: norm_sq
        integer :: i, j, k, l

        ! Conv1 weights contribution
        associate(grad => model%grad_conv1_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            norm_sq = norm_sq + grad(i,j,k,l)**2
                        end do
                    end do
                end do
            end do
        end associate

        ! Conv1 bias contribution
        associate(grad => model%grad_conv1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        ! Conv2 weights contribution
        associate(grad => model%grad_conv2_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            norm_sq = norm_sq + grad(i,j,k,l)**2
                        end do
                    end do
                end do
            end do
        end associate

        ! Conv2 bias contribution
        associate(grad => model%grad_conv2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        ! Conv3 weights contribution
        associate(grad => model%grad_conv3_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            norm_sq = norm_sq + grad(i,j,k,l)**2
                        end do
                    end do
                end do
            end do
        end associate

        ! Conv3 bias contribution
        associate(grad => model%grad_conv3_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        ! FC1 weights contribution
        associate(grad => model%grad_fc1_weights)
            !$cuf kernel do(2)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    norm_sq = norm_sq + grad(i,j)**2
                end do
            end do
        end associate

        ! FC1 bias contribution
        associate(grad => model%grad_fc1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        ! FC2 weights contribution
        associate(grad => model%grad_fc2_weights)
            !$cuf kernel do(2)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    norm_sq = norm_sq + grad(i,j)**2
                end do
            end do
        end associate

        ! FC2 bias contribution
        associate(grad => model%grad_fc2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        ! Batch norm gradients
        associate(grad => model%grad_bn1_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        associate(grad => model%grad_bn1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        associate(grad => model%grad_bn2_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        associate(grad => model%grad_bn2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        associate(grad => model%grad_bn3_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate

        associate(grad => model%grad_bn3_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                norm_sq = norm_sq + grad(i)**2
            end do
        end associate
    end subroutine compute_gradient_norm_squared

    subroutine scale_conv_gradients_by_spatial(model, batch_size)
        type(cnn_model), intent(inout) :: model
        integer, intent(in) :: batch_size
        integer :: i, j, k, l
        real(4) :: scale1, scale2, scale3

        ! Conv gradients from cuDNN are summed over spatial dimensions
        ! Need to divide by: batch_size * height * width
        scale1 = 1.0 / real(batch_size * 32 * 32)  ! Conv1: 32x32 output
        scale2 = 1.0 / real(batch_size * 16 * 16)  ! Conv2: 16x16 output
        scale3 = 1.0 / real(batch_size * 8 * 8)    ! Conv3: 8x8 output

        ! Scale Conv1 weights
        associate(grad => model%grad_conv1_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            grad(i,j,k,l) = grad(i,j,k,l) * scale1
                        end do
                    end do
                end do
            end do
        end associate

        associate(grad => model%grad_conv1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale1
            end do
        end associate

        ! Scale Conv2 weights
        associate(grad => model%grad_conv2_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            grad(i,j,k,l) = grad(i,j,k,l) * scale2
                        end do
                    end do
                end do
            end do
        end associate

        associate(grad => model%grad_conv2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale2
            end do
        end associate

        ! Scale Conv3 weights
        associate(grad => model%grad_conv3_weights)
            !$cuf kernel do(4)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    do k = 1, size(grad, 3)
                        do l = 1, size(grad, 4)
                            grad(i,j,k,l) = grad(i,j,k,l) * scale3
                        end do
                    end do
                end do
            end do
        end associate

        associate(grad => model%grad_conv3_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale3
            end do
        end associate
    end subroutine scale_conv_gradients_by_spatial

    subroutine scale_fc_and_bn_gradients(model, batch_size)
        type(cnn_model), intent(inout) :: model
        integer, intent(in) :: batch_size
        integer :: i, j
        real(4) :: scale

        ! FC and BatchNorm gradients only need batch size scaling
        scale = 1.0 / real(batch_size)

        ! Scale FC1 gradients
        associate(grad => model%grad_fc1_weights)
            !$cuf kernel do(2)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    grad(i,j) = grad(i,j) * scale
                end do
            end do
        end associate

        associate(grad => model%grad_fc1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale FC2 gradients
        associate(grad => model%grad_fc2_weights)
            !$cuf kernel do(2)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    grad(i,j) = grad(i,j) * scale
                end do
            end do
        end associate

        associate(grad => model%grad_fc2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale FC3 gradients
        associate(grad => model%grad_fc3_weights)
            !$cuf kernel do(2)
            do i = 1, size(grad, 1)
                do j = 1, size(grad, 2)
                    grad(i,j) = grad(i,j) * scale
                end do
            end do
        end associate

        associate(grad => model%grad_fc3_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        ! Scale BatchNorm gradients
        associate(grad => model%grad_bn1_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn1_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn2_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn2_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn3_scale)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate

        associate(grad => model%grad_bn3_bias)
            !$cuf kernel do(1)
            do i = 1, size(grad)
                grad(i) = grad(i) * scale
            end do
        end associate
    end subroutine scale_fc_and_bn_gradients

    ! ============================================================================
    ! This version uses cudnnActivationForward instead of custom_leaky_relu_4d
    ! CRITICAL FIX: Added is_training parameter to use correct batch norm mode
    ! Training: Uses cudnnBatchNormalizationForwardTraining (updates running stats)
    ! Inference: Uses cudnnBatchNormalizationForwardInference (uses learned stats)
    ! ============================================================================

    subroutine forward_pass(model, input_batch, batch_size, is_training)
        type(cnn_model), intent(inout) :: model
        real(4), device, intent(in) :: input_batch(:,:)  ! (batch_size, 3072)
        integer, intent(in) :: batch_size
        logical, intent(in) :: is_training

        integer(c_int) :: stat
        real(c_float), target :: alpha = 1.0, beta = 0.0
        real(c_double), parameter :: eps = 1.0d-3
        integer :: i, j, k, idx

        ! ====================================================================
        ! RESHAPE INPUT: (batch, 3072) â†’ (batch, 3, 32, 32)
        ! ====================================================================
        associate(input_array => model%input)
            !$cuf kernel do(4)
            do i = 1, batch_size
                do j = 1, 3
                    do k = 1, 32
                        do idx = 1, 32
                            input_array(i, j, k, idx) = input_batch(i, (j-1)*1024 + (k-1)*32 + idx)
                        end do
                    end do
                end do
            end do
        end associate

        ! ====================================================================
        ! CONV BLOCK 1: Conv1 â†’ Bias â†’ BN1 â†’ LeakyReLU â†’ Pool1
        ! ====================================================================

        ! Conv1: (batch, 3, 32, 32) â†’ (batch, 32, 32, 32)
        stat = cudnnConvolutionForward(cudnn_handle, c_loc(alpha), &
                model%input_desc, c_loc(model%input), &
                model%conv1_filter_desc, c_loc(model%conv1_weights), &
                model%conv1_conv_desc, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, &
                c_loc(model%workspace), model%workspace_size, &
                c_loc(beta), model%conv1_desc, c_loc(model%conv1_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Conv1 failed:", stat

        ! Add bias to Conv1
        stat = cudnnAddTensor(cudnn_handle, c_loc(alpha), &
                model%conv1_bias_desc, c_loc(model%conv1_bias), &
                c_loc(alpha), model%conv1_desc, c_loc(model%conv1_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Conv1 bias add failed:", stat

        ! BatchNorm1 with conditional mode
        if (is_training) then
            stat = cudnnBatchNormalizationForwardTraining(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                    c_loc(alpha), c_loc(beta), &
                    model%conv1_desc, c_loc(model%conv1_out), &
                    model%conv1_desc, c_loc(model%bn1_out), &
                    model%bn1_desc, c_loc(model%bn1_scale), c_loc(model%bn1_bias), &
                    0.5d0, c_loc(model%bn1_running_mean), c_loc(model%bn1_running_var), &
                    eps, c_loc(model%bn1_saved_mean), c_loc(model%bn1_saved_inv_var))
        else
            stat = cudnnBatchNormalizationForwardInference(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                    c_loc(alpha), c_loc(beta), &
                    model%conv1_desc, c_loc(model%conv1_out), &
                    model%conv1_desc, c_loc(model%bn1_out), &
                    model%bn1_desc, c_loc(model%bn1_scale), c_loc(model%bn1_bias), &
                    c_loc(model%bn1_running_mean), c_loc(model%bn1_running_var), &
                    eps)
        endif
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ BN1 failed:", stat

        if (debug_nan_checks) then
            block
                real(4) :: bn1_sample
                bn1_sample = model%bn1_out(1, 1, 1, 1)
                if (bn1_sample /= bn1_sample) then
                    print *, "âŒ NaN in BN1"
                    stop
                endif
            end block
        endif

        ! LeakyReLU1
        stat = cudnnActivationForward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%conv1_desc, c_loc(model%bn1_out), &
                c_loc(beta), model%conv1_desc, c_loc(model%relu1_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ ReLU1 failed:", stat

        ! Pool1: (batch, 32, 32, 32) â†’ (batch, 32, 16, 16)
        stat = cudnnPoolingForward(cudnn_handle, model%pooling_desc, &
                c_loc(alpha), model%conv1_desc, c_loc(model%relu1_out), &
                c_loc(beta), model%pool1_desc, c_loc(model%pool1_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Pool1 failed:", stat

        ! ====================================================================
        ! CONV BLOCK 2: Conv2 â†’ Bias â†’ BN2 â†’ LeakyReLU â†’ Pool2
        ! ====================================================================

        ! Conv2: (batch, 32, 16, 16) â†’ (batch, 64, 16, 16)
        stat = cudnnConvolutionForward(cudnn_handle, c_loc(alpha), &
                model%pool1_desc, c_loc(model%pool1_out), &
                model%conv2_filter_desc, c_loc(model%conv2_weights), &
                model%conv2_conv_desc, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, &
                c_loc(model%workspace), model%workspace_size, &
                c_loc(beta), model%conv2_desc, c_loc(model%conv2_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Conv2 failed:", stat

        ! Add bias to Conv2
        stat = cudnnAddTensor(cudnn_handle, c_loc(alpha), &
                model%conv2_bias_desc, c_loc(model%conv2_bias), &
                c_loc(alpha), model%conv2_desc, c_loc(model%conv2_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Conv2 bias add failed:", stat

        ! BatchNorm2 with conditional mode
        if (is_training) then
            stat = cudnnBatchNormalizationForwardTraining(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                    c_loc(alpha), c_loc(beta), &
                    model%conv2_desc, c_loc(model%conv2_out), &
                    model%conv2_desc, c_loc(model%bn2_out), &
                    model%bn2_desc, c_loc(model%bn2_scale), c_loc(model%bn2_bias), &
                    0.5d0, c_loc(model%bn2_running_mean), c_loc(model%bn2_running_var), &
                    eps, c_loc(model%bn2_saved_mean), c_loc(model%bn2_saved_inv_var))
        else
            stat = cudnnBatchNormalizationForwardInference(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                    c_loc(alpha), c_loc(beta), &
                    model%conv2_desc, c_loc(model%conv2_out), &
                    model%conv2_desc, c_loc(model%bn2_out), &
                    model%bn2_desc, c_loc(model%bn2_scale), c_loc(model%bn2_bias), &
                    c_loc(model%bn2_running_mean), c_loc(model%bn2_running_var), &
                    eps)
        endif
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ BN2 failed:", stat

        if (debug_nan_checks) then
            block
                real(4) :: bn2_sample
                bn2_sample = model%bn2_out(1, 1, 1, 1)
                if (bn2_sample /= bn2_sample) then
                    print *, "âŒ NaN in BN2"
                    stop
                endif
            end block
        endif

        ! LeakyReLU2
        stat = cudnnActivationForward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%conv2_desc, c_loc(model%bn2_out), &
                c_loc(beta), model%conv2_desc, c_loc(model%relu2_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ ReLU2 failed:", stat

        ! Pool2: (batch, 64, 16, 16) â†’ (batch, 64, 8, 8)
        stat = cudnnPoolingForward(cudnn_handle, model%pooling_desc, &
                c_loc(alpha), model%conv2_desc, c_loc(model%relu2_out), &
                c_loc(beta), model%pool2_desc, c_loc(model%pool2_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Pool2 failed:", stat

        ! ====================================================================
        ! CONV BLOCK 3: Conv3 â†’ Bias â†’ BN3 â†’ LeakyReLU â†’ Pool3
        ! ====================================================================

        ! Conv3: (batch, 64, 8, 8) â†’ (batch, 128, 8, 8)
        stat = cudnnConvolutionForward(cudnn_handle, c_loc(alpha), &
                model%pool2_desc, c_loc(model%pool2_out), &
                model%conv3_filter_desc, c_loc(model%conv3_weights), &
                model%conv3_conv_desc, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM, &
                c_loc(model%workspace), model%workspace_size, &
                c_loc(beta), model%conv3_desc, c_loc(model%conv3_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Conv3 failed:", stat

        ! Add bias to Conv3
        stat = cudnnAddTensor(cudnn_handle, c_loc(alpha), &
                model%conv3_bias_desc, c_loc(model%conv3_bias), &
                c_loc(alpha), model%conv3_desc, c_loc(model%conv3_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Conv3 bias add failed:", stat

        ! BatchNorm3 with conditional mode
        if (is_training) then
            stat = cudnnBatchNormalizationForwardTraining(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                    c_loc(alpha), c_loc(beta), &
                    model%conv3_desc, c_loc(model%conv3_out), &
                    model%conv3_desc, c_loc(model%bn3_out), &
                    model%bn3_desc, c_loc(model%bn3_scale), c_loc(model%bn3_bias), &
                    0.5d0, c_loc(model%bn3_running_mean), c_loc(model%bn3_running_var), &
                    eps, c_loc(model%bn3_saved_mean), c_loc(model%bn3_saved_inv_var))
        else
            stat = cudnnBatchNormalizationForwardInference(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                    c_loc(alpha), c_loc(beta), &
                    model%conv3_desc, c_loc(model%conv3_out), &
                    model%conv3_desc, c_loc(model%bn3_out), &
                    model%bn3_desc, c_loc(model%bn3_scale), c_loc(model%bn3_bias), &
                    c_loc(model%bn3_running_mean), c_loc(model%bn3_running_var), &
                    eps)
        endif
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ BN3 failed:", stat

        if (debug_nan_checks) then
            block
                real(4) :: bn3_sample
                bn3_sample = model%bn3_out(1, 1, 1, 1)
                if (bn3_sample /= bn3_sample) then
                    print *, "âŒ NaN in BN3"
                    stop
                endif
            end block
        endif

        ! LeakyReLU3
        stat = cudnnActivationForward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%conv3_desc, c_loc(model%bn3_out), &
                c_loc(beta), model%conv3_desc, c_loc(model%relu3_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ ReLU3 failed:", stat

        ! Pool3: (batch, 128, 8, 8) â†’ (batch, 128, 4, 4)
        stat = cudnnPoolingForward(cudnn_handle, model%pooling_desc, &
                c_loc(alpha), model%conv3_desc, c_loc(model%relu3_out), &
                c_loc(beta), model%pool3_desc, c_loc(model%pool3_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Pool3 failed:", stat

        ! ====================================================================
        ! FLATTEN: (batch, 128, 4, 4) â†’ (batch, 2048)
        ! ====================================================================
        associate(pool3_array => model%pool3_out, flatten_array => model%flatten)
            !$cuf kernel do(2)
            do i = 1, batch_size
                do j = 1, CONV3_FILTERS * 4 * 4
                    idx = j - 1
                    k = idx / (4 * 4)
                    idx = mod(idx, 4 * 4)
                    flatten_array(i, j) = pool3_array(i, k+1, idx/4 + 1, mod(idx,4) + 1)
                end do
            end do
        end associate

        ! ====================================================================
        ! FC1: (batch, 2048) â†’ (batch, 512)
        ! ====================================================================
        stat = cublasSgemm_v2(cublas_handle, 0, 1, &
                int(batch_size, c_int), int(512, c_int), int(CONV3_FILTERS * 4 * 4, c_int), &
                c_loc(alpha), c_loc(model%flatten), int(batch_size, c_int), &
                c_loc(model%fc1_weights), int(512, c_int), &
                c_loc(beta), c_loc(model%fc1_out), int(batch_size, c_int))

        ! Add bias to FC1
        associate(fc1_array => model%fc1_out, fc1_bias_array => model%fc1_bias)
            !$cuf kernel do(2)
            do i = 1, batch_size
                do j = 1, 512
                    fc1_array(i, j) = fc1_array(i, j) + fc1_bias_array(j)
                end do
            end do
        end associate

        ! LeakyReLU after FC1
        stat = cudnnActivationForward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%fc1_desc, c_loc(model%fc1_out), &
                c_loc(beta), model%fc1_desc, c_loc(model%fc1_relu))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ FC1 ReLU failed:", stat

        ! Dropout after FC1 (only during training)
        if (is_training) then
            stat = cudnnDropoutForward(cudnn_handle, model%dropout_desc, &
                    model%fc1_desc, c_loc(model%fc1_relu), &
                    model%fc1_desc, c_loc(model%fc1_dropout), &
                    c_loc(model%dropout_states), model%dropout_state_size)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ FC1 Dropout failed:", stat
        else
            model%fc1_dropout = model%fc1_relu
        endif

        ! ====================================================================
        ! FC2: (batch, 512) â†’ (batch, 256)
        ! ====================================================================
        stat = cublasSgemm_v2(cublas_handle, 0, 1, &
                int(batch_size, c_int), int(256, c_int), int(512, c_int), &
                c_loc(alpha), c_loc(model%fc1_dropout), int(batch_size, c_int), &
                c_loc(model%fc2_weights), int(256, c_int), &
                c_loc(beta), c_loc(model%fc2_out), int(batch_size, c_int))

        ! Add bias to FC2
        associate(fc2_array => model%fc2_out, fc2_bias_array => model%fc2_bias)
            !$cuf kernel do(2)
            do i = 1, batch_size
                do j = 1, 256
                    fc2_array(i, j) = fc2_array(i, j) + fc2_bias_array(j)
                end do
            end do
        end associate

        ! LeakyReLU after FC2
        stat = cudnnActivationForward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%fc2_desc, c_loc(model%fc2_out), &
                c_loc(beta), model%fc2_desc, c_loc(model%fc2_relu))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ FC2 ReLU failed:", stat

        ! Dropout after FC2 (only during training)
        if (is_training) then
            stat = cudnnDropoutForward(cudnn_handle, model%dropout_desc, &
                    model%fc2_desc, c_loc(model%fc2_relu), &
                    model%fc2_desc, c_loc(model%fc2_dropout), &
                    c_loc(model%dropout_states), model%dropout_state_size)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ FC2 Dropout failed:", stat
        else
            model%fc2_dropout = model%fc2_relu
        endif

        ! ====================================================================
        ! FC3: (batch, 256) â†’ (batch, 10)
        ! ====================================================================
        stat = cublasSgemm_v2(cublas_handle, 0, 1, &
                int(batch_size, c_int), int(CNN_NUM_CLASSES, c_int), int(256, c_int), &
                c_loc(alpha), c_loc(model%fc2_dropout), int(batch_size, c_int), &
                c_loc(model%fc3_weights), int(CNN_NUM_CLASSES, c_int), &
                c_loc(beta), c_loc(model%fc3_out), int(batch_size, c_int))

        ! Add bias to FC3
        associate(fc3_array => model%fc3_out, fc3_bias_array => model%fc3_bias)
            !$cuf kernel do(2)
            do i = 1, batch_size
                do j = 1, CNN_NUM_CLASSES
                    fc3_array(i, j) = fc3_array(i, j) + fc3_bias_array(j)
                end do
            end do
        end associate

        ! ====================================================================
        ! SOFTMAX: (batch, 10) â†’ (batch, 10)
        ! ====================================================================
        stat = cudnnSoftmaxForward(cudnn_handle, CUDNN_SOFTMAX_ACCURATE, CUDNN_SOFTMAX_MODE_CHANNEL, &
                c_loc(alpha), model%output_desc, c_loc(model%fc3_out), &
                c_loc(beta), model%output_desc, c_loc(model%softmax_out))
        if (stat /= CUDNN_STATUS_SUCCESS) print *, "âŒ Softmax failed:", stat
    end subroutine forward_pass

    ! ========================================================================
    ! Backward pass - pure cuDNN calls
    ! ========================================================================

    subroutine backward_pass(model, grad_output, batch_size, learning_rate, is_training)
        type(cnn_model), intent(inout) :: model
        real(4), device, intent(in) :: grad_output(:,:)
        integer, intent(in) :: batch_size
        real(4), intent(in) :: learning_rate
        logical, intent(in) :: is_training
        integer(c_int) :: stat
        real(c_float), target :: alpha = 1.0, beta = 0.0, beta_acc = 1.0
        real(c_double), parameter :: eps = 1e-3
        integer :: i, j, k, l, idx
        real(4), device, allocatable, target :: ones_vector(:)
        real(4), device, allocatable, target :: grad_flatten_temp(:,:)

        logical, save :: first_call = .true.
        if (first_call) then
            print *, "ðŸ” Zeroing gradients at start of backward_pass"
            first_call = .false.
        endif

        ! Zero out all gradients before backward pass
        model%grad_conv1_weights = 0.0
        model%grad_conv1_bias = 0.0
        model%grad_conv2_weights = 0.0
        model%grad_conv2_bias = 0.0
        model%grad_conv3_weights = 0.0
        model%grad_conv3_bias = 0.0
        model%grad_fc1_weights = 0.0
        model%grad_fc1_bias = 0.0
        model%grad_fc2_weights = 0.0
        model%grad_fc2_bias = 0.0
        model%grad_fc3_weights = 0.0
        model%grad_fc3_bias = 0.0
        model%grad_bn1_scale = 0.0
        model%grad_bn1_bias = 0.0
        model%grad_bn2_scale = 0.0
        model%grad_bn2_bias = 0.0
        model%grad_bn3_scale = 0.0
        model%grad_bn3_bias = 0.0

        allocate(ones_vector(batch_size))
        ones_vector = 1.0

        ! ========================================================================
        ! FC3 + Softmax Backward
        ! ========================================================================

        model%grad_fc3 = grad_output

        ! FC3 weight gradients
        stat = cublasSgemm_v2(cublas_handle, 1, 0, &
                int(256, c_int), int(CNN_NUM_CLASSES, c_int), int(batch_size, c_int), &
                c_loc(alpha), c_loc(model%fc2_dropout), int(batch_size, c_int), &
                c_loc(model%grad_fc3), int(batch_size, c_int), &
                c_loc(beta), c_loc(model%grad_fc3_weights), int(256, c_int))

        ! FC3 bias gradients
        stat = cublasSgemv_v2(cublas_handle, 1, &
                int(batch_size, c_int), int(CNN_NUM_CLASSES, c_int), &
                c_loc(alpha), c_loc(model%grad_fc3), int(batch_size, c_int), &
                c_loc(ones_vector), 1, &
                c_loc(beta), c_loc(model%grad_fc3_bias), 1)

        ! FC3 input gradients
        stat = cublasSgemm_v2(cublas_handle, 0, 0, &
                int(batch_size, c_int), int(256, c_int), int(CNN_NUM_CLASSES, c_int), &
                c_loc(alpha), c_loc(model%grad_fc3), int(batch_size, c_int), &
                c_loc(model%fc3_weights), int(CNN_NUM_CLASSES, c_int), &
                c_loc(beta), c_loc(model%grad_fc2), int(batch_size, c_int))

        ! ========================================================================
        ! FC2 Backward
        ! ========================================================================

        if (is_training) then
            stat = cudnnDropoutBackward(cudnn_handle, model%dropout_desc, &
                    model%fc2_desc, c_loc(model%grad_fc2), &
                    model%fc2_desc, c_loc(model%grad_fc2_dropout), &
                    c_loc(model%dropout_states), model%dropout_state_size)
        else
            model%grad_fc2_dropout = model%grad_fc2
        endif

        stat = cudnnActivationBackward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%fc2_desc, c_loc(model%fc2_relu), &
                model%fc2_desc, c_loc(model%grad_fc2_dropout), &
                model%fc2_desc, c_loc(model%fc2_out), &
                c_loc(beta), model%fc2_desc, c_loc(model%grad_fc2_relu))

        stat = cublasSgemm_v2(cublas_handle, 1, 0, &
                int(512, c_int), int(256, c_int), int(batch_size, c_int), &
                c_loc(alpha), c_loc(model%fc1_dropout), int(batch_size, c_int), &
                c_loc(model%grad_fc2_relu), int(batch_size, c_int), &
                c_loc(beta), c_loc(model%grad_fc2_weights), int(512, c_int))

        stat = cublasSgemv_v2(cublas_handle, 1, &
                int(batch_size, c_int), int(256, c_int), &
                c_loc(alpha), c_loc(model%grad_fc2_relu), int(batch_size, c_int), &
                c_loc(ones_vector), 1, &
                c_loc(beta), c_loc(model%grad_fc2_bias), 1)

        stat = cublasSgemm_v2(cublas_handle, 0, 0, &
                int(batch_size, c_int), int(512, c_int), int(256, c_int), &
                c_loc(alpha), c_loc(model%grad_fc2_relu), int(batch_size, c_int), &
                c_loc(model%fc2_weights), int(256, c_int), &
                c_loc(beta), c_loc(model%grad_fc1), int(batch_size, c_int))

        ! ========================================================================
        ! FC1 Backward
        ! ========================================================================

        if (is_training) then
            stat = cudnnDropoutBackward(cudnn_handle, model%dropout_desc, &
                    model%fc1_desc, c_loc(model%grad_fc1), &
                    model%fc1_desc, c_loc(model%grad_fc1_dropout), &
                    c_loc(model%dropout_states), model%dropout_state_size)
        else
            model%grad_fc1_dropout = model%grad_fc1
        endif

        stat = cudnnActivationBackward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%fc1_desc, c_loc(model%fc1_relu), &
                model%fc1_desc, c_loc(model%grad_fc1_dropout), &
                model%fc1_desc, c_loc(model%fc1_out), &
                c_loc(beta), model%fc1_desc, c_loc(model%grad_fc1_relu))

        stat = cublasSgemm_v2(cublas_handle, 1, 0, &
                int(CONV3_FILTERS * 4 * 4, c_int), int(512, c_int), int(batch_size, c_int), &
                c_loc(alpha), c_loc(model%flatten), int(batch_size, c_int), &
                c_loc(model%grad_fc1_relu), int(batch_size, c_int), &
                c_loc(beta), c_loc(model%grad_fc1_weights), int(CONV3_FILTERS * 4 * 4, c_int))

        stat = cublasSgemv_v2(cublas_handle, 1, &
                int(batch_size, c_int), int(512, c_int), &
                c_loc(alpha), c_loc(model%grad_fc1_relu), int(batch_size, c_int), &
                c_loc(ones_vector), 1, &
                c_loc(beta), c_loc(model%grad_fc1_bias), 1)

        allocate(grad_flatten_temp(batch_size, CONV3_FILTERS * 4 * 4))
        stat = cublasSgemm_v2(cublas_handle, 0, 0, &
                int(batch_size, c_int), int(CONV3_FILTERS * 4 * 4, c_int), int(512, c_int), &
                c_loc(alpha), c_loc(model%grad_fc1_relu), int(batch_size, c_int), &
                c_loc(model%fc1_weights), int(512, c_int), &
                c_loc(beta), c_loc(grad_flatten_temp), int(batch_size, c_int))

        ! ========================================================================
        ! UNFLATTEN GRADIENT
        ! ========================================================================
        associate(flatten_grad => grad_flatten_temp, pool3_grad => model%grad_pool3)
            !$cuf kernel do(4)
            do i = 1, batch_size
                do j = 1, CONV3_FILTERS
                    do k = 1, 4
                        do l = 1, 4
                            idx = (j-1)*16 + (k-1)*4 + l
                            pool3_grad(i, j, k, l) = flatten_grad(i, idx)
                            if (abs(pool3_grad(i, j, k, l)) > 10.0) then
                                pool3_grad(i, j, k, l) = sign(10.0, pool3_grad(i, j, k, l))
                            endif
                        end do
                    end do
                end do
            end do
        end associate

        ! ========================================================================
        ! CONV BLOCK 3 BACKWARD
        ! ========================================================================

        stat = cudnnPoolingBackward(cudnn_handle, model%pooling_desc, &
                c_loc(alpha), model%pool3_desc, c_loc(model%pool3_out), &
                model%pool3_desc, c_loc(model%grad_pool3), &
                model%conv3_desc, c_loc(model%relu3_out), &
                c_loc(beta), model%conv3_desc, c_loc(model%grad_relu3))

        stat = cudnnActivationBackward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%conv3_desc, c_loc(model%relu3_out), &
                model%conv3_desc, c_loc(model%grad_relu3), &
                model%conv3_desc, c_loc(model%bn3_out), &
                c_loc(beta), model%conv3_desc, c_loc(model%grad_bn3))

        stat = cudnnBatchNormalizationBackward(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                c_loc(alpha), c_loc(beta), c_loc(alpha), c_loc(beta), &
                model%conv3_desc, c_loc(model%conv3_out), &
                model%conv3_desc, c_loc(model%grad_bn3), &
                model%conv3_desc, c_loc(model%grad_conv3), &
                model%bn3_desc, c_loc(model%bn3_scale), &
                c_loc(model%grad_bn3_scale), c_loc(model%grad_bn3_bias), &
                eps, c_loc(model%bn3_saved_mean), c_loc(model%bn3_saved_inv_var))

        stat = cudnnConvolutionBackwardBias(cudnn_handle, &
                c_loc(alpha), model%conv3_desc, c_loc(model%grad_conv3), &
                c_loc(beta), model%conv3_bias_desc, c_loc(model%grad_conv3_bias))

        stat = cudnnConvolutionBackwardFilter(cudnn_handle, &
                c_loc(alpha), model%pool2_desc, c_loc(model%pool2_out), &
                model%conv3_desc, c_loc(model%grad_conv3), &
                model%conv3_conv_desc, CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1, &
                c_loc(model%workspace), model%workspace_size, &
                c_loc(beta), model%conv3_filter_desc, c_loc(model%grad_conv3_weights))

        stat = cudnnConvolutionBackwardData(cudnn_handle, &
                c_loc(alpha), model%conv3_filter_desc, c_loc(model%conv3_weights), &
                model%conv3_desc, c_loc(model%grad_conv3), &
                model%conv3_conv_desc, CUDNN_CONVOLUTION_BWD_DATA_ALGO_1, &
                c_loc(model%workspace), model%workspace_size, &
                c_loc(beta), model%pool2_desc, c_loc(model%grad_pool2))

        ! ========================================================================
        ! CONV BLOCK 2 BACKWARD
        ! ========================================================================

        stat = cudnnPoolingBackward(cudnn_handle, model%pooling_desc, &
                c_loc(alpha), model%pool2_desc, c_loc(model%pool2_out), &
                model%pool2_desc, c_loc(model%grad_pool2), &
                model%conv2_desc, c_loc(model%relu2_out), &
                c_loc(beta), model%conv2_desc, c_loc(model%grad_relu2))

        stat = cudnnActivationBackward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%conv2_desc, c_loc(model%relu2_out), &
                model%conv2_desc, c_loc(model%grad_relu2), &
                model%conv2_desc, c_loc(model%bn2_out), &
                c_loc(beta), model%conv2_desc, c_loc(model%grad_bn2))

        stat = cudnnBatchNormalizationBackward(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                c_loc(alpha), c_loc(beta), c_loc(alpha), c_loc(beta), &
                model%conv2_desc, c_loc(model%conv2_out), &
                model%conv2_desc, c_loc(model%grad_bn2), &
                model%conv2_desc, c_loc(model%grad_conv2), &
                model%bn2_desc, c_loc(model%bn2_scale), &
                c_loc(model%grad_bn2_scale), c_loc(model%grad_bn2_bias), &
                eps, c_loc(model%bn2_saved_mean), c_loc(model%bn2_saved_inv_var))

        stat = cudnnConvolutionBackwardBias(cudnn_handle, &
                c_loc(alpha), model%conv2_desc, c_loc(model%grad_conv2), &
                c_loc(beta), model%conv2_bias_desc, c_loc(model%grad_conv2_bias))

        stat = cudnnConvolutionBackwardFilter(cudnn_handle, &
                c_loc(alpha), model%pool1_desc, c_loc(model%pool1_out), &
                model%conv2_desc, c_loc(model%grad_conv2), &
                model%conv2_conv_desc, CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1, &
                c_loc(model%workspace), model%workspace_size, &
                c_loc(beta), model%conv2_filter_desc, c_loc(model%grad_conv2_weights))

        stat = cudnnConvolutionBackwardData(cudnn_handle, &
                c_loc(alpha), model%conv2_filter_desc, c_loc(model%conv2_weights), &
                model%conv2_desc, c_loc(model%grad_conv2), &
                model%conv2_conv_desc, CUDNN_CONVOLUTION_BWD_DATA_ALGO_1, &
                c_loc(model%workspace), model%workspace_size, &
                c_loc(beta), model%pool1_desc, c_loc(model%grad_pool1))

        ! ========================================================================
        ! CONV BLOCK 1 BACKWARD
        ! ========================================================================

        stat = cudnnPoolingBackward(cudnn_handle, model%pooling_desc, &
                c_loc(alpha), model%pool1_desc, c_loc(model%pool1_out), &
                model%pool1_desc, c_loc(model%grad_pool1), &
                model%conv1_desc, c_loc(model%relu1_out), &
                c_loc(beta), model%conv1_desc, c_loc(model%grad_relu1))

        stat = cudnnActivationBackward(cudnn_handle, model%activation_desc, &
                c_loc(alpha), model%conv1_desc, c_loc(model%relu1_out), &
                model%conv1_desc, c_loc(model%grad_relu1), &
                model%conv1_desc, c_loc(model%bn1_out), &
                c_loc(beta), model%conv1_desc, c_loc(model%grad_bn1))

        stat = cudnnBatchNormalizationBackward(cudnn_handle, CUDNN_BATCHNORM_SPATIAL, &
                c_loc(alpha), c_loc(beta), c_loc(alpha), c_loc(beta), &
                model%conv1_desc, c_loc(model%conv1_out), &
                model%conv1_desc, c_loc(model%grad_bn1), &
                model%conv1_desc, c_loc(model%grad_conv1), &
                model%bn1_desc, c_loc(model%bn1_scale), &
                c_loc(model%grad_bn1_scale), c_loc(model%grad_bn1_bias), &
                eps, c_loc(model%bn1_saved_mean), c_loc(model%bn1_saved_inv_var))

        stat = cudnnConvolutionBackwardBias(cudnn_handle, &
                c_loc(alpha), model%conv1_desc, c_loc(model%grad_conv1), &
                c_loc(beta), model%conv1_bias_desc, c_loc(model%grad_conv1_bias))

        stat = cudnnConvolutionBackwardFilter(cudnn_handle, &
                c_loc(alpha), model%input_desc, c_loc(model%input), &
                model%conv1_desc, c_loc(model%grad_conv1), &
                model%conv1_conv_desc, CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1, &
                c_loc(model%workspace), model%workspace_size, &
                c_loc(beta), model%conv1_filter_desc, c_loc(model%grad_conv1_weights))

        block
            real(4) :: grad_check
            logical :: has_nan

            grad_check = model%grad_fc3_weights(1, 1)
            has_nan = .false.
            if (grad_check /= grad_check) has_nan = .true.

            grad_check = model%grad_conv1_weights(1, 1, 1, 1)
            if (grad_check /= grad_check) has_nan = .true.

            if (has_nan) then
                print *, "âš ï¸  NaN detected in gradients! Skipping weight update"
            endif
        end block

        call scale_conv_gradients_by_spatial(model, batch_size)
        call scale_fc_and_bn_gradients(model, batch_size)
        call update_weights(model, learning_rate)

        deallocate(ones_vector)
        if (allocated(grad_flatten_temp)) deallocate(grad_flatten_temp)
    end subroutine backward_pass

    ! ========================================================================
    ! ADAM OPTIMIZER UPDATE (matching PyTorch)
    ! ========================================================================
    subroutine update_weights(model, learning_rate)
        type(cnn_model), intent(inout) :: model
        real(4), intent(in) :: learning_rate
        real(4) :: bias_correction1, bias_correction2, corrected_lr

        ! Increment timestep
        model%adam_timestep = model%adam_timestep + 1

        ! âœ… ADD: Debug first few updates
        if (model%adam_timestep <= 3) then
            bias_correction1 = 1.0 - adam_beta1**model%adam_timestep
            bias_correction2 = 1.0 - adam_beta2**model%adam_timestep
            corrected_lr = learning_rate * sqrt(bias_correction2) / bias_correction1

            print *, "ðŸ” Adam step", model%adam_timestep
            print *, "   bias_corr1:", bias_correction1
            print *, "   bias_corr2:", bias_correction2
            print *, "   corrected_lr:", corrected_lr
            print *, "   base_lr:", learning_rate
        endif

        ! Compute bias corrections
        bias_correction1 = 1.0 - adam_beta1**model%adam_timestep
        bias_correction2 = 1.0 - adam_beta2**model%adam_timestep

        ! Update all parameters with Adam
        call adam_update_4d(model%conv1_weights, model%grad_conv1_weights, &
                           model%m_conv1_weights, model%v_conv1_weights, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%conv1_bias, model%grad_conv1_bias, &
                           model%m_conv1_bias, model%v_conv1_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        call adam_update_4d(model%conv2_weights, model%grad_conv2_weights, &
                           model%m_conv2_weights, model%v_conv2_weights, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%conv2_bias, model%grad_conv2_bias, &
                           model%m_conv2_bias, model%v_conv2_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        call adam_update_4d(model%conv3_weights, model%grad_conv3_weights, &
                           model%m_conv3_weights, model%v_conv3_weights, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%conv3_bias, model%grad_conv3_bias, &
                           model%m_conv3_bias, model%v_conv3_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        call adam_update_2d(model%fc1_weights, model%grad_fc1_weights, &
                           model%m_fc1_weights, model%v_fc1_weights, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%fc1_bias, model%grad_fc1_bias, &
                           model%m_fc1_bias, model%v_fc1_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        call adam_update_2d(model%fc2_weights, model%grad_fc2_weights, &
                           model%m_fc2_weights, model%v_fc2_weights, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%fc2_bias, model%grad_fc2_bias, &
                           model%m_fc2_bias, model%v_fc2_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        call adam_update_2d(model%fc3_weights, model%grad_fc3_weights, &
                           model%m_fc3_weights, model%v_fc3_weights, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%fc3_bias, model%grad_fc3_bias, &
                           model%m_fc3_bias, model%v_fc3_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        call adam_update_1d(model%bn1_scale, model%grad_bn1_scale, &
                           model%m_bn1_scale, model%v_bn1_scale, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%bn1_bias, model%grad_bn1_bias, &
                           model%m_bn1_bias, model%v_bn1_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        call adam_update_1d(model%bn2_scale, model%grad_bn2_scale, &
                           model%m_bn2_scale, model%v_bn2_scale, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%bn2_bias, model%grad_bn2_bias, &
                           model%m_bn2_bias, model%v_bn2_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        call adam_update_1d(model%bn3_scale, model%grad_bn3_scale, &
                           model%m_bn3_scale, model%v_bn3_scale, &
                           learning_rate, bias_correction1, bias_correction2)
        call adam_update_1d(model%bn3_bias, model%grad_bn3_bias, &
                           model%m_bn3_bias, model%v_bn3_bias, &
                           learning_rate, bias_correction1, bias_correction2)

        ! At the END of update_weights, add:
        if (model%adam_timestep == 1) then
            block
                real(4) :: fc3_weight_sample
                fc3_weight_sample = model%fc3_weights(1, 1)
                print *, "ðŸ” After first Adam update:"
                print *, "   FC3 weight sample:", fc3_weight_sample
            end block
        endif
    end subroutine update_weights

    ! Adam update for 4D arrays (conv weights)
    subroutine adam_update_4d(weights, gradients, m, v, lr, bias_corr1, bias_corr2)
        real(4), device, intent(inout) :: weights(:,:,:,:), m(:,:,:,:), v(:,:,:,:)
        real(4), device, intent(in) :: gradients(:,:,:,:)
        real(4), intent(in) :: lr, bias_corr1, bias_corr2
        integer :: i, j, k, l
        real(4) :: corrected_lr

        corrected_lr = lr * sqrt(bias_corr2) / bias_corr1

        !$cuf kernel do(4)
        do l = 1, size(weights, 4)
            do k = 1, size(weights, 3)
                do j = 1, size(weights, 2)
                    do i = 1, size(weights, 1)
                        ! Update moments
                        m(i,j,k,l) = adam_beta1 * m(i,j,k,l) + (1.0 - adam_beta1) * gradients(i,j,k,l)
                        v(i,j,k,l) = adam_beta2 * v(i,j,k,l) + (1.0 - adam_beta2) * gradients(i,j,k,l)**2

                        ! Update weights with L2 regularization
                        weights(i,j,k,l) = weights(i,j,k,l) * (1.0 - lr * weight_decay) - &
                                          corrected_lr * m(i,j,k,l) / (sqrt(v(i,j,k,l)) + adam_epsilon)
                    end do
                end do
            end do
        end do
    end subroutine adam_update_4d

    ! Adam update for 2D arrays (FC weights)
    subroutine adam_update_2d_b(weights, gradients, m, v, lr, bias_corr1, bias_corr2)
        real(4), device, intent(inout) :: weights(:,:), m(:,:), v(:,:)
        real(4), device, intent(in) :: gradients(:,:)
        real(4), intent(in) :: lr, bias_corr1, bias_corr2
        integer :: i, j
        real(4) :: corrected_lr

        corrected_lr = lr * sqrt(bias_corr2) / bias_corr1

        !$cuf kernel do(2)
        do j = 1, size(weights, 2)
            do i = 1, size(weights, 1)
                ! Update moments
                m(i,j) = adam_beta1 * m(i,j) + (1.0 - adam_beta1) * gradients(i,j)
                v(i,j) = adam_beta2 * v(i,j) + (1.0 - adam_beta2) * gradients(i,j)**2

                ! Update weights with L2 regularization
                weights(i,j) = weights(i,j) * (1.0 - lr * weight_decay) - &
                              corrected_lr * m(i,j) / (sqrt(v(i,j)) + adam_epsilon)
            end do
        end do
    end subroutine adam_update_2d_b

    subroutine adam_update_2d(weights, gradients, m, v, lr, bias_corr1, bias_corr2)
        real(4), device, intent(inout) :: weights(:,:), m(:,:), v(:,:)
        real(4), device, intent(in) :: gradients(:,:)
        real(4), intent(in) :: lr, bias_corr1, bias_corr2
        integer :: i, j
        real(4) :: corrected_lr
        real(4), parameter :: adam_beta1 = 0.9
        real(4), parameter :: adam_beta2 = 0.999
        real(4), parameter :: adam_epsilon = 1e-8
        real(4), parameter :: weight_decay = 1e-4

        ! âœ… CRITICAL DEBUG: Print intermediate values for ONE weight
        block
            real(4) :: w, g, m_val, v_val, update, w_new
            w = weights(1,1)
            g = gradients(1,1)
            m_val = adam_beta1 * m(1,1) + (1.0 - adam_beta1) * g
            v_val = adam_beta2 * v(1,1) + (1.0 - adam_beta2) * g**2

            corrected_lr = lr * sqrt(bias_corr2) / bias_corr1

            update = corrected_lr * m_val / (sqrt(v_val) + adam_epsilon)
            w_new = w * (1.0 - lr * weight_decay) - update

            print *, "ðŸ”¬ DETAILED Adam UPDATE:"
            print *, "   weight[1,1]:", w
            print *, "   gradient[1,1]:", g
            print *, "   m_val:", m_val
            print *, "   v_val:", v_val
            print *, "   sqrt(v_val):", sqrt(v_val)
            print *, "   corrected_lr:", corrected_lr
            print *, "   update:", update
            print *, "   w_new:", w_new
            print *, "   actual_delta:", w_new - w
        end block

        corrected_lr = lr * sqrt(bias_corr2) / bias_corr1

        !$cuf kernel do(2)
        do j = 1, size(weights, 2)
            do i = 1, size(weights, 1)
                m(i,j) = adam_beta1 * m(i,j) + (1.0 - adam_beta1) * gradients(i,j)
                v(i,j) = adam_beta2 * v(i,j) + (1.0 - adam_beta2) * gradients(i,j)**2
                weights(i,j) = weights(i,j) * (1.0 - lr * weight_decay) - &
                              corrected_lr * m(i,j) / (sqrt(v(i,j)) + adam_epsilon)
            end do
        end do
    end subroutine adam_update_2d

    ! Adam update for 1D arrays (biases)
    subroutine adam_update_1d(weights, gradients, m, v, lr, bias_corr1, bias_corr2)
        real(4), device, intent(inout) :: weights(:), m(:), v(:)
        real(4), device, intent(in) :: gradients(:)
        real(4), intent(in) :: lr, bias_corr1, bias_corr2
        integer :: i
        real(4) :: corrected_lr

        corrected_lr = lr * sqrt(bias_corr2) / bias_corr1

        !$cuf kernel do(1)
        do i = 1, size(weights, 1)
            ! Update moments
            m(i) = adam_beta1 * m(i) + (1.0 - adam_beta1) * gradients(i)
            v(i) = adam_beta2 * v(i) + (1.0 - adam_beta2) * gradients(i)**2

            ! Update weights with L2 regularization
            weights(i) = weights(i) * (1.0 - lr * weight_decay) - &
                        corrected_lr * m(i) / (sqrt(v(i)) + adam_epsilon)
        end do
    end subroutine adam_update_1d

    ! ========================================================================
    ! Helper function for random normal initialization
    ! ========================================================================
    subroutine random_normal_2d(array, mean, std)
        real(4), device, intent(out) :: array(:,:)
        real(4), intent(in) :: mean, std
        real(4), allocatable :: temp(:,:)
        integer :: i, j

        allocate(temp(size(array,1), size(array,2)))
        call random_number(temp)
        temp = (temp - 0.5) * 2.0 * std + mean
        array = temp
        deallocate(temp)
    end subroutine random_normal_2d

    ! ========================================================================
    ! Compute loss and accuracy
    ! ========================================================================
    subroutine compute_metrics(model, labels, batch_size, loss, accuracy)
        type(cnn_model), intent(in) :: model
        integer, device, intent(in) :: labels(:)
        integer, intent(in) :: batch_size
        real(4), intent(out) :: loss, accuracy
        real(4), allocatable :: host_probs(:,:)
        integer, allocatable :: host_labels(:)
        integer :: i, j, pred_class, correct
        real(4) :: max_prob

        allocate(host_probs(batch_size, CNN_NUM_CLASSES))
        allocate(host_labels(batch_size))

        host_probs = model%softmax_out(1:batch_size, :)
        host_labels = labels(1:batch_size)

        loss = 0.0
        correct = 0

        do i = 1, batch_size
            ! Cross-entropy loss
            loss = loss - log(max(host_probs(i, host_labels(i) + 1), 1e-7))

            ! Find predicted class
            max_prob = -1.0
            pred_class = 0
            do j = 1, CNN_NUM_CLASSES
                if (host_probs(i, j) > max_prob) then
                    max_prob = host_probs(i, j)
                    pred_class = j - 1
                end if
            end do

            if (pred_class == host_labels(i)) correct = correct + 1
        end do

        loss = loss / batch_size
        accuracy = real(correct) / real(batch_size) * 100.0

        deallocate(host_probs, host_labels)
    end subroutine compute_metrics

    ! ============================================================================
    ! FIX #3: Complete cleanup_model implementation
    ! ============================================================================
    ! INSTRUCTIONS: Replace the entire cleanup_model subroutine with this version
    ! This version properly destroys all descriptors and deallocates all arrays
    ! ============================================================================

    subroutine cleanup_model(model)
        type(cnn_model), intent(inout) :: model
        integer(c_int) :: stat

        print *, "ðŸ§¹ Cleaning up model..."

        ! ========================================================================
        ! DESTROY ALL CUDNN DESCRIPTORS
        ! ========================================================================

        ! Tensor descriptors
        if (c_associated(model%input_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%input_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy input_desc"
        endif

        if (c_associated(model%conv1_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%conv1_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv1_desc"
        endif

        if (c_associated(model%pool1_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%pool1_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy pool1_desc"
        endif

        if (c_associated(model%conv2_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%conv2_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv2_desc"
        endif

        if (c_associated(model%pool2_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%pool2_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy pool2_desc"
        endif

        if (c_associated(model%conv3_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%conv3_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv3_desc"
        endif

        if (c_associated(model%pool3_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%pool3_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy pool3_desc"
        endif

        if (c_associated(model%fc1_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%fc1_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy fc1_desc"
        endif

        if (c_associated(model%fc2_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%fc2_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy fc2_desc"
        endif

        if (c_associated(model%output_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%output_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy output_desc"
        endif

        ! Filter descriptors
        if (c_associated(model%conv1_filter_desc)) then
            stat = cudnnDestroyFilterDescriptor(model%conv1_filter_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv1_filter_desc"
        endif

        if (c_associated(model%conv2_filter_desc)) then
            stat = cudnnDestroyFilterDescriptor(model%conv2_filter_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv2_filter_desc"
        endif

        if (c_associated(model%conv3_filter_desc)) then
            stat = cudnnDestroyFilterDescriptor(model%conv3_filter_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv3_filter_desc"
        endif

        ! Bias descriptors
        if (c_associated(model%conv1_bias_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%conv1_bias_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv1_bias_desc"
        endif

        if (c_associated(model%conv2_bias_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%conv2_bias_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv2_bias_desc"
        endif

        if (c_associated(model%conv3_bias_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%conv3_bias_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv3_bias_desc"
        endif

        ! Convolution descriptors
        if (c_associated(model%conv1_conv_desc)) then
            stat = cudnnDestroyConvolutionDescriptor(model%conv1_conv_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv1_conv_desc"
        endif

        if (c_associated(model%conv2_conv_desc)) then
            stat = cudnnDestroyConvolutionDescriptor(model%conv2_conv_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv2_conv_desc"
        endif

        if (c_associated(model%conv3_conv_desc)) then
            stat = cudnnDestroyConvolutionDescriptor(model%conv3_conv_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy conv3_conv_desc"
        endif

        ! Pooling descriptor (shared)
        if (c_associated(model%pooling_desc)) then
            stat = cudnnDestroyPoolingDescriptor(model%pooling_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy pooling_desc"
        endif

        ! Activation descriptor (shared)
        if (c_associated(model%activation_desc)) then
            stat = cudnnDestroyActivationDescriptor(model%activation_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy activation_desc"
        endif

        ! Batch normalization descriptors
        if (c_associated(model%bn1_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%bn1_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy bn1_desc"
        endif

        if (c_associated(model%bn2_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%bn2_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy bn2_desc"
        endif

        if (c_associated(model%bn3_desc)) then
            stat = cudnnDestroyTensorDescriptor(model%bn3_desc)
            if (stat /= CUDNN_STATUS_SUCCESS) print *, "âš ï¸  Failed to destroy bn3_desc"
        endif

        ! ========================================================================
        ! DEALLOCATE ALL WEIGHT ARRAYS
        ! ========================================================================

        ! Convolution weights and biases
        if (allocated(model%conv1_weights)) deallocate(model%conv1_weights)
        if (allocated(model%conv1_bias)) deallocate(model%conv1_bias)
        if (allocated(model%conv2_weights)) deallocate(model%conv2_weights)
        if (allocated(model%conv2_bias)) deallocate(model%conv2_bias)
        if (allocated(model%conv3_weights)) deallocate(model%conv3_weights)
        if (allocated(model%conv3_bias)) deallocate(model%conv3_bias)

        ! Batch normalization parameters
        if (allocated(model%bn1_scale)) deallocate(model%bn1_scale)
        if (allocated(model%bn1_bias)) deallocate(model%bn1_bias)
        if (allocated(model%bn1_running_mean)) deallocate(model%bn1_running_mean)
        if (allocated(model%bn1_running_var)) deallocate(model%bn1_running_var)
        if (allocated(model%bn1_saved_mean)) deallocate(model%bn1_saved_mean)
        if (allocated(model%bn1_saved_inv_var)) deallocate(model%bn1_saved_inv_var)

        if (allocated(model%bn2_scale)) deallocate(model%bn2_scale)
        if (allocated(model%bn2_bias)) deallocate(model%bn2_bias)
        if (allocated(model%bn2_running_mean)) deallocate(model%bn2_running_mean)
        if (allocated(model%bn2_running_var)) deallocate(model%bn2_running_var)
        if (allocated(model%bn2_saved_mean)) deallocate(model%bn2_saved_mean)
        if (allocated(model%bn2_saved_inv_var)) deallocate(model%bn2_saved_inv_var)

        if (allocated(model%bn3_scale)) deallocate(model%bn3_scale)
        if (allocated(model%bn3_bias)) deallocate(model%bn3_bias)
        if (allocated(model%bn3_running_mean)) deallocate(model%bn3_running_mean)
        if (allocated(model%bn3_running_var)) deallocate(model%bn3_running_var)
        if (allocated(model%bn3_saved_mean)) deallocate(model%bn3_saved_mean)
        if (allocated(model%bn3_saved_inv_var)) deallocate(model%bn3_saved_inv_var)

        ! Fully connected weights and biases
        if (allocated(model%fc1_weights)) deallocate(model%fc1_weights)
        if (allocated(model%fc1_bias)) deallocate(model%fc1_bias)
        if (allocated(model%fc2_weights)) deallocate(model%fc2_weights)
        if (allocated(model%fc2_bias)) deallocate(model%fc2_bias)

        ! ========================================================================
        ! DEALLOCATE ALL INTERMEDIATE OUTPUT ARRAYS
        ! ========================================================================

        if (allocated(model%input)) deallocate(model%input)
        if (allocated(model%conv1_out)) deallocate(model%conv1_out)
        if (allocated(model%bn1_out)) deallocate(model%bn1_out)
        if (allocated(model%relu1_out)) deallocate(model%relu1_out)
        if (allocated(model%pool1_out)) deallocate(model%pool1_out)
        if (allocated(model%conv2_out)) deallocate(model%conv2_out)
        if (allocated(model%bn2_out)) deallocate(model%bn2_out)
        if (allocated(model%relu2_out)) deallocate(model%relu2_out)
        if (allocated(model%pool2_out)) deallocate(model%pool2_out)
        if (allocated(model%conv3_out)) deallocate(model%conv3_out)
        if (allocated(model%bn3_out)) deallocate(model%bn3_out)
        if (allocated(model%relu3_out)) deallocate(model%relu3_out)
        if (allocated(model%pool3_out)) deallocate(model%pool3_out)
        if (allocated(model%flatten)) deallocate(model%flatten)
        if (allocated(model%fc1_out)) deallocate(model%fc1_out)
        if (allocated(model%fc2_out)) deallocate(model%fc2_out)
        if (allocated(model%softmax_out)) deallocate(model%softmax_out)

        ! ========================================================================
        ! DEALLOCATE ALL GRADIENT ARRAYS
        ! ========================================================================

        ! Conv gradient arrays
        if (allocated(model%grad_input)) deallocate(model%grad_input)
        if (allocated(model%grad_conv1)) deallocate(model%grad_conv1)
        if (allocated(model%grad_bn1)) deallocate(model%grad_bn1)
        if (allocated(model%grad_relu1)) deallocate(model%grad_relu1)
        if (allocated(model%grad_pool1)) deallocate(model%grad_pool1)
        if (allocated(model%grad_conv2)) deallocate(model%grad_conv2)
        if (allocated(model%grad_bn2)) deallocate(model%grad_bn2)
        if (allocated(model%grad_relu2)) deallocate(model%grad_relu2)
        if (allocated(model%grad_pool2)) deallocate(model%grad_pool2)
        if (allocated(model%grad_conv3)) deallocate(model%grad_conv3)
        if (allocated(model%grad_bn3)) deallocate(model%grad_bn3)
        if (allocated(model%grad_relu3)) deallocate(model%grad_relu3)
        if (allocated(model%grad_pool3)) deallocate(model%grad_pool3)

        ! FC gradient arrays
        if (allocated(model%grad_fc1)) deallocate(model%grad_fc1)
        if (allocated(model%grad_fc1_dropout)) deallocate(model%grad_fc1_dropout)
        if (allocated(model%grad_fc1_relu)) deallocate(model%grad_fc1_relu)
        if (allocated(model%grad_fc2)) deallocate(model%grad_fc2)
        if (allocated(model%grad_fc2_dropout)) deallocate(model%grad_fc2_dropout)
        if (allocated(model%grad_fc2_relu)) deallocate(model%grad_fc2_relu)
        if (allocated(model%grad_fc3)) deallocate(model%grad_fc3)

        ! Weight gradient arrays
        if (allocated(model%grad_conv1_weights)) deallocate(model%grad_conv1_weights)
        if (allocated(model%grad_conv1_bias)) deallocate(model%grad_conv1_bias)
        if (allocated(model%grad_conv2_weights)) deallocate(model%grad_conv2_weights)
        if (allocated(model%grad_conv2_bias)) deallocate(model%grad_conv2_bias)
        if (allocated(model%grad_conv3_weights)) deallocate(model%grad_conv3_weights)
        if (allocated(model%grad_conv3_bias)) deallocate(model%grad_conv3_bias)
        if (allocated(model%grad_fc1_weights)) deallocate(model%grad_fc1_weights)
        if (allocated(model%grad_fc1_bias)) deallocate(model%grad_fc1_bias)
        if (allocated(model%grad_fc2_weights)) deallocate(model%grad_fc2_weights)
        if (allocated(model%grad_fc2_bias)) deallocate(model%grad_fc2_bias)
        if (allocated(model%grad_fc3_weights)) deallocate(model%grad_fc3_weights)
        if (allocated(model%grad_fc3_bias)) deallocate(model%grad_fc3_bias)

        ! Batch norm gradient arrays
        if (allocated(model%grad_bn1_scale)) deallocate(model%grad_bn1_scale)
        if (allocated(model%grad_bn1_bias)) deallocate(model%grad_bn1_bias)
        if (allocated(model%grad_bn2_scale)) deallocate(model%grad_bn2_scale)
        if (allocated(model%grad_bn2_bias)) deallocate(model%grad_bn2_bias)
        if (allocated(model%grad_bn3_scale)) deallocate(model%grad_bn3_scale)
        if (allocated(model%grad_bn3_bias)) deallocate(model%grad_bn3_bias)

        print *, "âœ… Gradient arrays cleaned up"

        ! ========================================================================
        ! DEALLOCATE ADAM OPTIMIZER STATE
        ! ========================================================================

        ! Conv layers - first moments
        if (allocated(model%m_conv1_weights)) deallocate(model%m_conv1_weights)
        if (allocated(model%m_conv1_bias)) deallocate(model%m_conv1_bias)
        if (allocated(model%m_conv2_weights)) deallocate(model%m_conv2_weights)
        if (allocated(model%m_conv2_bias)) deallocate(model%m_conv2_bias)
        if (allocated(model%m_conv3_weights)) deallocate(model%m_conv3_weights)
        if (allocated(model%m_conv3_bias)) deallocate(model%m_conv3_bias)

        ! Conv layers - second moments
        if (allocated(model%v_conv1_weights)) deallocate(model%v_conv1_weights)
        if (allocated(model%v_conv1_bias)) deallocate(model%v_conv1_bias)
        if (allocated(model%v_conv2_weights)) deallocate(model%v_conv2_weights)
        if (allocated(model%v_conv2_bias)) deallocate(model%v_conv2_bias)
        if (allocated(model%v_conv3_weights)) deallocate(model%v_conv3_weights)
        if (allocated(model%v_conv3_bias)) deallocate(model%v_conv3_bias)

        ! FC layers - first moments
        if (allocated(model%m_fc1_weights)) deallocate(model%m_fc1_weights)
        if (allocated(model%m_fc1_bias)) deallocate(model%m_fc1_bias)
        if (allocated(model%m_fc2_weights)) deallocate(model%m_fc2_weights)
        if (allocated(model%m_fc2_bias)) deallocate(model%m_fc2_bias)
        if (allocated(model%m_fc3_weights)) deallocate(model%m_fc3_weights)
        if (allocated(model%m_fc3_bias)) deallocate(model%m_fc3_bias)

        ! FC layers - second moments
        if (allocated(model%v_fc1_weights)) deallocate(model%v_fc1_weights)
        if (allocated(model%v_fc1_bias)) deallocate(model%v_fc1_bias)
        if (allocated(model%v_fc2_weights)) deallocate(model%v_fc2_weights)
        if (allocated(model%v_fc2_bias)) deallocate(model%v_fc2_bias)
        if (allocated(model%v_fc3_weights)) deallocate(model%v_fc3_weights)
        if (allocated(model%v_fc3_bias)) deallocate(model%v_fc3_bias)

        ! Batch norm - first moments
        if (allocated(model%m_bn1_scale)) deallocate(model%m_bn1_scale)
        if (allocated(model%m_bn1_bias)) deallocate(model%m_bn1_bias)
        if (allocated(model%m_bn2_scale)) deallocate(model%m_bn2_scale)
        if (allocated(model%m_bn2_bias)) deallocate(model%m_bn2_bias)
        if (allocated(model%m_bn3_scale)) deallocate(model%m_bn3_scale)
        if (allocated(model%m_bn3_bias)) deallocate(model%m_bn3_bias)

        ! Batch norm - second moments
        if (allocated(model%v_bn1_scale)) deallocate(model%v_bn1_scale)
        if (allocated(model%v_bn1_bias)) deallocate(model%v_bn1_bias)
        if (allocated(model%v_bn2_scale)) deallocate(model%v_bn2_scale)
        if (allocated(model%v_bn2_bias)) deallocate(model%v_bn2_bias)
        if (allocated(model%v_bn3_scale)) deallocate(model%v_bn3_scale)
        if (allocated(model%v_bn3_bias)) deallocate(model%v_bn3_bias)

        print *, "âœ… Adam optimizer state cleaned up"

        ! âœ… Cleanup FC3
        if (allocated(model%fc3_weights)) deallocate(model%fc3_weights)
        if (allocated(model%fc3_bias)) deallocate(model%fc3_bias)
        if (allocated(model%fc3_out)) deallocate(model%fc3_out)
        if (allocated(model%grad_fc3_weights)) deallocate(model%grad_fc3_weights)
        if (allocated(model%grad_fc3_bias)) deallocate(model%grad_fc3_bias)

        ! âœ… Cleanup intermediate FC states
        if (allocated(model%fc1_relu)) deallocate(model%fc1_relu)
        if (allocated(model%fc1_dropout)) deallocate(model%fc1_dropout)
        if (allocated(model%fc2_relu)) deallocate(model%fc2_relu)
        if (allocated(model%fc2_dropout)) deallocate(model%fc2_dropout)

        ! âœ… Cleanup dropout
        if (allocated(model%dropout_states)) deallocate(model%dropout_states)
        if (c_associated(model%dropout_desc)) then
            stat = cudnnDestroyDropoutDescriptor(model%dropout_desc)
        endif

        print *, "âœ… Model cleanup completed successfully"
    end subroutine cleanup_model

    subroutine cleanup_cudnn_cublas()
        integer :: stat

        stat = cudnnDestroy(cudnn_handle)
        stat = cublasDestroy_v2(cublas_handle)

        print *, "âœ… cuDNN and cuBLAS cleaned up"
    end subroutine cleanup_cudnn_cublas
end module cudnn_cifar10

program cifar10_cudnn6_training
    use cudafor
    use cudnn_cifar10
    use cifar10_data_module
    implicit none

    type(cnn_model) :: model
    real(4), device, allocatable :: batch_data(:,:)
    integer, device, allocatable :: batch_labels(:)
    real(4), device, allocatable :: grad_output(:,:)

    ! Training parameters
    real(4) :: learning_rate, initial_lr
    integer :: num_epochs, epoch, batch, num_batches
    integer :: batch_start, batch_end

    ! Metrics
    real(4) :: train_loss, train_acc, test_loss, test_acc, best_test_acc
    real(4) :: epoch_time, total_time
    integer(8) :: start_time, end_time, time_freq
    logical :: success
    integer(8) :: dummy_count, dummy_max

    ! Configuration
    num_epochs = 10
    initial_lr = 0.001 ! matching PyTorch 0.001 with Adam
    learning_rate = initial_lr
    best_test_acc = 0.0
    total_time = 0.0

    ! ========================================================================
    ! INITIALIZATION
    ! ========================================================================
    print *, ""
    print *, "ðŸš€ CIFAR-10 CNN Training with cuDNN"
    print *, "======================================"
    print *, "ðŸ“Š Configuration:"
    print *, "   Epochs:", num_epochs
    print *, "   Batch size:", BATCH_SIZE
    print *, "   Learning rate:", learning_rate
    print *, "   Optimizer: SGD"
    print *, ""

    call init_cudnn_cublas()
    call load_cifar10_data()
    call create_model(model, BATCH_SIZE)

    ! Allocate batch arrays
    allocate(batch_data(BATCH_SIZE, input_size))
    allocate(batch_labels(BATCH_SIZE))
    allocate(grad_output(BATCH_SIZE, CNN_NUM_CLASSES))

    num_batches = train_samples / BATCH_SIZE
    call system_clock(dummy_count, time_freq, dummy_max)

    if (time_freq == 0) then
        print *, "âš ï¸  WARNING: system_clock frequency is zero, using fallback"
        time_freq = 1000  ! 1000 ticks/second fallback
    else
        print *, "â±ï¸  System clock frequency:", time_freq, "ticks/second"
    endif

    ! ========================================================================
    ! MAIN TRAINING LOOP
    ! ========================================================================
    do epoch = 1, num_epochs
        call system_clock(start_time)

        ! Update learning rate every 30 epochs
        if (mod(epoch, 30) == 0 .and. epoch > 0) then
            learning_rate = learning_rate * 0.5
            print *, "ðŸ“‰ Learning rate reduced to:", learning_rate
        endif

        ! ====================================================================
        ! TRAINING PHASE
        ! ====================================================================
        call train_epoch(model, batch_data, batch_labels, grad_output, &
                        learning_rate, num_batches, train_loss, train_acc, success)

        if (.not. success) then
            print *, "âŒ Training failed at epoch", epoch
            stop
        endif

        ! ====================================================================
        ! EVALUATION PHASE
        ! ====================================================================
        call evaluate_epoch(model, batch_data, batch_labels, &
                           test_loss, test_acc, success)

        if (.not. success) then
            print *, "âŒ Evaluation failed at epoch", epoch
            stop
        endif

        ! ====================================================================
        ! METRICS AND LOGGING
        ! ====================================================================
        call system_clock(end_time)
        epoch_time = real(end_time - start_time) / real(time_freq)
        total_time = total_time + epoch_time

        ! Update best accuracy
        if (test_acc > best_test_acc) then
            best_test_acc = test_acc
        endif

        ! Print epoch summary
        print *, "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        print '(A,I3,A,I3)', "ðŸ“ˆ EPOCH ", epoch, "/", num_epochs
        print '(A,F8.4,A,F6.2,A)', "   Train: Loss=", train_loss, " Acc=", train_acc, "%"
        print '(A,F8.4,A,F6.2,A)', "   Test:  Loss=", test_loss, " Acc=", test_acc, "%"
        print '(A,F6.2,A)', "   Best Test: ", best_test_acc, "%"
        print '(A,F6.1,A)', "   Time: ", epoch_time, "s"
        print *, "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
        print *, ""
    end do

    ! ========================================================================
    ! FINAL SUMMARY
    ! ========================================================================
    print *, ""
    print *, "ðŸŽ‰ TRAINING COMPLETED!"
    print *, "======================"
    print '(A,F6.2,A)', "ðŸ† Best Test Accuracy: ", best_test_acc, "%"
    print '(A,F6.2,A)', "ðŸ“Š Final Train Accuracy: ", train_acc, "%"
    print '(A,F6.2,A)', "ðŸ“Š Final Test Accuracy: ", test_acc, "%"
    print '(A,F7.1,A)', "â±ï¸  Total Time: ", total_time, "s"
    print '(A,F6.1,A)', "â±ï¸  Avg Time/Epoch: ", total_time/num_epochs, "s"
    print *, ""

    ! Performance evaluation
    print *, "ðŸ“Š Performance Analysis:"
    print *, "   Expected (Python ref): ~70-80%"
    print '(A,F6.2,A)', "   Achieved (cuDNN): ", best_test_acc, "%"

    if (best_test_acc >= 70.0) then
        print *, "âœ… EXCELLENT! Matches Python reference"
    else if (best_test_acc >= 50.0) then
        print *, "ðŸ“ˆ GOOD! Model is learning - may need more epochs"
    else
        print *, "âš ï¸  Needs improvement - check hyperparameters"
    endif
    print *, ""

    ! Cleanup
    call cleanup_model(model)
    call cleanup_cudnn_cublas()
    deallocate(batch_data, batch_labels, grad_output)
contains
    ! ========================================================================
    ! COMPUTE LOSS GRADIENT (Softmax + Cross-Entropy)
    ! ========================================================================
    ! Fix for compute_loss_gradient
    subroutine compute_loss_gradient(probs, labels, grad, batch_size)
        real(4), device, intent(in) :: probs(:,:)
        integer, device, intent(in) :: labels(:)
        real(4), device, intent(out) :: grad(:,:)
        integer, intent(in) :: batch_size
        integer :: i, j

        ! Remove scaling - cross entropy loss already normalizes by batch size
        !$cuf kernel do(2)
        do i = 1, batch_size
            do j = 1, CNN_NUM_CLASSES
                if (j == labels(i) + 1) then
                    grad(i, j) = probs(i, j) - 1.0  ! For correct class
                else
                    grad(i, j) = probs(i, j)        ! For incorrect classes
                end if
            end do
        end do
    end subroutine

    ! ========================================================================
    ! TRAIN ONE EPOCH
    ! ========================================================================
    subroutine train_epoch(model, batch_data, batch_labels, grad_output, &
                          lr, num_batches, epoch_loss, epoch_acc, success)
        type(cnn_model), intent(inout) :: model
        real(4), device, intent(inout) :: batch_data(:,:)
        integer, device, intent(inout) :: batch_labels(:)
        real(4), device, intent(inout) :: grad_output(:,:)
        real(4), intent(in) :: lr
        integer, intent(in) :: num_batches
        real(4), intent(out) :: epoch_loss, epoch_acc
        logical, intent(out) :: success

        integer :: batch, batch_start, batch_end
        real(4) :: batch_loss, batch_accuracy
        real(4) :: loss_sum
        integer :: correct_total, total_samples

        success = .false.
        loss_sum = 0.0
        correct_total = 0
        total_samples = 0

        do batch = 1, num_batches
            batch_start = (batch - 1) * BATCH_SIZE + 1
            batch_end = batch_start + BATCH_SIZE - 1

            ! Get batch
            batch_data = gpu_train_data(batch_start:batch_end, :)
            batch_labels = gpu_train_labels(batch_start:batch_end)

            ! âœ… UPDATED: Forward pass in TRAINING mode
            call forward_pass(model, batch_data, BATCH_SIZE, .true.)

            ! Compute gradients
            call compute_loss_gradient(model%softmax_out, batch_labels, grad_output, BATCH_SIZE)

            ! Backward pass and update
            call backward_pass(model, grad_output, BATCH_SIZE, lr, .true.)  ! Training mode

            ! In train_epoch, after backward_pass on first batch
            if (epoch == 1 .and. batch == 1) then
                block
                    real(4) :: fc3_grad_check(5), conv1_grad_check(5)
                    real(4) :: fc3_grad_norm, conv1_grad_norm

                    fc3_grad_check = model%grad_fc3_weights(1, 1:5)
                    conv1_grad_check = model%grad_conv1_weights(1, 1, 1, 1:5)

                    print *, "ðŸ” Gradient diagnostics:"
                    print *, "   FC3 weight grads:", fc3_grad_check
                    print *, "   Conv1 weight grads:", conv1_grad_check

                    ! Compute approximate gradient norms
                    fc3_grad_norm = sqrt(sum(fc3_grad_check**2))
                    conv1_grad_norm = sqrt(sum(conv1_grad_check**2))

                    print *, "   FC3 grad norm:", fc3_grad_norm
                    print *, "   Conv1 grad norm:", conv1_grad_norm

                    if (fc3_grad_norm < 1e-6) then
                        print *, "âŒ FC3 gradients too small!"
                    endif
                end block
            endif
            ! In train_epoch, after backward_pass, for first batch only
            if (epoch == 1 .and. batch == 1) then
                block
                    real(4) :: fc3_weight_before, fc3_weight_after

                    fc3_weight_before = model%fc3_weights(1, 1)
                    print *, "ðŸ” FC3 weight BEFORE update:", fc3_weight_before

                    ! (backward_pass already called here, which calls update_weights)

                    fc3_weight_after = model%fc3_weights(1, 1)
                    print *, "ðŸ” FC3 weight AFTER update:", fc3_weight_after
                    print *, "ðŸ” Weight delta:", fc3_weight_after - fc3_weight_before

                    if (abs(fc3_weight_after - fc3_weight_before) < 1e-8) then
                        print *, "âŒ CRITICAL: Weights NOT changing!"
                    else
                        print *, "âœ… Weights ARE changing"
                    endif
                end block
            endif

            ! Compute batch metrics
            call compute_metrics(model, batch_labels, BATCH_SIZE, batch_loss, batch_accuracy)

            ! Accumulate
            loss_sum = loss_sum + batch_loss * BATCH_SIZE
            correct_total = correct_total + int(batch_accuracy * BATCH_SIZE / 100.0)
            total_samples = total_samples + BATCH_SIZE
        end do

        ! Compute epoch averages
        epoch_loss = loss_sum / real(total_samples)
        epoch_acc = real(correct_total) / real(total_samples) * 100.0
        success = .true.
    end subroutine train_epoch

    ! ========================================================================
    ! EVALUATE ON TEST SET
    ! ========================================================================
    subroutine evaluate_epoch(model, batch_data, batch_labels, &
                             epoch_loss, epoch_acc, success)
        type(cnn_model), intent(inout) :: model
        real(4), device, intent(inout) :: batch_data(:,:)
        integer, device, intent(inout) :: batch_labels(:)
        real(4), intent(out) :: epoch_loss, epoch_acc
        logical, intent(out) :: success

        integer :: batch, num_test_batches, batch_start, batch_end, actual_batch_size
        real(4) :: batch_loss, batch_accuracy
        real(4) :: loss_sum
        integer :: correct_total, total_samples

        success = .false.
        loss_sum = 0.0
        correct_total = 0
        total_samples = 0

        num_test_batches = (test_samples + BATCH_SIZE - 1) / BATCH_SIZE

        do batch = 1, num_test_batches
            batch_start = (batch - 1) * BATCH_SIZE + 1
            batch_end = min(batch_start + BATCH_SIZE - 1, test_samples)
            actual_batch_size = batch_end - batch_start + 1

            ! Get test batch
            batch_data(1:actual_batch_size, :) = gpu_test_data(batch_start:batch_end, :)
            batch_labels(1:actual_batch_size) = gpu_test_labels(batch_start:batch_end)

            ! âœ… UPDATED: Forward pass in INFERENCE mode (uses learned running stats)
            call forward_pass(model, batch_data, actual_batch_size, .false.)

            ! Compute batch metrics
            call compute_metrics(model, batch_labels, actual_batch_size, batch_loss, batch_accuracy)

            ! Accumulate
            loss_sum = loss_sum + batch_loss * actual_batch_size
            correct_total = correct_total + int(batch_accuracy * actual_batch_size / 100.0)
            total_samples = total_samples + actual_batch_size
        end do

        ! Compute epoch averages
        epoch_loss = loss_sum / real(total_samples)
        epoch_acc = real(correct_total) / real(total_samples) * 100.0
        success = .true.
    end subroutine evaluate_epoch
end program cifar10_cudnn6_training
! nvfortran -cuda -O3 cuda_batch_state3.cuf cifar10_cudnn6.cuf -o cifar10_cudnn6 -lcublas -lcudart -lcudnn
! pure cuDNN version
